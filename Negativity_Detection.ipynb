{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83a56ec",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io, json\n",
    "import csv\n",
    "import requests\n",
    "import stanza\n",
    "import emojis\n",
    "import collections\n",
    "import math\n",
    "import twitter\n",
    "import fnmatch\n",
    "import pickle\n",
    "import urllib\n",
    "import re, string, ast, emoji, json, urlexpander, sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files  \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.corpus import words\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy.stats import kurtosis, skew, entropy\n",
    "from textstat.textstat import textstat\n",
    "from textblob import TextBlob\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from persiantools.jdatetime import JalaliDate\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import hazm\n",
    "from hazm import Normalizer, Lemmatizer, WordTokenizer\n",
    "from cleantext import clean\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import imblearn\n",
    "from numpy import mean\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from googletrans import Translator\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "#fasttext.util.download_model('fa', if_exists='ignore')\n",
    "#!pip install googletrans==4.0.0-rc1\n",
    "ft = fasttext.load_model('cc.fa.300.bin')\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b07b2",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3c114c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>neg</th>\n",
       "      <th>pol</th>\n",
       "      <th>char</th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MohajerMr ØºØ°Ø§ÛŒ Ú†Ø±Ø¨ ! Ø³Ø§Ù„Ù… Ù…Ø­Ø³ÙˆØ¨ Ù†Ù…ÛŒØ´Ù‡</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Oct 11 15:24:43 +0000 2021</td>\n",
       "      <td>[11, 38]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø´ Ù‡Ù…ÛŒÙ† ÙˆØ²ÛŒØ± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ù¾Ø±ÙˆØ±Ø´ Ú©Ù‡ Ø¯...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wed Nov 17 09:20:50 +0000 2021</td>\n",
       "      <td>[0, 90]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [{'dis...</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'contributors': None, 'coordinates': None, 'c...</td>\n",
       "      <td>1.460879e+18</td>\n",
       "      <td>1.460879e+18</td>\n",
       "      <td>{'display': 'twitter.com/mah_sadeghi/stâ€¦', 'ex...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mah_sadeghi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø±Ø¦ÛŒØ³ Ø³Ø§Ø²Ù…Ø§Ù† Ø¨Ø§Ø²Ø±Ø³ÛŒ Ú©Ù„ Ú©Ø´ÙˆØ± Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚ ÙˆØ¸Ø§ÛŒÙ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thu Nov 11 13:56:42 +0000 2021</td>\n",
       "      <td>[0, 183]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>1660</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ejei_com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø§Ù‚Ø¯Ø§Ù… Ø§ÛŒØ«Ø§Ø±Ú¯Ø±Ø§Ù†Ù‡ Ùˆ Ù‚Ù‡Ø±Ù…Ø§Ù†Ø§Ù†Ù‡ #Ø¹Ù„ÛŒ_Ù„Ù†Ø¯ÛŒ Ù†ÙˆØ¬ÙˆØ§Ù† ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Sep 25 18:05:50 +0000 2021</td>\n",
       "      <td>[0, 253]</td>\n",
       "      <td>{'hashtags': [{'indices': [29, 38], 'text': 'Ø¹...</td>\n",
       "      <td>1149</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GhazizadehSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ØªØ´Ú©Ø± Ø§Ø² Ø¨Ø§Ø²ÛŒÚ©Ù†Ø§Ù† ØªÛŒÙ… Ù…Ù„ÛŒ #ÙÙˆØªØ¨Ø§Ù„ Ú©Ù‡ Ø¨Ø§ #Ù¾ÛŒØ±ÙˆØ²ÛŒ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thu Oct 07 18:49:04 +0000 2021</td>\n",
       "      <td>[0, 199]</td>\n",
       "      <td>{'hashtags': [{'indices': [25, 32], 'text': 'Ù...</td>\n",
       "      <td>374</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'media': [{'display_url': 'pic.twitter.com/8w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hosseini_social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>@azarijahromi ÙØ±Ù‡Ø§Ø¯ Ù…Ø¬ÛŒØ¯ÛŒ Ø¨Ø§ Ø¨ÛŒ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø§ÛŒÙ† Ú©Ù„â€Œ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri May 14 20:24:25 +0000 2021</td>\n",
       "      <td>[14, 97]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>268</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>@zoheirmousavi Ø¬Ø§ÛŒ Ù„Ú¯Ø¯ØŒ Ø¨Ø§Ø²ÛŒ Ù…ÛŒÚ©Ø±Ø¯Ù† Ø§Ù„Ø§Ù† ÙˆØ¶Ø¹Ø´Ùˆ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri May 14 18:00:56 +0000 2021</td>\n",
       "      <td>[15, 56]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>@fahimehtabaee1 @YeganehKhodami Ø³Ù„Ø§Ù… ÙˆØ§Ú©Ø³Ù† Ú©ÙˆØ¨...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri May 14 12:58:24 +0000 2021</td>\n",
       "      <td>[32, 130]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>@YeganehKhodami Ø³Ù„Ø§Ù…ØŒ Ù…Ù† Ù…Ù†Ú©Ø± Ø§ÛŒÙ†Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Øª...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri May 14 12:56:36 +0000 2021</td>\n",
       "      <td>[16, 170]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>@tahamohammadi_ @drjahanpur Ø³Ù„Ø§Ù…ØŒ Ù…Ø§ Ú©Ø§Ø±Ù‡â€ŒØ§ÛŒ Ù†...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri May 14 09:55:52 +0000 2021</td>\n",
       "      <td>[28, 179]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5100 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              full_text  neg  pol  char   \n",
       "0                @MohajerMr ØºØ°Ø§ÛŒ Ú†Ø±Ø¨ ! Ø³Ø§Ù„Ù… Ù…Ø­Ø³ÙˆØ¨ Ù†Ù…ÛŒØ´Ù‡    0    0     0  \\\n",
       "1     Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø´ Ù‡Ù…ÛŒÙ† ÙˆØ²ÛŒØ± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ù¾Ø±ÙˆØ±Ø´ Ú©Ù‡ Ø¯...    1    0     1   \n",
       "2     Ø±Ø¦ÛŒØ³ Ø³Ø§Ø²Ù…Ø§Ù† Ø¨Ø§Ø²Ø±Ø³ÛŒ Ú©Ù„ Ú©Ø´ÙˆØ± Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚ ÙˆØ¸Ø§ÛŒÙ...    0    0     0   \n",
       "3     Ø§Ù‚Ø¯Ø§Ù… Ø§ÛŒØ«Ø§Ø±Ú¯Ø±Ø§Ù†Ù‡ Ùˆ Ù‚Ù‡Ø±Ù…Ø§Ù†Ø§Ù†Ù‡ #Ø¹Ù„ÛŒ_Ù„Ù†Ø¯ÛŒ Ù†ÙˆØ¬ÙˆØ§Ù† ...    0    0     0   \n",
       "4     ØªØ´Ú©Ø± Ø§Ø² Ø¨Ø§Ø²ÛŒÚ©Ù†Ø§Ù† ØªÛŒÙ… Ù…Ù„ÛŒ #ÙÙˆØªØ¨Ø§Ù„ Ú©Ù‡ Ø¨Ø§ #Ù¾ÛŒØ±ÙˆØ²ÛŒ...    0    0     0   \n",
       "...                                                 ...  ...  ...   ...   \n",
       "5095  @azarijahromi ÙØ±Ù‡Ø§Ø¯ Ù…Ø¬ÛŒØ¯ÛŒ Ø¨Ø§ Ø¨ÛŒ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø§ÛŒÙ† Ú©Ù„â€Œ...    1    0     1   \n",
       "5096  @zoheirmousavi Ø¬Ø§ÛŒ Ù„Ú¯Ø¯ØŒ Ø¨Ø§Ø²ÛŒ Ù…ÛŒÚ©Ø±Ø¯Ù† Ø§Ù„Ø§Ù† ÙˆØ¶Ø¹Ø´Ùˆ...    1    1     0   \n",
       "5097  @fahimehtabaee1 @YeganehKhodami Ø³Ù„Ø§Ù… ÙˆØ§Ú©Ø³Ù† Ú©ÙˆØ¨...    0    0     0   \n",
       "5098  @YeganehKhodami Ø³Ù„Ø§Ù…ØŒ Ù…Ù† Ù…Ù†Ú©Ø± Ø§ÛŒÙ†Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Øª...    0    0     0   \n",
       "5099  @tahamohammadi_ @drjahanpur Ø³Ù„Ø§Ù…ØŒ Ù…Ø§ Ú©Ø§Ø±Ù‡â€ŒØ§ÛŒ Ù†...    1    0     1   \n",
       "\n",
       "      contributors  coordinates                      created_at   \n",
       "0              NaN          NaN  Mon Oct 11 15:24:43 +0000 2021  \\\n",
       "1              NaN          NaN  Wed Nov 17 09:20:50 +0000 2021   \n",
       "2              NaN          NaN  Thu Nov 11 13:56:42 +0000 2021   \n",
       "3              NaN          NaN  Sat Sep 25 18:05:50 +0000 2021   \n",
       "4              NaN          NaN  Thu Oct 07 18:49:04 +0000 2021   \n",
       "...            ...          ...                             ...   \n",
       "5095           NaN          NaN  Fri May 14 20:24:25 +0000 2021   \n",
       "5096           NaN          NaN  Fri May 14 18:00:56 +0000 2021   \n",
       "5097           NaN          NaN  Fri May 14 12:58:24 +0000 2021   \n",
       "5098           NaN          NaN  Fri May 14 12:56:36 +0000 2021   \n",
       "5099           NaN          NaN  Fri May 14 09:55:52 +0000 2021   \n",
       "\n",
       "     display_text_range                                           entities   \n",
       "0              [11, 38]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...  \\\n",
       "1               [0, 90]  {'hashtags': [], 'symbols': [], 'urls': [{'dis...   \n",
       "2              [0, 183]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "3              [0, 253]  {'hashtags': [{'indices': [29, 38], 'text': 'Ø¹...   \n",
       "4              [0, 199]  {'hashtags': [{'indices': [25, 32], 'text': 'Ù...   \n",
       "...                 ...                                                ...   \n",
       "5095           [14, 97]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "5096           [15, 56]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "5097          [32, 130]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "5098          [16, 170]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "5099          [28, 179]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "\n",
       "      favorite_count  ...  truncated   \n",
       "0                  1  ...      False  \\\n",
       "1                 82  ...      False   \n",
       "2               1660  ...      False   \n",
       "3               1149  ...      False   \n",
       "4                374  ...      False   \n",
       "...              ...  ...        ...   \n",
       "5095             268  ...      False   \n",
       "5096               6  ...      False   \n",
       "5097               2  ...      False   \n",
       "5098               9  ...      False   \n",
       "5099               3  ...      False   \n",
       "\n",
       "                                                   user  possibly_sensitive   \n",
       "0     {'contributors_enabled': False, 'created_at': ...                 NaN  \\\n",
       "1     {'contributors_enabled': False, 'created_at': ...                 0.0   \n",
       "2     {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "3     {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "4     {'contributors_enabled': False, 'created_at': ...                 0.0   \n",
       "...                                                 ...                 ...   \n",
       "5095  {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "5096  {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "5097  {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "5098  {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "5099  {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "\n",
       "                                          quoted_status quoted_status_id   \n",
       "0                                                   NaN              NaN  \\\n",
       "1     {'contributors': None, 'coordinates': None, 'c...     1.460879e+18   \n",
       "2                                                   NaN              NaN   \n",
       "3                                                   NaN              NaN   \n",
       "4                                                   NaN              NaN   \n",
       "...                                                 ...              ...   \n",
       "5095                                                NaN              NaN   \n",
       "5096                                                NaN              NaN   \n",
       "5097                                                NaN              NaN   \n",
       "5098                                                NaN              NaN   \n",
       "5099                                                NaN              NaN   \n",
       "\n",
       "      quoted_status_id_str                            quoted_status_permalink   \n",
       "0                      NaN                                                NaN  \\\n",
       "1             1.460879e+18  {'display': 'twitter.com/mah_sadeghi/stâ€¦', 'ex...   \n",
       "2                      NaN                                                NaN   \n",
       "3                      NaN                                                NaN   \n",
       "4                      NaN                                                NaN   \n",
       "...                    ...                                                ...   \n",
       "5095                   NaN                                                NaN   \n",
       "5096                   NaN                                                NaN   \n",
       "5097                   NaN                                                NaN   \n",
       "5098                   NaN                                                NaN   \n",
       "5099                   NaN                                                NaN   \n",
       "\n",
       "                                      extended_entities  retweeted_status   \n",
       "0                                                   NaN               NaN  \\\n",
       "1                                                   NaN               NaN   \n",
       "2                                                   NaN               NaN   \n",
       "3                                                   NaN               NaN   \n",
       "4     {'media': [{'display_url': 'pic.twitter.com/8w...               NaN   \n",
       "...                                                 ...               ...   \n",
       "5095                                                NaN               NaN   \n",
       "5096                                                NaN               NaN   \n",
       "5097                                                NaN               NaN   \n",
       "5098                                                NaN               NaN   \n",
       "5099                                                NaN               NaN   \n",
       "\n",
       "             username  \n",
       "0      vahabzadeh_ali  \n",
       "1         mah_sadeghi  \n",
       "2            Ejei_com  \n",
       "3        GhazizadehSA  \n",
       "4     hosseini_social  \n",
       "...               ...  \n",
       "5095   vahabzadeh_ali  \n",
       "5096   vahabzadeh_ali  \n",
       "5097   vahabzadeh_ali  \n",
       "5098   vahabzadeh_ali  \n",
       "5099   vahabzadeh_ali  \n",
       "\n",
       "[5100 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(r'../Files/5100_new.xlsx')\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac0739",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "organizations = pd.read_excel(r'../Files/organizations.xlsx')\n",
    "organizations['name2'] = organizations['name2'].astype(str)\n",
    "name_of_organizations = organizations['name2'].iloc[:130].to_list()\n",
    "organization = []\n",
    "for n in name_of_organizations:\n",
    "    organization.append(n.rstrip())\n",
    "organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c0e81",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swears = pd.read_json(r'../Files/data_swear.json', encoding='utf-8')\n",
    "swear = swears['word'].to_list()\n",
    "swear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46830b73",
   "metadata": {},
   "source": [
    "# Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_like'] = [like for like in data['favorite_count']]\n",
    "\n",
    "data['tweet_retweet_count'] = [retweet for retweet in data['retweet_count']]\n",
    "\n",
    "data['tweet_length_word'] = [len(word_tokenize(t)) for t in data['full_text']]\n",
    "\n",
    "data['tweet_length_characters'] = [len(t) for t in data['full_text']]\n",
    "\n",
    "data['hashtags'] = data['full_text'].str.findall(\"(#[^#\\s]+)\")\n",
    "\n",
    "pattern = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)'\n",
    "data['urls']= data['full_text'].str.findall(pattern)\n",
    "\n",
    "data['tweet_num_hashtags'] = [len(h) for h in data['hashtags']]\n",
    "\n",
    "def extract_mention_set(text):\n",
    "    mention_list = re.findall(\"@([a-zA-Z0-9]{1,15})\", text)\n",
    "    return len(mention_list)\n",
    "\n",
    "data['tweet_num_mention'] = data['full_text'].apply(extract_mention_set)\n",
    "\n",
    "data['tweet_num_urls'] = [len(h) for h in data['urls']]\n",
    "\n",
    "def extract_emojis(s):\n",
    "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "data['tweet_num_emoji'] = [len(extract_emojis(t)) for t in data['full_text']]\n",
    "\n",
    "data['tweet_emoji'] = [extract_emojis(t) for t in data['full_text']]\n",
    "\n",
    "persian_punctuation = '.ØŒØ›:()Â«Â»ØŸ![]-/'\n",
    "def extract_punctuation(s):\n",
    "  return ''.join(c for c in s if c in list(persian_punctuation))\n",
    "\n",
    "data['tweet_num_punctuation'] = [len(extract_punctuation(t)) for t in data['full_text']]\n",
    "\n",
    "list_o = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_o.append(0)\n",
    "    for name in organization:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_o[i] += 1\n",
    "data['organize_names'] = list_o\n",
    "\n",
    "list_sw = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_sw.append(0)\n",
    "    for name in swear:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_sw[i] += 1\n",
    "data['swear_words'] = list_sw\n",
    "\n",
    "cn = ['Ø±Ø¦ÛŒØ³ÛŒ' , 'Ø¬Ù„ÛŒÙ„ÛŒ' , 'Ø±ÛŒÛŒØ³ÛŒ' , 'Ù‡Ù…ØªÛŒ' , 'Ù…Ù‡Ø±Ø¹Ù„ÛŒØ²Ø§Ø¯Ù‡' , 'Ù…Ù‡Ø± Ø¹Ù„ÛŒØ²Ø§Ø¯Ù‡' , 'Ù…Ø­Ø³Ù† Ø±Ø¶Ø§ÛŒÛŒ' , 'Ù‚Ø§Ø¶ÛŒ Ø²Ø§Ø¯Ù‡' , 'Ù‚Ø§Ø¶ÛŒâ€ŒØ²Ø§Ø¯Ù‡',\n",
    "      'Ø²Ø§Ú©Ø§Ù†ÛŒ' , 'ÙˆÙ‡Ø§Ø¨â€ŒØ²Ø§Ø¯Ù‡' , 'ÙˆÙ‡Ø§Ø¨ Ø²Ø§Ø¯Ù‡' , 'Ø¨Ø·Ø­Ø§ÛŒÛŒ' , 'Ø­Ù†Ø§Ú†ÛŒ' , 'Ù…ÙˆÙ„Ø§ÙˆØ±Ø¯ÛŒ' , 'ØªØ§Ø¬ Ø²Ø§Ø¯Ù‡' , 'ØªØ§Ø¬â€ŒØ²Ø§Ø¯Ù‡' ,\n",
    "      'ØªØ§Ø¬Ø²Ø§Ø¯Ù‡' , 'Ù‚Ø§Ù„ÛŒØ¨Ø§Ù' , 'Ù‚Ø§Ù„ÛŒâ€ŒØ¨Ø§Ù' , 'Ù‚Ø§Ù„ÛŒ Ø¨Ø§Ù' , 'Ù…Ø­Ù…ÙˆØ¯ ØµØ§Ø¯Ù‚ÛŒ' , 'Ù…Ù‡Ø¯ÛŒ Ø§Ø³Ù…Ø§Ø¹ÛŒÙ„ÛŒ' ,\n",
    "      'Ù…Ø­Ù…Ø¯ Ø§Ø³Ù…Ø§Ø¹ÛŒÙ„ÛŒ' , 'Ú©ÙˆØ§Ú©Ø¨ÛŒØ§Ù†' , 'Ø¬ÙˆØ§Ø¯ Ø§ÙˆØ¬ÛŒ' , 'Ø®Ø§ØªÙ…ÛŒ' , 'Ø®Ø§Ù…Ù†Ù‡â€ŒØ§ÛŒ' , 'Ø®Ø§Ù…Ù†Ù‡ Ø§ÛŒ' , 'Ù…Ø­Ù…Ø¯ Ø­Ø³ÛŒÙ†ÛŒ', \n",
    "      'Ø­Ù…ÛŒØ¯ Ø³Ø¬Ø§Ø¯ÛŒ' , 'Ù…Ø¹ØµÙˆÙ…Ù‡ Ø§Ø¨ØªÚ©Ø§Ø±' , 'Ø³Ø§Ø¯Ø§ØªÛŒ Ù†Ú˜Ø§Ø¯' , 'Ø³Ø§Ø¯Ø§ØªÛŒâ€ŒÙ†Ú˜Ø§Ø¯' , 'Ù…Ø­Ù…Ø¯ Ø¯Ù‡Ù‚Ø§Ù†' , 'Ø¬Ù‡Ø±Ù…ÛŒ' , 'Ø¹Ø¨Ø¯Ø§Ù„Ù…Ù„Ú©ÛŒ' , 'Ù„Ø§Ø±ÛŒØ¬Ø§Ù†ÛŒ',\n",
    "      'Ø¹Ø±Ø§Ù‚Ú†ÛŒ' , 'Ø¶Ø±ØºØ§Ù…ÛŒ' , 'Ø±ÙˆØ­Ø§Ù†ÛŒ' , 'Ø±Ø³ØªÙ… Ù‚Ø§Ø³Ù…ÛŒ' , 'Ù…Ø±ØªØ¶ÙˆÛŒ' , 'Ù†ÙˆØ¨Ø®Øª' , 'Ù…ÛŒØ±Ú©Ø§Ø¸Ù…ÛŒ' , 'Ù…Ø®Ø¨Ø±' , 'Ø¸Ø±ÛŒÙ' , \n",
    "      'Ø¹Ø±Ø§Ù‚â€ŒÚ†ÛŒ' , 'Ø¹Ø±Ø§Ù‚ Ú†ÛŒ' , 'Ø²Ø§Ø±Ø¹ Ù¾ÙˆØ±' , 'Ø²Ø§Ø±Ø¹â€ŒÙ¾ÙˆØ±' , 'Ø¹ÛŒÙ† Ø§Ù„Ù„Ù‡ÛŒ' , 'Ø¹ÛŒÙ†â€ŒØ§Ù„Ù„Ù‡ÛŒ' , 'Ø¬Ù‡Ø§Ù†Ú¯ÛŒØ±ÛŒ' , 'Ø¬Ù‡Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ' ,\n",
    "      'Ø¬Ù‡Ø§Ù† Ú¯ÛŒØ±ÛŒ' , 'Ø®Ø²Ø¹Ù„ÛŒ' , 'Ø§Ú˜Ù‡â€ŒØ§ÛŒ' , 'ÙˆØ§Ø¹Ø¸ÛŒ' , 'Ø¨Ø§Ù‚Ø±ÛŒ Ú©Ù†ÛŒ' , 'Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ÛŒØ§Ù†' , 'Ø§Ø­Ù…Ø¯ÛŒâ€ŒÙ†Ú˜Ø§Ø¯' , 'Ø§Ø­Ù…Ø¯ÛŒ Ù†Ú˜Ø§Ø¯' , 'Ø¢Ø´ØªÛŒØ§Ù†ÛŒ' ,\n",
    "     'Ø¢Ù‚Ø§' , 'Ø¢Ù‚Ø§ÛŒ' , 'Ø®Ø§Ù†Ù…' , 'Ù…Ø³Ø¦ÙˆÙ„' , 'ÙˆØ²ÛŒØ±' , 'Ø¢Ø®ÙˆÙ†Ø¯' , 'Ø±Ù‡Ø¨Ø±' , 'Ø±Ù‡Ø¨Ø±ÛŒ' , 'Ø¢Ø®ÙˆÙ†Ø¯' , 'Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±' , 'Ø´Ù‡Ø±Ø¯Ø§Ø±']\n",
    "list_cn = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_cn.append(0)\n",
    "    for name in cn:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_cn[i] += 1\n",
    "data['person_names'] = list_cn\n",
    "\n",
    "'''def get_domain_from_url(url):\n",
    "  return urllib.parse.urlparse(url).netloc\n",
    "\n",
    "data['urls_dom'] = data['urls'].apply(lambda X: [get_domain_from_url(x) for x in X])\n",
    "url_list= []\n",
    "for i in range(len(data['urls_dom'])):\n",
    "    for url in data['urls_dom'].iloc[i]:\n",
    "        url_list.append(url)\n",
    "url_set = set(url_list)\n",
    "urls = []\n",
    "for url in url_set:\n",
    "    urls.append(url)\n",
    "if(len(urls) >= 10):\n",
    "    U = urls[:10]\n",
    "else:\n",
    "    U = urls\n",
    "for j in range(len(U)):\n",
    "    list_u = [0 for i in range(len(data))]\n",
    "    for i in range(len(data)):\n",
    "        if data['tweet_num_urls'].iloc[i] > 0:\n",
    "            if U[j] in data['full_text'].iloc[i]:\n",
    "                list_u[i] = 1\n",
    "    data[U[j]] = list_u.copy()'''\n",
    "\n",
    "data['tweet_emoji'] = [extract_emojis(t) for t in data['full_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2e5fb",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji_to_text(text, delimiters=(' ', ' ')):\n",
    "    \"\"\" Convert emojis to something readable by the vocab and model \"\"\"\n",
    "    text = emoji.demojize(text, delimiters=delimiters)\n",
    "    return text\n",
    "emo_list = []\n",
    "emoji_set = set(data[data['tweet_num_emoji']>0]['tweet_emoji'])\n",
    "for e in emoji_set:\n",
    "    for ee in e.strip():\n",
    "        emo_list.append(ee)\n",
    "emo_set = set(emo_list)\n",
    "emo_pic = []\n",
    "for e in emo_set:\n",
    "    emo_pic.append(e)\n",
    "emo = [convert_emoji_to_text(e) for e in emo_set]\n",
    "emo = [e.split('_') for e in emo]\n",
    "emo = [' '.join(e) for e in emo]\n",
    "emo_fa = []\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "for i in range(len(emo)):\n",
    "    trans = translator.translate(emo[i], dest='fa')\n",
    "    emo_fa.append(trans)\n",
    "emo_farsi = []\n",
    "for e in emo_fa:\n",
    "    emo_farsi.append(e.text)\n",
    "emo_farsi = [' ' + e + ' ' for e in emo_farsi]\n",
    "Emojis = dict(zip(emo_pic, emo_farsi))\n",
    "Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ffe3565e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    if data['tweet_num_emoji'].iloc[i] > 0:\n",
    "        for e in emo_pic:\n",
    "            if e in data['full_text'].iloc[i]:\n",
    "                data['full_text'].iloc[i] = data['full_text'].iloc[i].replace(e, Emojis[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6e5132ab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Ø§Ù‚ØªØµØ§Ø¯ Ø§Ø³Ù„Ø§Ù…ÛŒ ',\n",
       " ' Ø¹Ø¨Ø±Øª Ø§ÙˆÚ©Ø±Ø§ÛŒÙ† ',\n",
       " ' ØªÙˆØ±Ù… ',\n",
       " ' Ø§Ù†Ø¯ÛŒÚ©Ø§ ',\n",
       " ' Ø³ÙˆØ±ÛŒÙ‡ Ù†Ù…ÛŒØ´ÙˆÛŒÙ… ',\n",
       " ' Ø­Ø¶Ø±Øª Ù…Ø­Ù…Ø¯ ',\n",
       " ' Ù…Ø§Ø¯Ø±Ø§Ù† ØªØ­ÙˆÙ„ Ø³Ø§Ø² ',\n",
       " ' Ø¹Ø±Ø¨Ø³ØªØ§Ù† ',\n",
       " ' Ø³ÛŒØ¯Ù…Ø­Ù…Ø¯Ø®Ø§ØªÙ…ÛŒ: ',\n",
       " ' Ù…Ø§ÙÛŒØ§ÛŒ ÙˆØ§Ø±Ø¯Ø§Øª ',\n",
       " ' Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø¹Ù„ÙˆÙ… Ùˆ ØªØ­Ù‚ÛŒÙ‚Ø§Øª ',\n",
       " ' Ù…Ø¯ÛŒØ±ÛŒØª Ø³Ù†Ø¬Ø§Ù‚ÛŒ ',\n",
       " ' ÙˆÙ„ÛŒ Ù†Ø¹Ù…ØªØ§Ù† Ø²Ù…Ø§Ù…Ø¯Ø§Ø±Ø§Ù† ',\n",
       " ' Ù‚Ø§Ø³Ù… Ø³Ù„ÛŒÙ…Ø§Ù†ÛŒ ',\n",
       " ' Ø±Ù‡Ù†ÙˆØ±Ø¯ ',\n",
       " ' Ø¢Ø²Ù…ÙˆÙ† Ø¹Ø¯Ø§Ù„Øª ',\n",
       " ' Ø§Ù…ØªÛŒ ØªØ±ÛŒÙ† ',\n",
       " ' Ø¨Ù‚Ø§ÛŒÛŒ ',\n",
       " ' Ø±ÙˆØ­Ø§Ù†ÛŒ ',\n",
       " ' Ø§Ø³Ù„Ø§Ù…Ø´Ù‡Ø± ',\n",
       " ' Ú¯Ø²Ø§Ø±Ø´ Ø¨ÙˆØ¯Ø¬Ù‡ ',\n",
       " ' Ø´ÙØ§ÙÛŒØª Ø¢Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒÙ†Ø¯Ú¯Ø§Ù† ',\n",
       " ' Ø§Ù‚Ø¯Ø§Ù… Ùˆ ',\n",
       " ' Ø¢ÛŒØª Ø§Ù„Ù„Ù‡ Ø±Ø¦ÛŒØ³ÛŒ ',\n",
       " ' Ø®Ø§Ù† Ù‡ÙØªÙ… ',\n",
       " ' ÙƒØ±Ù‡ Ø¬Ù†ÙˆØ¨ÙŠ ',\n",
       " ' Ø¬Ù…Ù†Ø§ ',\n",
       " ' Ø®Ø³Ø§Ø±Øª Ù…Ø­Ø¶ ',\n",
       " ' Ù†Ø´Ø± Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ ',\n",
       " ' Ø®ÙˆØ²Ø³ØªØ§Ù† ',\n",
       " ' Ø¨Ø§Ù†Ú© Ù‡Ø§ ',\n",
       " ' Ø®Ø±ÛŒØ¯ Ù…Ù„Ú© ',\n",
       " ' Ø§Ù…Ù†ÛŒØª ',\n",
       " ' Ù…ÛŒØ¯ÙˆÙ† Ù…Ø±Ø¯Ù… ',\n",
       " ' Ù†ÙˆØ±ÙˆØ²Û±Û´Û°Û° ',\n",
       " ' ÙØ³Ø§Ø¯. ',\n",
       " ' Ø§Ù…Ø§Ù… Ø®Ø§Ù…Ù†Ù‡ Ø§ÛŒ ',\n",
       " ' Ø´ÛŒØ¹Ù‡ Ø³Ù†ÛŒ ',\n",
       " ' Ù¾Ø±Ø³ØªÙˆ ',\n",
       " ' Ø³Ù†Ú¯ Ù‚Ø¨Ø± ',\n",
       " ' Ø·Ù„Ø§ ',\n",
       " ' Ø±Ù‡Ø¨Ø± ',\n",
       " ' Ù…Ø³Ù„Ù…Ø§Ù†ØŒ ',\n",
       " ' Ù…Ø±Ø¯Ù…ÛŒ Ø³Ø§Ø²ÛŒ ',\n",
       " ' Ø¢Ù…Ø±ÛŒÚ©Ø§! ',\n",
       " ' Ø¬Ù†Ø§ÛŒØª ØªØ±ÙˆØ±ÛŒØ³ØªÛŒ Ø§Ù‡ÙˆØ§Ø² ',\n",
       " ' Ù…ÙˆØ´Ú© Ù‡Ø§ÛŒ Ø¨Ø§Ù„Ø³ØªÛŒÚ© ',\n",
       " ' Ø¯ÙˆÙ„Øª Ù¾Ø§Ø³Ø®Ú¯Ùˆ ',\n",
       " ' Ø§Ù„Ø¨Ø­Ø±ÙŠÙ† ',\n",
       " ' Ù…Ø¸Ù„ÙˆÙ… ',\n",
       " ' Ø¢ØªØ´ Ø³ÙˆØ²ÛŒ Ø²Ø§Ù‡Ø¯Ø§Ù† ',\n",
       " ' Ù†Ø¸Ø§Ù… Ø³Ù„Ø·Ù‡ ',\n",
       " ' Ø¨Ø§Ø²Ù†Ú¯Ø±ÛŒ Ù‚Ø§Ù†ÙˆÙ† Ø§Ø³Ø§Ø³ÛŒ ',\n",
       " ' Ù…Ø³ÙƒÙ† ',\n",
       " ' Ø±Ø£ÛŒ ',\n",
       " ' Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø²Ø§Ú©Ø§Ù†ÛŒ ',\n",
       " ' Ø¢Ø´ØªÛŒ Ù…Ù„ÛŒ ',\n",
       " ' Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¨Ø§Ø² ',\n",
       " ' Ø´Ø¹Ø§Ø± ',\n",
       " ' Ú©Ø±Ø¨Ù„Ø§. ',\n",
       " ' Ø¬Ø±ÛŒØ§Ù† ØªØ­Ø±ÛŒÙ ',\n",
       " ' ØªØ­ÙˆÙ„ ',\n",
       " ' Ø¨ÛŒ Ø§Ù†ØµØ§ÙÛŒ ',\n",
       " ' Ø¬Ù‡Ø´ Ø§Ù‚ØªØµØ§Ø¯ÛŒ ',\n",
       " ' Ø§Ø¨ØªÚ©Ø§Ø± ',\n",
       " ' ØªÙˆØ§ÙÙ‚ØŒ ',\n",
       " ' Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Û±Û´Û°Û° ',\n",
       " ' Ù…Ø¬Ù…Ø¹ ØªØ´Ø®ÛŒØµ Ù…ØµÙ„Ø­ØªØŒ ',\n",
       " ' Ø³Ø§Ù…Ø§Ù†Ù‡  ',\n",
       " ' ØµØ¯Ø§ÛŒ Ù…Ø±Ø¯Ù… ',\n",
       " ' ÙÙ„Ø³Ø·ÛŒÙ†! ',\n",
       " ' Ø±Ø´Øª ',\n",
       " ' Ú©ÙˆØ¯Ú© Ù‡Ù…Ø³Ø±ÛŒ ',\n",
       " ' Ù…Ø­Ø³Ù†ÛŒ Ø§Ú˜Ù‡ Ø§ÛŒ ',\n",
       " ' Ù¾Ø§Ø³Ø®Ú¯ÙˆÙŠÙŠ ',\n",
       " ' Ø§Ù‡ÙˆØ§Ø² ',\n",
       " ' Ø±Ø­Ù…Øª ',\n",
       " ' Ø§Ù„ÙˆØ¯Ú¯ÛŒ Ù‡ÙˆØ§ ',\n",
       " ' Ù†Ù‚Ø¶ Ø¨Ø±Ø¬Ø§Ù… ',\n",
       " ' Ø¯Ø§Ø¯Ú¯Ø§Ù‡ Ø¹Ù„Ù†ÛŒ ',\n",
       " ' Ø¯Ø®ØªØ± ',\n",
       " ' Ø¢Ù…Ø±ÙŠÙƒØ§ ',\n",
       " ' Ø±ÛŒÛŒØ³ Ø¬Ù…Ù‡ÙˆØ± ',\n",
       " ' Ø­Ø³Ù† ÛŒØ²Ø¯Ø§Ù†ÛŒ ',\n",
       " ' Ø§ÛŒØ±Ø§Ù† Ù‚ÙˆÛŒØŒ ',\n",
       " ' Ù…Ù‚Ø§ÙˆÙ…Øª ',\n",
       " ' Ø¨Ø³ÛŒØ¬ÛŒ ',\n",
       " ' ÙˆØ§Ø±Ø¯Ø§Øª ',\n",
       " ' Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒ ',\n",
       " ' Ù¢Ù¨ Ù…Ø±Ø¯Ø§Ø¯ ',\n",
       " ' Ø§Ø²Ø¯ÙˆØ§Ø¬ ',\n",
       " ' Ø¯Ø±ÙˆØº Ù…Ù…Ù†ÙˆØ¹ ',\n",
       " ' Ù…Ø­Ù…ÙˆØ¯ÛŒÙ‡ ',\n",
       " ' Ù…Ø±Ø®ØµÛŒ ',\n",
       " ' Ù…Ø°Ø§Ú©Ø±Ù‡ ',\n",
       " ' Ø®Ù…ÛŒÙ†ÛŒ ',\n",
       " ' Ø´ÛŒØ¹Ù‡ ',\n",
       " ' Ù…Ø§Ù†Ø¹ Ø²Ø¯Ø§ÛŒÛŒ ',\n",
       " ' Ù†Ø¸Ø§Ù… Ø§Ø³Ù„Ø§Ù…ÛŒØŒ ',\n",
       " ' Ø³Ø§Ø­Ù„ ',\n",
       " ' Ø³ÛŒÙ†Ù…Ø§ÛŒÛŒ ',\n",
       " ' Ø±ÙØ§Ù‡ ',\n",
       " ' ØªØ§Ø«ÛŒØ± Ù‚Ø·Ø¹ÛŒ Ù…Ø¹Ø¯Ù„ ',\n",
       " ' Ø·Ø¨Ø±ÛŒ ',\n",
       " ' Ø§ÙˆÚ©Ø±Ø§ÛŒÙ† ',\n",
       " ' Ø¯Ø±Ø³Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ® ',\n",
       " ' ØµØ±Ø¨Ø³ØªØ§Ù†Ø› ',\n",
       " ' Ø¬Ù‡Ø§Ù†Ú¯ÛŒØ±ÛŒ ',\n",
       " ' Ø¬Ø¨Ø±Ø§Ù† ',\n",
       " ' Ø³Ø§Ø²Ù…Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨ÙˆØ¯Ø¬Ù‡ ',\n",
       " ' Ø±Ø¬Ø§Ù„ ',\n",
       " ' Ù…Ù†Ø§Ø¸Ø±Ù‡Û±Û´Û°Û° ',\n",
       " ' Ø§Ø¹Ù„ÛŒØ­Ø¶Ø±ØªÛŒ ',\n",
       " ' ØªÛŒÙ…Û±Û±Ù†ÙØ±Ù‡ ',\n",
       " ' Ø¬ÙˆØ§Ù†Ø§Ù† ',\n",
       " ' Ø§ÙŠØ±Ø§Ù† Ù„Ù„Ø´Ø¹Ø¨ Ø¨ØµÙ…Ø© ',\n",
       " ' Ù‡Ø§Ø´Ù…ÛŒ Ø±ÙØ³Ù†Ø¬Ø§Ù†ÛŒ ',\n",
       " ' iran ',\n",
       " ' ØªØ¬Ø±Ø¨Ù‡ Ø¯ÙˆÙ„Øª Ø¯ÙˆØ§Ø²Ø¯Ù‡Ù… ',\n",
       " ' Ú©ÙˆØ¯ØªØ§  Ø¹Ù„ÛŒÙ‡ Ø¬Ù…Ù‡ÙˆØ±ÛŒØª ',\n",
       " ' Ø¢Ù„ Ø³Ø¹ÙˆØ¯ ',\n",
       " ' Ø­Ú©ÙˆÙ…Øª ',\n",
       " ' UNGA ',\n",
       " ' Ø²Ø¨Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ',\n",
       " ' Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ ',\n",
       " ' Ø¢Ø¨Ø§Ù†Û¹Û¸ ',\n",
       " ' ØªÙˆØ¨Ù‡ ',\n",
       " ' Ø¢Ø²Ø§Ø¯ ',\n",
       " ' Ù‡Ù…ØªÛŒ! ',\n",
       " ' Ø³Ù¾Ø§Ù‡ ',\n",
       " ' Ø§Ø³ØªØ§Ù† ÙØ§Ø±Ø³ ',\n",
       " ' Ù…Ø±Ø¯Ù… Ú¯Ù„Ù‡ Ù…Ù†Ø¯Ù†Ø¯ ',\n",
       " ' ÛŒØ§Ø±Ø§Ù†Ù‡ ',\n",
       " ' Ú©Ø±Ù…Ø§Ù† ',\n",
       " ' Riyadh ',\n",
       " ' Ø§Ø·Ù„Ø§Ø¹ÛŒÙ‡ ',\n",
       " ' Ø³ÛŒØ¯Ù…Ø­Ù…Ø¯Ø®Ø§ØªÙ…ÛŒØŒ ',\n",
       " ' Ú©Ø§ÙØ± Ø¹Ø§Ø¯Ù„ ',\n",
       " ' ÙˆÛŒØ±ÙˆØ³ ',\n",
       " ' Ù„Ø§Ø±ÛŒØ¬Ø§Ù†ÛŒ ',\n",
       " ' ØªÙˆØ²ÛŒØ¹ Ø¹Ø§Ø¯Ù„Ø§Ù†Ù‡ ',\n",
       " ' Ø±ÙˆØ²Ù†Ø§Ù…Ù‡ Ù†Ú¯Ø§Ø±Ø§Ù†ØŒ ',\n",
       " ' Ø§Ù†ØªØµØ§Ø¨Ø§Øª Ø±Ù‡Ø¨Ø±ÛŒ ',\n",
       " ' Ø³Ø§Ø²Ù…Ø§Ù† Ù¾Ø²Ø´Ú©Ø§Ù† Ø¨Ø¯ÙˆÙ† Ù…Ø±Ø² ',\n",
       " ' Ø§ØµÙÙ‡Ø§Ù† ',\n",
       " ' Ø¯Ø§Ù†Ø´Ø¬Ùˆ ',\n",
       " ' Ù¾ÛŒØ§Ù…Ø¨Ø± Ø§Ú©Ø±Ù…(Øµ) ',\n",
       " ' Ø¬ÙØ§ ',\n",
       " ' Ø§ØµÙ„Ø§Ø­ ',\n",
       " ' Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø¯Ø§Ø±ÛŒ ',\n",
       " ' Ø§Ù„Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ø±Ø¨ÛŒØ© ',\n",
       " ' Ø§Ù†ØªØ®Ø§Ø¨Ø§ØªÛ¹Û´ ',\n",
       " ' Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨ÙˆØ¯Ø¬Ù‡ ',\n",
       " ' Ù‚Ø§Ú†Ø§Ù‚ ',\n",
       " ' Ø·Ø±Ø­ ØµÛŒØ§Ù†ØªØŒ ',\n",
       " ' ØªØ±Ø¯ÛŒØ¯ØŒ ',\n",
       " ' Ø±ÙˆØ³ÙŠØ§ ',\n",
       " ' Ø§ÛŒØ±Ø§Ù† Ù‚ÙˆÛŒ ',\n",
       " ' Ø´ÙˆØ±Ø§Ù‡Ø§ÛŒ Ø´Ù‡Ø± ',\n",
       " ' Ø§Ù…Ø§Ù… Ø®Ù…ÛŒÙ†ÛŒ ',\n",
       " ' Ø§Ø¹ØªØ±Ø§Ù ',\n",
       " ' Ø­Ù‚ Ø§Ù„Ù†Ø§Ø³ ',\n",
       " ' Ø³ÛŒØ³ØªØ§Ù†ÛŒ ',\n",
       " ' 16Day ',\n",
       " ' Ø¨Ø§Ú©Ø±ÛŒ) ',\n",
       " ' Ø³ÛŒØ§Ø³Øª Ø²Ø¯Ú¯ÛŒ ',\n",
       " ' Ø¬Ù‡Ø´ ',\n",
       " ' islam ',\n",
       " ' Ù…Ø³Ø¤Ù„ÛŒØª Ú©ÛŒÙØ±ÛŒ ',\n",
       " ' Ø§Ù„Ø±ÛŒØ§Ø¶ ',\n",
       " ' ØµØ§Ù„Ø­ÙŠ ',\n",
       " ' ÙˆØ²ÛŒØ± Ú©Ø´ÙˆØ±: ',\n",
       " ' Ø¢Ù„ ',\n",
       " ' Ø¯Ø§ÙˆÙˆØ³ ',\n",
       " ' Ù…Ø¹ÛŒØ´Øª ',\n",
       " ' Ù‡Ù†Ø¯Ø¨Ø§Ù„ Ø²Ù†Ø§Ù† ',\n",
       " ' Ù†Ø´Ø§Ø· Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ ',\n",
       " ' ÙˆØ§Ù… Ø®Ø±Ø¯ ',\n",
       " ' ØºØ²Ø© ',\n",
       " ' Ø·Ø§Ù„Ù‚Ø§Ù†ÛŒ ',\n",
       " ' Ù†Ù‡ Ø¬Ù†Ú¯ Ù†Ù‡ Ù…Ø°Ø§Ú©Ø±Ù‡ ',\n",
       " ' Ø´Ø±Ù‚ ',\n",
       " ' Ú¯Ø±Ø§Ù†ÛŒ Ø´Ø¨ Ø¹ÛŒØ¯ ',\n",
       " ' ØªØ¨Ù„Øª ',\n",
       " ' Ø³Ø¹ÙˆØ¯ÛŒ ',\n",
       " ' Ø®Ø§ÙˆØ±Ù…ÛŒØ§Ù†Ù‡ ',\n",
       " ' ÛŒÙ…Ù† ',\n",
       " ' Iranelection ',\n",
       " ' ÙˆÛŒØ¯Ø¦ÙˆÛŒÛŒ ',\n",
       " ' Ø§Ù„ØµÙŠÙ† ',\n",
       " ' Ø¯Ù‡Ù‡60ØŒ ',\n",
       " ' Ø§Ù…Ø±ÛŒÚ©Ø§ ',\n",
       " ' Ø±Ø´ØªÙˆ ',\n",
       " ' Ù…Ø·Ø§Ø± ',\n",
       " ' ØµØ¯Ø±Ø§Ù„Ø³Ø§Ø¯Ø§ØªÛŒ ',\n",
       " ' Ø­ØµØ± ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ ',\n",
       " ' ØªØ­Ù„ÛŒÙ: ',\n",
       " ' Ø§ØªÙˆÚ©Ø´ÛŒØ¯Ù‡ ',\n",
       " ' ØªØ­Ø±ÛŒÙ…  ',\n",
       " ' Ù‚Ø§Ù†ÙˆÙ† Ø§Ø³Ø§Ø³ÛŒ ',\n",
       " ' Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒ Ø§ÛŒØ±Ø§Ù† ',\n",
       " ' Ù…Ù„ÛŒ ØªØ±ÛŒÙ† ',\n",
       " ' Ø¨Ù† Ø³Ù„Ù…Ø§Ù† ',\n",
       " ' Ø¬Ù†Ú¯ Ù†ÛŒØ§Ø¨ØªÛŒ ',\n",
       " ' Ø§Ù‡Ø§Ù†Øª Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† ',\n",
       " ' NCAA ',\n",
       " ' Ø¯Ù„ÙˆØ§Ù¾Ø³Ø§Ù† ',\n",
       " ' Ø±Ø¦ÛŒØ³ Ø³ØªØ§Ø¯Ø§   Ù Ø§Ù…Ø§Ù…)Ù…ÛŒ Ø®ÙˆØ§Ù‡Ù… ',\n",
       " ' Ø®Ø¨Ø±Ø®ÙˆØ¨! ',\n",
       " ' Ú©Ø§Ù„Ø§ ',\n",
       " ' Ø¢Ø±Ù…Ø§Ù† ',\n",
       " ' Ø¢Ø²Ø§Ø¯ÛŒ Ù…Ø®Ø§Ù„Ù ',\n",
       " ' Ø·Ø±Ø­ Ù…Ù„ÛŒ Ø¨ÛŒÙ…Ù‡ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ ',\n",
       " ' ÙƒÙ„Ø§Ø¨ Ù‡Ø§ÙˆØ³ ',\n",
       " ' Ø·Ø±Ø­ ØªØ³Ù‡ÛŒÙ„ ØµØ¯ÙˆØ± Ù…Ø¬ÙˆØ²Ù‡Ø§ÛŒ Ú©Ø³Ø¨ ÙˆÚ©Ø§Ø± ',\n",
       " ' Ø¯Ø±ÙˆØº ',\n",
       " ' Ø±Ø­Ù…Ù‡ Ù„Ù„Ø¹Ø§Ù„Ù…ÛŒÙ† ',\n",
       " ' Ø§Ù†Ù‚Ù„Ø§Ø¨ÛŒ ',\n",
       " ' ÙÙ‚Ù‡Ø§ ',\n",
       " ' Ø®ÙˆØ²Ø³ØªØ§Ù† ØªÙ†Ù‡Ø§ Ù†ÛŒØ³Øª ',\n",
       " ' ØªØ®Ø±ÛŒØ¨ÛŒ ',\n",
       " ' Ø¯Ø§Ø¯Ú¯Ø§Ù‡ ',\n",
       " ' Ø¢Ù† Ú†Ù†Ø§Ù† ',\n",
       " ' Ø®Ø±Ø§Ø³Ø§Ù† Ø´Ù…Ø§Ù„ÛŒ ',\n",
       " ' Ø¨Ø¯Ø§Ø®Ù„Ø§Ù‚ÛŒ ',\n",
       " ' Ú©ÙˆØ¯ØªØ§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨Ø§ØªÛŒ ',\n",
       " ' Ù…Ø¬Ø§Ø²ÛŒ ',\n",
       " ' Ø§ÙØºØ§Ù†Ø³ØªØ§Ù† ',\n",
       " ' Ø§Ø®Ù„Ø§Ù‚ ',\n",
       " ' Ø§ÛŒØ±Ø§Ù†\\u2066ğŸ‡®ğŸ‡· ',\n",
       " ' Ø§Ø­Ù…Ø¯ Ù…Ø³Ø¹ÙˆØ¯ ',\n",
       " ' Ø§Ù‚ØªØµØ§Ø¯ Ù‚ÙˆÛŒ ',\n",
       " ' Ú¯Ø±Ø³Ù†Ú¯ÛŒ ',\n",
       " ' Ø§Ù„Ø¥Ù…Ø±ÛŒÚ©ÛŒØ© ',\n",
       " ' ØµØ¯ÛŒÙ‚ÛŒ!Ø¨Ù‡ ',\n",
       " ' Ú†ÛŒÙ† ',\n",
       " ' Ø¬Ù‡Ø§Ù†Ú¯ÛŒØ±ÛŒØŒ ',\n",
       " ' Ú¯Ø§Ø±Ø§Ù†ØªÛŒ ',\n",
       " ' Ø±ÛŒÛŒØ³ Ø¬Ù…Ù‡ÙˆØ± Ø§Ù‚ØªØµØ§Ø¯Ø¯Ø§Ù† ',\n",
       " ' Ø´Ø¨Ù‡ Ø¹Ù…Ø¯) ',\n",
       " ' Ø´Ù‡Ø±Ø¯Ø§Ø± ØªÙ‡Ø±Ø§Ù† ',\n",
       " ' Ø¯ÙˆÚ¯Ø§Ù†Ù‡ Ù…ØªØ¹Ø§Ø±Ø¶ ',\n",
       " ' ØªÙÚ©Ø±Ø§Ø± ',\n",
       " ' Ø³Ø±Ø¯Ø§Ø± ',\n",
       " ' Ø­Ù„Ø¨Ú†Ù‡ ',\n",
       " ' Ø±Ø§Ø¶ÙŠ ',\n",
       " ' Ø­Ø°Ù ',\n",
       " ' Ù…Ø§Ø¯Ø± ',\n",
       " ' Ø¬ÙˆØ§Ù† ',\n",
       " ' ØªØ§Ø±ÛŒØ® ',\n",
       " ' Ø­Ø§Ø¬ Ø§Ø­Ù…Ø¯ Ú©Ø§Ø¸Ù…ÛŒ ',\n",
       " ' Ø§Ø³ØªØ§Ù† Ù‡Ø±Ù…Ø²Ú¯Ø§Ù† ',\n",
       " ' fatf ',\n",
       " ' Ø±Ø¦ÛŒØ³ Ø¬Ù…Ù‡ÙˆØ± ',\n",
       " ' Ø¬Ø±Ù… Ø³ÛŒØ§Ø³ÛŒ ',\n",
       " ' Ø²Ù†Ø¬Ø§Ù† ',\n",
       " ' ØªÙˆÙ„ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† ',\n",
       " ' Ø²Ù† ',\n",
       " ' Ù…Ø¬Ù„Ø³ ',\n",
       " ' Ø¨Ù‡ Ø¹Ù…Ù„ Ú©Ø§Ø± Ø¨Ø±Ø¢ÛŒØ¯. ',\n",
       " ' Ø§Ù…ÙŠØ¯ ',\n",
       " ' Ø­Ù‚ Ø§Ù†ØªØ®Ø§Ø¨ ',\n",
       " ' Ø¯Ø²Ø¯Ø§Ù† Ø¯Ø±ÛŒØ§ÛŒÛŒ ',\n",
       " ' Ø´Ø±ÙˆØ· Ø¶Ù…Ù† Ø¹Ù‚Ø¯ ',\n",
       " ' ØªÙØ§Ù‡Ù… ',\n",
       " ' Ø­Ú©Ù…Ø±Ø§Ù†ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ ',\n",
       " ' Ù…Ø­Ø³Ù† ÙØ®Ø±ÛŒ Ø²Ø§Ø¯Ù‡ ',\n",
       " ' Ø¯Ù†Ø§Ù¾Ù„Ø§Ø³ ',\n",
       " ' Ø³Ø±Ø¯Ø§Ø± Ø¯Ù„Ù‡Ø§ ',\n",
       " ' Ø¬Ù†Ø³ÛŒØª ',\n",
       " ' Ø­ÙØ¸ Ø§Ù…Ù†ÛŒØª Ø±ÙˆØ§Ù†ÛŒ Ø¬Ø§Ù…Ø¹Ù‡ ',\n",
       " ' ÙÛŒØ±ÙˆØ²Ø¬Ø§ ',\n",
       " ' Ù…Ø¬Ù„Ø³ØŒ ',\n",
       " ' Ù…Ø¯ÛŒØ±Ø§Ù† Ø²Ù† ',\n",
       " ' Ø±Ø¦ÛŒØ³ÛŒØŒ ',\n",
       " ' Ù…Ø³Ø¹ÙˆØ¯Ø´Ø¬Ø§Ø¹ÛŒ ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø¬ÙˆØ§Ù† Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡ÛŒ ',\n",
       " ' NeverThreatenAnIranian. ',\n",
       " ' Ø¹Ù…Ù„Ú©Ø±Ø¯Ù‡Ø§ ',\n",
       " ' ÛŒØ§Ø± Ú©Ù…Ú©ÛŒ ',\n",
       " ' Ø¨Ø²Ù†ÛŒÙ… Ø²ÛŒØ± Ù…ÛŒØ² ',\n",
       " ' Ú©Ø§Ù„Ø§ Ø¨Ø±Ú¯ ',\n",
       " ' Ø¢Ø¨Ø²ÛŒØ§Ù† ',\n",
       " ' Ø±Ø¯ØµÙ„Ø§Ø­ÛŒØª ',\n",
       " ' ØªÙˆØ³Ø¹Ù‡ Ø³ÙˆØ§Ø¯ Ù…Ø­ÛŒØ· Ø²ÛŒØ³ØªÛŒ ',\n",
       " ' Ø®ÙˆØ§Ø³Øª Ø¹Ù…ÙˆÙ…ÛŒ ',\n",
       " ' Ú©Ø§Ø³Ø¨Ø§Ù† Ø±Ø§ÛŒ ',\n",
       " ' Ù†Ø¬ÙÛŒ ',\n",
       " ' Ø­Ø±Ù Ù…Ø±Ø¯Ù… ',\n",
       " ' Ø§Ù…ÛŒØ±Ú©Ø¨ÛŒØ±:Ø­Ø±Ú©Øª ',\n",
       " ' Ù¾Ø§ÛŒØ§Ù† ',\n",
       " ' ØªØºÙŠÛŒØ±Ø§Øª Ù…Ù„Ù…ÙˆØ³ ',\n",
       " ' Ù…Ø¨Ø§Ø±Ú© ',\n",
       " ' Ù‡Ø§Ø´Ù…ÛŒ ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø³ÙˆÙ… Ø±ÙˆØ­Ø§Ù†ÛŒ ',\n",
       " ' Ù…ØµÙ„Ø­Øª  ',\n",
       " ' letter4u ',\n",
       " ' Ø±ÙˆØ³ØªØ§ Ø±Ø§ Ø­Ø°Ù Ù†Ú©Ù†ÛŒØ¯ ',\n",
       " ' Ø¯Ú©ØªØ± Ø·Ø­Ø§Ù† Ù†Ø¸ÛŒÙ ',\n",
       " ' Ø´Ù‡ÛŒØ¯ Ú¯Ù†Ø¬ÛŒ ',\n",
       " ' ØªÙˆÙ„ÛŒØ¯ Ù…Ù„ÛŒ ',\n",
       " ' Ú©Ø§Ø±Ú¯Ø±Ø§Ù† ',\n",
       " ' Ø­Ù„Ù‚Ù‡ Ø¨Ø³ØªÙ‡ ',\n",
       " ' Ø§Ù†ØªØ®Ø§Ø¨Ø§ØªÛ±Û´Û°Û° ',\n",
       " ' Ø¢Ù…Ù„ÛŒ Ù„Ø§Ø±ÛŒØ¬Ø§Ù†ÛŒ ',\n",
       " ' ØªØ§Ø¬Ø²Ø§Ø¯Ù‡ ',\n",
       " ' Ù…Ú©Ù…Ù„ Ù†Ú¯Ø§Ù‡ Ø¢Ù…Ø±ÛŒÚ©Ø§ÛŒÛŒ ',\n",
       " ' Ø§ÙŠØ±Ø§Ù† ',\n",
       " ' Ø³ÛŒÙ„ÛŒ ',\n",
       " ' Ø§Ù…Ø§Ù… ØµØ§Ø¯Ù‚(Ø¹) ',\n",
       " ' Ø³ÙˆØ±ÙŠØ© ',\n",
       " ' Ø§Ù†Ù‚Ù„Ø§Ø¨ ',\n",
       " ' ÙØµÙ„ Ù‡ÙØªÙ… ',\n",
       " ' Ø¶Ø¯ Ø§Ù‚ØªØµØ§Ø¯ Ù…Ù‚Ø§ÙˆÙ…ØªÛŒ ',\n",
       " ' Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª Ù…Ø³Ø§Ù„Ù…Øª Ø¢Ù…ÛŒØ² ',\n",
       " ' Ù¾Ø§ÛŒØ§Ù† ÙˆØ³ ',\n",
       " ' ÙØ±Ø¯Ø§ Ø¨Ø±Ø§ÛŒ Ø´Ù…Ø§Ø³Øª ',\n",
       " ' Ù…Ù†Ø§Ø¸Ø±Ù‡ Ø³ÙˆÙ… ',\n",
       " ' Ù¾Ø§Ø³Ø®Ú¯ÙˆÛŒÛŒ ',\n",
       " ' Ø¨Ø§Ù†Ú© ',\n",
       " ' Ø®ÙˆØ¯Ø±ÙˆØ³Ø§Ø²Ø§Ù† Ø¬ÙˆØ§Ù† ',\n",
       " ' Ø§Ù†ØªØ®Ø§Ø¨Ø§ØªØŒ ',\n",
       " ' Ù‡Ù… Ø§Ú©Ù†ÙˆÙ† ',\n",
       " ' Ù†Ø§ØµØ± Ù…Ù„Ú© Ù…Ø·ÛŒØ¹ÛŒ ',\n",
       " ' election94 ',\n",
       " ' Ø³ÙˆÙ… Ø®Ø±Ø¯Ø§Ø¯ ',\n",
       " ' Ø³Ø§Ø²Ø´ ',\n",
       " ' Ø¨ÛŒÙ†ÙˆØ§ÛŒØ§Ù† ',\n",
       " ' Ø¢Ø²Ø§Ø¯ÛŒ ',\n",
       " ' Ø¯Ø±ÙˆØºØŒ ',\n",
       " ' Ø¨Ù„Ø§ÛŒ Ø¨Ø²Ø±Ú¯ ',\n",
       " ' Ø§ØµÙ„Ø§Ø­ Ø·Ù„Ø¨ØŒ ',\n",
       " ' ØµØ§ÙÛŒ Ú¯Ù„Ù¾Ø§ÛŒÚ¯Ø§Ù†ÛŒ ',\n",
       " ' Ø§Ù…Ù†ÛŒØª Ù…Ù„ÛŒ ',\n",
       " ' Nowruz ',\n",
       " ' Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª: ',\n",
       " ' Ù‡Ø± Ù†ÙØ± Ø¯Ù‡ Ù†ÙØ± ',\n",
       " ' Û±Û¶ Ø¢Ø°Ø± ',\n",
       " ' Ø§Ù†Ø­ØµØ§Ø± Ø§Ù‚Ù„ÛŒØª ',\n",
       " ' Ù†Ù‡ Ø¬Ù†Ú¯ Ù†Ù‡ ØªØ­Ø±ÛŒÙ…\\u2069 ',\n",
       " ' ØµØ§Ù„Ø­Ø§Ù† ',\n",
       " ' Ù…Ù†Ø§ÙÙ‚ÛŒÙ† ',\n",
       " ' Ø§ÙŠØ±Ø§Ù†ÙŠØ§Ù† ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø³Ø§ÛŒÙ‡\" ',\n",
       " ' ØªÙˆÙ„ÛŒØ¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù‡Ø§ Ù…Ø§Ù†Ø¹ Ø²Ø¯Ø§ÛŒÛŒ Ù‡Ø§ ',\n",
       " ' Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Û±Û´Ù Ù  ',\n",
       " ' Ø²Ù†ØŒÛŒÚ© ',\n",
       " ' Ù…Ø®Ø§Ø·Ø±Ù‡ ',\n",
       " ' Ú©Ø¯Ø®Ø¯Ø§ ',\n",
       " ' Ø®Ø§ØªÙ…ÛŒ Ø­Ø°Ù Ø´Ø¯Ù†ÛŒ Ù†ÛŒØ³Øª ',\n",
       " ' Ù†ØªØ§Ù†ÛŒØ§Ù‡Ùˆ ',\n",
       " ' Ø§Ù…ÙˆØ§Ù„ ØªÙ…Ù„ÛŒÚ©ÛŒ ',\n",
       " ' Ø§Ø±ØªØ¬Ø§Ø¹ ',\n",
       " ' Ø¬Ù†Ú¯ Ø¯Ø§Ø®Ù„ÛŒ ',\n",
       " ' Ø¨Ù…Ø¨Ø§Ø±Ø§Ù† Ø´ÛŒÙ…ÛŒØ§ÛŒÛŒ ',\n",
       " ' Ø·Ù„Ø¨Ú©Ø§Ø± ',\n",
       " ' Ø³ÛŒØ¯Ù…Ø­Ù…Ø¯Ø®Ø§ØªÙ…ÛŒ ',\n",
       " ' Ø¸Ù„Ù… ',\n",
       " ' Ù…Ù‡Ø§Ø¬Ø±ÛŒ ',\n",
       " ' Ø¬Ù…Ù„Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ ',\n",
       " ' Ù…Ø³Ø¦ÙˆÙ„Ø§Ù† Ú©Ø§Ø® Ù†Ø´ÛŒÙ† ',\n",
       " ' Ù¾Ø±ÙˆÙ†Ø¯Ù‡ Ù‡Ø³ØªÙ‡ Ø§ÛŒ ',\n",
       " ' Ù…Ø­Ø±Ù… ',\n",
       " ' Ø§Ø³Ù„Ø§Ù… Ø¯ÛŒÙ† Ú©Ø§Ù…Ù„ ',\n",
       " ' Ø§Ù‚ØªØµØ§Ø¯ Ù…Ù‚Ø§ÙˆÙ…ØªÛŒ ',\n",
       " ' Ù‡Ø³ØªÙ‡ ',\n",
       " ' Ù¾ÛŒØ±ÙˆØ²ÛŒ ',\n",
       " ' Ù‡ÙˆØ´Ù…Ù†Ø¯Ø³Ø§Ø²ÛŒ ',\n",
       " ' Ù‡Ù…Ù‡ Ù¾Ø±Ø³ÛŒ ',\n",
       " ' womensday2021 ',\n",
       " ' Ù†Ù‚Ø¯ÛŒÙ†Ú¯ÛŒ ',\n",
       " ' Ø¹Ø´Ù‚ ',\n",
       " ' Ø§Ù…Ø§Ù… ',\n",
       " ' ÙˆØ¹Ø¯Ù‡ Ù†Ø§Ù…Ù…Ú©Ù† Ù†Ø¯Ù‡ÛŒÙ… ',\n",
       " ' Ø±ÙˆØºÙ† ',\n",
       " ' Ø±ÙŠØ§Ø¶ ',\n",
       " ' Ø§Ø³ØªØ§Ù† Ú©Ø±Ù…Ø§Ù† ',\n",
       " ' Ø§Ù…ÙŠØ±ÙƒØ§ ',\n",
       " ' Ø¯ÙˆÙ„Øª ÛŒØ§Ø²Ø¯Ù‡Ù… ',\n",
       " ' Ø¸Ù„Ù… Ø¢Ø´Ú©Ø§Ø± ',\n",
       " ' ÙØ§ØªØ­Ø§Ù† Ø®ÛŒØ¨Ø± ',\n",
       " ' Ø§Ø³ØªØ§Ù†ÛŒ Ø´Ø¯Ù† Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª ',\n",
       " ' Ù…Ø§ Ø¯Ø§Ø¦Ø± Ù…Ø¯Ø§Ø± Ù…Ù†Ø§ÙØ¹ÛŒÙ… ',\n",
       " ' Ø§Ø­Ù…Ù‚ Ù‚Ø¨Ù„ÛŒ ',\n",
       " ' Ø³ÛŒØ§Ø³Øª Ø®Ø§Ø±Ø¬ÛŒ ',\n",
       " ' ÙˆØ­Ø¯Øª Ù…Ù„ÛŒ ',\n",
       " ' Ø®ÙˆØ¯Ø±Ùˆ ',\n",
       " ' Ø­Ú©ÙˆÙ…ØªØŒ ',\n",
       " ' Ú©Ø±Ø¯Ø³ØªØ§Ù† Ø¹Ø±Ø§Ù‚ ',\n",
       " ' Ø­Ù‚ÙˆÙ‚ Ù…Ø±Ø¯Ù… ',\n",
       " ' ØªØ³Ù„ÛŒØª ',\n",
       " ' Ø¨Ù‡Ø§Ø± ',\n",
       " ' Ø­Ø°Ù Ú¯Ø²ÛŒÙ†Ù‡ ',\n",
       " ' Ø±ÙØ¹ ØªØ¨Ø¹ÛŒØ¶ ',\n",
       " ' Ø±ÛŒØ§Ø³Øª Ø¬Ù…Ù‡ÙˆØ±ÛŒØŒ ',\n",
       " ' Ù…Ú©ØªØ¨ Ø´Ù‡ÛŒØ¯ Ø³Ù„ÛŒÙ…Ø§Ù†ÛŒ ',\n",
       " ' Ú©Ø§Ø± Ù†Ù…Ø§ÛŒØ´ÛŒ ',\n",
       " ' Ø¸Ø±ÛŒÙ! ',\n",
       " ' Ø²Ù‡Ø±Ø§ Ù†Ø¹Ù…ØªÛŒ: ',\n",
       " ' Ú©Ù…ÛŒØ³ÛŒÙˆÙ† Ú©Ø´Ø§ÙˆØ±Ø²ÛŒØŒ ',\n",
       " ' ÛŒØ§Ø¯Ø¯Ø§Ø´Øª ',\n",
       " ' Ø¢Ø²Ø§Ø¯ÛŒ Ø®ÙˆØ§Ù‡ ',\n",
       " ' Ù…Ø±Ø§Ø¬Ø¹ ØªÙ‚Ù„ÛŒØ¯ ',\n",
       " ' Ú¯ÙØªÚ¯ÙˆÛŒ Ù…Ù„ÛŒ ',\n",
       " ' Ø²Ø§Ú¯Ø±Ø³ Ø¯Ø± Ø¢ØªØ´ ',\n",
       " ' Ø±Ù‡Ø¨Ø±ÛŒ ',\n",
       " ' Ø§ÙˆÙ„ÙˆÛŒØª Ø³Ø±Ù¾Ø±Ø³Øª ',\n",
       " ' ÙˆØ§Ú©Ø³Ù† Ø¨Ø±Ú©Øª ',\n",
       " ' Ù„Ø¥ÙŠØ±Ø§Ù† ',\n",
       " ' Ø­Ø¶Ø±Øª Ø²Ù‡Ø±Ø§(Ø³) ',\n",
       " ' Ø­Ú©Ù… Ø±Ù‡Ø¨Ø±Ù…Ø¹Ø¸Ù… ',\n",
       " ' Ø§Ø³ØªØ§Ù† Ø®ÙˆØ²Ø³ØªØ§Ù† ',\n",
       " ' Ù†Ù‡ Ø¨Ù‡ Ø®Ø´ÙˆÙ†Øª Ø¹Ù„ÛŒÙ‡ Ø²Ù†Ø§Ù† ',\n",
       " ' Ù†Ù‡ Ø¨Ù‡ Ø­Ø±Ù Ø¯Ø±Ù…Ø§Ù†ÛŒ ',\n",
       " ' Ø±Ø¯ ØµÙ„Ø§Ø­ÛŒØª ',\n",
       " ' ØªØºÛŒÛŒØ± Ø±ÛŒÙ„ Ø¨ÙˆØ¯Ø¬Ù‡ ',\n",
       " ' Ø²Ù†Ø§Ù† ',\n",
       " ' Ø¹Ù„Ù…Ø¯Ø§Ø± ',\n",
       " ' Ø­Ø³ÛŒÙ† Ø®Ø±Ø§Ø²ÛŒ ',\n",
       " ' Ø®Ø¯Ù…Øª Ø³Ø±Ø¨Ø§Ø²ÛŒ ',\n",
       " ' Ø§Ù„ÙŠÙ…Ù†. ',\n",
       " ' Ø¯Ù…ÙˆÚ©Ø±Ø§Ø³ÛŒ ',\n",
       " ' Ø§Ø³ØªØ§Ù† Ù…Ø±Ú©Ø²ÛŒ ',\n",
       " ' Ø¨Ø­Ø±Ø§Ù† Ø¢Ø¨ ',\n",
       " ' Ø§Ø­Ù…Ø¯ÛŒ Ù†Ú˜Ø§Ø¯ ',\n",
       " ' Ø¨Ø±Ù†Ø§Ù…Ù‡ ',\n",
       " ' Ù†Ø¸Ø±Ø³Ù†Ø¬ÛŒ: ',\n",
       " ' Ø´Ù‡ÛŒØ¯ØŒ ',\n",
       " ' Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ«Ù†Ø§ ',\n",
       " ' Ù†Ø³Ù„ Ú†Ù‡Ø§Ø±Ù… ',\n",
       " ' Ø§Ø­Ù…Ø¯ÛŒ Ù†Ú˜Ø§Ø¯ ',\n",
       " ' MiddleEast. ',\n",
       " ' ØªÙˆÙ„ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù†ØŒ ',\n",
       " ' Ø§Ø³ØªØ§Ù† Ú©Ø±Ø¯Ø³ØªØ§Ù† ',\n",
       " ' ØµØ¯ÛŒÙ‚ÛŒ ',\n",
       " ' Ø­Ù‚ Ù…Ø±Ø¯Ù… Ø¯Ø± Ø¬ÛŒØ¨ Ù…Ø±Ø¯Ù… ',\n",
       " ' Ú¯Ø²Ø§Ø±Ø´ Ø¨Ù‡ Ù…Ø±Ø¯Ù… ',\n",
       " ' Ø¯Ú©ØªØ± Ø¬Ø¨Ù„ÛŒ ',\n",
       " ' Ù…Ù‚Ø§ÙˆÙ…Øª Ø§ÛŒØ±Ø§Ù†ÛŒØ§Ù† ',\n",
       " ' Ù†ÛŒÙˆÛŒÙˆØ±Ú© ØªØ§ÛŒÙ…Ø²:Ù…Ù‚Ø§Ù„Ø§Øª ',\n",
       " ' Ø±Ø§Ø³ØªÛŒ Ø¢Ø²Ù…Ø§ÛŒÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ',\n",
       " ' ØºØ±Ø¨Ø§Ù„Ú¯Ø±ÛŒ Ú˜Ù†ØªÛŒÚ© ',\n",
       " ' Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ù¾Ø±ÙˆØ±Ø´ ',\n",
       " ' Ú¯Ø±Ø§Ù†ÛŒ ',\n",
       " ' Ø¬Ù…Ù‡ÙˆØ±ÛŒØŒ ',\n",
       " ' Ù‚Ø·Ø¹ Ø¨Ø±Ù‚ ',\n",
       " ' Ø§Ø³ØªØ§Ù† Ù„Ø±Ø³ØªØ§Ù† ',\n",
       " ' Ø¬ÙˆØ±Ø¬ ÙÙ„ÙˆÛŒØ¯ ',\n",
       " ' ØªØ±ÙˆØ±ÙŠØ³Ù… ',\n",
       " ' Ø¨ÙˆØ¯Ø¬Ù‡Û±Û´Û°Û° ',\n",
       " ' Ù…Ø³ØªØ¶Ø¹ÙÛŒÙ† ',\n",
       " ' Ø³Ù¾Ø§Ù‡ØŒ ',\n",
       " ' Ù…Ø±Ø¯Ù… Ø¨ÛŒ Ù¾Ù†Ø§Ù‡ ',\n",
       " ' Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ø³ØªÙ…Ø²Ø¯ ',\n",
       " ' Ø²Ø¯Ù† Ø²ÛŒØ± Ù…ÛŒØ² ',\n",
       " ' Ø¹Ø±Ø§Ù‚: ',\n",
       " ' Ú©Ø±Ø¯Ø³ØªØ§Ù† ',\n",
       " ' Iran ',\n",
       " ' Ø¨ÙˆÚ©ÙˆØ­Ø±Ø§Ù… ',\n",
       " ' Ù…Ø¬Ù„Ø³ Ù‚ÙˆÛŒ ',\n",
       " ' Ù…Ø­Ø§ÙØ¸Ù‡ Ú©Ø§Ø±ÛŒ ',\n",
       " ' Ú©Ø§Ù‡Ø´ ØªØ¨Ø¹ÛŒØ¶ ',\n",
       " ' Ø§ÛŒØ±Ø§Ù†ØŸ ',\n",
       " ' ØµÙÙ‚Ø© Ø§Ù„Ù‚Ø±Ù†Â» ',\n",
       " ' Ø´Ú¯ÙØªÛŒ ',\n",
       " ' ÙØ§Ø¬Ø¹Ù‡ Ù…Ù„ÛŒ ',\n",
       " ' Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ ',\n",
       " ' Ù…Ø§ Ù…ÛŒØªÙˆØ§Ù†ÛŒÙ… ',\n",
       " ' Ø¬Ù‡Ø§Ø¯ÛŒ ',\n",
       " ' Ø§Ù‚ØªØµØ§Ø¯ Ø¶Ø¹ÛŒÙ ',\n",
       " ' Ø¬Ù„ÛŒÙ„ÛŒ ',\n",
       " ' ÛŒÚ© Ù…ÛŒÙ„ÛŒÙˆÙ† ÙˆØ§Ø­Ø¯ Ù…Ø³Ú©Ù† ',\n",
       " ' Ù…ÙˆØ³ÙˆÛŒ ',\n",
       " ' Ø§ØµÙ„Ø§Ø­Ø§Øª. ',\n",
       " ' Ø§Ø³ØªØ§Ù† Ø§Ù„Ø¨Ø±Ø² ',\n",
       " ' Ù…Ø¹Ù„ÙˆÙ„Ø§Ù† ',\n",
       " ' Ø¯ÙˆÙ„Øª Ù…Ø±Ø¯Ù…ÛŒ ',\n",
       " ' Ø¹Ù‚Ù„Ø§Ù†ÛŒ ',\n",
       " ' Ø³Ù¾Ø§Ù‡ Ù‚Ø¯Ø³ ',\n",
       " ' Ø§Ø±Ø² ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø³Ù„Ø§Ù… ',\n",
       " ' Ø§ÛŒØ±Ø§Ù† Ù…Ø§Ù† ',\n",
       " ' Ø´Ø§Ù‡Ú©Ø§Ø± Ø±ÙˆØ­Ø§Ù†ÛŒ ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø§Ù‚Ø¯Ø§Ù… Ùˆ ØªØ­ÙˆÙ„ ',\n",
       " ' Ø±Ø§Ù†Øª ',\n",
       " ' Ø³Ø±Ú©ÙˆØ¨ Ø¯Ø±Ù…Ø§Ù†ÛŒ ',\n",
       " ' Ù…Ø¹ÙˆÙ‚Ø§Øª Ø¨Ø§Ù†Ú©ÛŒ ',\n",
       " ' Ú©Ø±ÙˆÙ†Ø§ Ø±Ø§ Ø´Ú©Ø³Øª Ù…ÛŒØ¯Ù‡ÛŒÙ…. ',\n",
       " ' Ù…Ù„Ú©Ù‡ Ø§Ù„ÛŒØ²Ø§Ø¨Øª ',\n",
       " ' Ù†Ø¸Ø±Ù…Ø±Ø¯Ù… ',\n",
       " ' Ø³Ú©ÙˆØª ',\n",
       " ' ØªØ±ÙˆØ± ',\n",
       " ' Ø¨Ø§Ø±Ø²Ø§Ù†ÛŒ ',\n",
       " ' ÙØ±Ø§Ø± Ù…Ø§Ù„ÛŒØ§ØªÛŒ ',\n",
       " ' Ø­Ø±ÛŒÙ… Ø®ØµÙˆØµÛŒ ',\n",
       " ' Ø¨ÙˆØ±Ø³ ',\n",
       " ' Ø´Ø¬Ø±ÛŒØ§Ù†ØŒ ',\n",
       " ' Ø¹Ù„ÛŒ Ù„Ù†Ø¯ÛŒ ',\n",
       " ' Ø§ÛŒØ±Ø§Ù†  Ø§ÛŒÙ†ØªØ±Ù†Ø´Ù†Ø§Ù„: ',\n",
       " ' Ø¹Ø¯Ø§Ù„Øª ',\n",
       " ' Ø­Ù‚ÙˆÙ‚ Ø®Ø§Ù†Ù‡ Ø¯Ø§Ø±ÛŒ ',\n",
       " ' IranElection2021 ',\n",
       " ' Ø³Ø¹ÛŒØ¯ Ø¬Ù„ÛŒÙ„ÛŒ ',\n",
       " ' Ø§Ù…Ø§Ù… Ø®Ù…ÛŒÙ†ÛŒ(Ø±Ù‡)ØŒ ',\n",
       " ' Ù…Ù…Ø§Ù†Ø¹Øª ',\n",
       " ' Ú©Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø±Ø¯Ù… ',\n",
       " ' Ù†Ø¨ÙˆÛŒØ§Ù† ',\n",
       " ' Ù…Ø±Ú¯ Ø¨Ø±Ø¬Ø§Ù… ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø¬ÙˆØ§Ù† Ùˆ Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡ÛŒ ',\n",
       " ' Ø¨Ø§Ù‡Ù†Ø± ',\n",
       " ' Ø¨ÙˆØ´Ù‡Ø± ',\n",
       " ' ØµØ¯Ø§ Ùˆ Ø³ÛŒÙ…Ø§ ',\n",
       " ' Ø³ÛŒØ§Ø³ÛŒ ',\n",
       " ' Ø±ÙˆÙ…ÛŒÙ†Ø§ Ø§Ø´Ø±ÙÛŒ ',\n",
       " ' Ø­Ø§Ø¬ Ù‚Ø§Ø³Ù… ',\n",
       " ' Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† ',\n",
       " ' Ø¯ÙˆÙ„Øª ',\n",
       " ' Ù‡Ø³ØªÙ‡ Ø§ÛŒ ',\n",
       " ' Ø­Ø²Ø¨ Ø§Ù„Ù„Ù‡ ',\n",
       " ' Ø²ÙŠØ§Ø±Ø© Ø§Ø±Ø¨Ø¹ÙŠÙ† ',\n",
       " ' ØªØ±Ø§Ø´Ù‡ ',\n",
       " ' Øº ',\n",
       " ' ØµÙ‡ÛŒÙˆÙ†ÛŒØ³ØªÛŒ ',\n",
       " ' Ø¬ÙˆØ§Ù†Ú¯Ø±Ø§ÛŒÛŒ ',\n",
       " ' Ø­Ø¬Ø§Ø¨ ',\n",
       " ' Ø¨Ù†ÛŒØ§Ø¯ Ø´Ù‡ÛŒØ¯ ',\n",
       " ' ØºÙ†ÛŒ Ø³Ø§Ø²ÛŒ ',\n",
       " ' Ø´Ù‡Ø± Ú©Ø´ÙˆØ± ',\n",
       " ' ØªØ±Ø§Ù…Ø¨ ',\n",
       " ' Ø¨ØµÛŒØ±ØªØŒ ',\n",
       " ' Û±Û´Û°Û° ',\n",
       " ' Ø¨Ø­Ø±Ø§Ù† Ø³Ø§Ø²ÛŒ ',\n",
       " ' Ø§Ù†ØªØµØ§Ø¨Ø§Øª ',\n",
       " ' ØªÛŒÙ… Ù…Ù„Øª Ø§ÛŒØ±Ø§Ù† ',\n",
       " ' 3YearsOfWarOnYemen ',\n",
       " ' Ø³ÛŒÙ†Ù…Ø§ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ù†Ù‚Ù„Ø§Ø¨ ',\n",
       " ' Ø³ÛŒØ§Ø³Øª Ù‡Ø§ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø¯Ø§Ù†Ø´ ',\n",
       " ' Ø§ØªØ§Ù‚ Ø¨Ø§Ø²Ø±Ú¯Ø§Ù†ÛŒØŒ ',\n",
       " ' Ø§ØµÙ„Ø§Ø­Ø§Øª ',\n",
       " ' Ø¶Ø¹Ù Ø§ÛŒÙ…Ø§Ù† ',\n",
       " ' ØµØ§Ø¯Ù‚ Ù„Ø§Ø±ÛŒØ¬Ø§Ù†ÛŒ ',\n",
       " ' Ø¯Ø³ØªÚ¯Ø§Ù‡ Ù‚Ø¶Ø§ÛŒÛŒ ',\n",
       " ' Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø±Ø³Øª Ú©Ø§Ø± Ø¯Ø±Ø³Øª ',\n",
       " ' Ø§Ù„Ù„Ù‡Ù… Ø¹Ø¬Ù„ Ù„ÙˆÙ„ÙŠÙƒ Ø§Ù„ÙØ±Ø¬ ',\n",
       " ' Ù…ØµØ§Ø¯Ø±Ù‡ ',\n",
       " ' ØªÙ†Ù‡Ø§ Ú¯Ø±ÛŒÙ‡ Ú©Ù† ',\n",
       " ' Ø³Ø±Ø§Ø¬ ',\n",
       " ' Ø¨Ù†ÛŒ ØµØ¯Ø± ',\n",
       " ' Ø¸Ù„Ù… Ùˆ Ø¬ÙØ§ ',\n",
       " ' Ù†Ù‚Ø¯ ',\n",
       " ' Ù¾Ø¯ÛŒØ¯Ù‡   ',\n",
       " ' Ù¾Ù†Ø¬Ø´ÙŠØ± ',\n",
       " ' Ø­Ø¯Ø§ÙƒØ«Ø±ÙŠ ',\n",
       " ' ØºØ²Ù„ ',\n",
       " ' Ú¯Ø§Ù†Ø¯Ùˆ  ',\n",
       " ' Ø´Ø±Ù…Ù† ',\n",
       " ' Ø¬Ù†Ø§ÛŒØª ØºÛŒØ±Ø¹Ù…Ø¯ ',\n",
       " ' ÙÛŒÙ„ØªØ± ',\n",
       " ' Ø­ØµØ± ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒØŒ ',\n",
       " ' Ø¯Ù„ ØªÙ†Ú¯ÛŒ ',\n",
       " ' Ù‚Ø·Ø¹ÛŒ Ø¨Ø±Ù‚ ',\n",
       " ' Ù…Ø´Ø§Ø±Ú©Øª Ø­Ø¯Ø§Ú©Ø«Ø±ÛŒ ',\n",
       " ' Ø§Ù…ØªÛŒØ§Ø²Ø§ØªÛŒ ',\n",
       " ' Ù…Ø§ÙÛŒØ§ÛŒ Ø®ÙˆØ¯Ø±Ùˆ ',\n",
       " ' Ø·Ø¨ÛŒØ¹Øª ',\n",
       " ' Ú©Ø§Ù†Ø¯ÛŒØ¯Ø§ÛŒ Ù¾ÙˆØ´Ø´ÛŒ ',\n",
       " ' Ú¯Ø§Ù… Ø¯ÙˆÙ… ',\n",
       " ' Arbaeen2020 ',\n",
       " ' Ø¯ÙˆÙ‚Ø·Ø¨ÛŒ ',\n",
       " ' Ø±ÛŒØ§Ø³Øª Ø¬Ù…Ù‡ÙˆØ±ÛŒ ',\n",
       " ' Ø¬Ø±Ù… ',\n",
       " ' Ù…ÛŒØ¯Ø§Ù† ',\n",
       " ' Ù…Ø´Ø§ÙŠÙŠ ',\n",
       " ' Ø­Ø¨Ø³ ',\n",
       " ' ØºØ±Ø¨Ú¯Ø±Ø§ÛŒÛŒ ',\n",
       " ' Ø¬Ù‡Ø´ ØªÙˆÙ„ÛŒØ¯ ',\n",
       " ' Ø§Ù…Ø§Ù… Ø¬Ù…Ø¹Ù‡ Ø§ØµÙÙ‡Ø§Ù†ØŒ ',\n",
       " ' Ù„Ø§ÛŒØ­Ù‡ ÛŒ Ø´ÙØ§ÙÛŒØª ',\n",
       " ' Ù„ÙŠÙ„Ø© Ø§Ù„Ù‚Ø¯Ø± ',\n",
       " ' Ù¾Ù„ÛŒØ³ ',\n",
       " ' Ø´ÙØ§ÙÛŒØª. ',\n",
       " ' Ú©ØªØ§Ø¨ ',\n",
       " ' 24Ø®Ø±Ø¯Ø§Ø¯ ',\n",
       " ' Ù¾ÛŒØ´Ø±ÙØª Ù‡Ù…Ù‡ Ø¬Ø§Ù†Ø¨Ù‡ ',\n",
       " ' Ø¯Ø§Ø¹Ø´ØŒ ',\n",
       " ' Ø­Ù…ÙŠØ¯ Ø¨Ù‚Ø§ÛŒÛŒ ',\n",
       " ' Ø§Ù…Ù†ÛŒØª Ù…Ù„ÛŒ Ø§ÛŒØ±Ø§Ù† ',\n",
       " ' Ø§ÙˆØ±Ú˜Ø§Ù†Ø³ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ ',\n",
       " ' Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ',\n",
       " ' Ù†Ø¸Ø§Ø±Øª Ø§Ø³ØªØµÙˆØ§Ø¨ÛŒ ',\n",
       " ' Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ù…Ø±Ø¯Ù… ',\n",
       " ' Ø§Ø³Ù„Ø§Ù…ÛŒ ',\n",
       " ' Ø®ÙˆØ¯ÛŒ ØºÛŒØ±Ø®ÙˆØ¯ÛŒØŒ ',\n",
       " ' Ú¯ÙˆÙ‡Ø± Ø¹Ø´Ù‚ÛŒ ',\n",
       " ' Ø±Ø§Ø³ØªÛŒ Ø¢Ø²Ù…Ø§ÛŒÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ',\n",
       " ' Ø§Ø³ØªØ¹ÙØ§ ',\n",
       " ' Ø¯ÙˆÙ„Øª ØªÙˆØ§Ù†Ø§ Ø¯Ø± Ø¬Ø§Ù…Ø¹Ù‡ ØªÙˆØ§Ù†Ø§ ',\n",
       " ' Ø´Ø¨Ú©Ù‡ ÛŒÚ© ',\n",
       " ' ØªØ§Ø±ÛŒØ® Ú©Ø±ÙˆÙ†Ø§ ',\n",
       " ' Ù…Ø±Ø¯Ù… ',\n",
       " ' Ù…Ø±Ø²Ø¨Ø§Ù† ',\n",
       " ' Ø¬Ø§Ù…Ø¹Ù‡ Ø§Ù†Ø³Ø§Ù†ÛŒ ',\n",
       " ' Ù…Ø³Ù„Ù…Ø§Ù‹ ',\n",
       " ' Ù†ÙØª ',\n",
       " ' Ø´Ù‡Ø¯Ø§ØŒ ',\n",
       " ' Ø¯ÙˆÙ„ØªÛŒ ',\n",
       " ' Ø§Ù…Ø§Ø±Ø§Øª\\u2069 ',\n",
       " ' Ø¨Ø§Ù†ÛŒØ§Ù† ÙˆØ¶Ø¹ Ù…ÙˆØ¬ÙˆØ¯ ',\n",
       " ' Ø¢ÛŒØª Ø§Ù„Ù„Ù‡ Ù‡Ø§Ø´Ù…ÛŒ Ø±ÙØ³Ù†Ø¬Ø§Ù†ÛŒ ',\n",
       " ' Ø§Ø³ØªØ§Ù† ÛŒØ²Ø¯ ',\n",
       " ' Ø¨Ù†Ø²ÛŒÙ† ',\n",
       " ' Ù¾ÛŒÙ…Ø§Ù† Ø¬Ø¨Ù„ÛŒ ',\n",
       " ' Ù¾ ',\n",
       " ' Ø§ÛŒØ±Ù„Ùˆ ',\n",
       " ' Ú¯Ø§Ù… Ø¯ÙˆÙ… Ø§Ù†Ù‚Ù„Ø§Ø¨ ',\n",
       " ' Ú†Ù‡Ù„ Ø³Ø§Ù„Ú¯ÛŒ Ø§Ù†Ù‚Ù„Ø§Ø¨ ',\n",
       " ' Ø³Ø¹Ø¯ÛŒ Ø´ÛŒØ±Ø§Ø²ÛŒ ',\n",
       " ' ÙØ±ÛŒØ¯ÙˆÙ† ',\n",
       " ' Ú©ÛŒÙ‡Ø§Ù†ØŒ ',\n",
       " ' ØªØ®Ù… Ù…Ø±Ø® ',\n",
       " ' ØªÙˆØ¬ÛŒÙ‡ ',\n",
       " ' EconomicTerrorism ',\n",
       " ' Ú¯ÙˆØ´ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ ',\n",
       " ' Ø®Ø§Ù†Ù‡ Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ ',\n",
       " ' Ø±Ù‡Ø¨Ø± Ø§Ù†Ù‚Ù„Ø§Ø¨ ',\n",
       " ' Ø§ØªØ­Ø§Ø¯ Ù…Ù„ÛŒ ',\n",
       " ' Ø¯ÙˆÚ¯Ø§Ù†Ù‡ Ø·Ø¨ÛŒØ¹ÛŒ ',\n",
       " ' ÙØ±ÙˆØ´ Ø§ÙˆØ±Ø§Ù‚ Ù†ÙØªÛŒ ',\n",
       " ' Ù‡Ø±Ù†ÙØ±Û±Û°Ù†ÙØ± ',\n",
       " ' Ø¯ÛŒÙ¾Ù„Ù…Ø§Ø³ÛŒ ',\n",
       " ' Ø®ÛŒØ²Ø´ Ù…Ø³ØªØ¶Ø¹ÙØ§Ù† ',\n",
       " ' Ú¯ÛŒÙ„Ø§Ù† ',\n",
       " ' Ú†Ù†Ø¯Ø³Ø§Ù„ ',\n",
       " ' Ù…Ø³Ú©Ù† ',\n",
       " ' Ù…Ù†Ø·Ù‚Ù‡ ',\n",
       " ' Ù…Ø±Ú©Ù„ ',\n",
       " ' Ù‚Ø±ÙˆÙ† ÙˆØ³Ø·ÛŒ ',\n",
       " ' Ø¬Ù‡Ø´ Ø¨Ø²Ø±Ú¯ ',\n",
       " ' Ù…Ù†Ø§Ø¸Ø±Ù‡  Ù‡Ø§ ',\n",
       " ' Ù¾Ø§Ø¯Ú¯Ø§Ù† ',\n",
       " ' Ú©ÙˆØ¯ØªØ§ ',\n",
       " ' Ø¨Ø§ÛŒØ¯ ',\n",
       " ' Ø§Ø±Ø¨Ø¹ÛŒÙ† Ø¨Ø¯ÙˆÙ† Ø¯Ù„Ø§Ø± ',\n",
       " ' Ø§Ù†Ø²ÙˆØ§ ',\n",
       " ' ØµØ¯Ø§ÙˆØ³ÙŠÙ…Ø§ ',\n",
       " ' Ø§Ù†ØªÙ‚Ø§Ø¯ ',\n",
       " ' Ù…Ø±Ø¬Ø¹ Ø¨ØµÛŒØ± ',\n",
       " ' Ù¾ÛŒØ´Ø±ÙØª Ú©Ø´ÙˆØ± ',\n",
       " ' Ù¾Ù†Ø¬ Ø¯Ø±Ø¨Ø±Ø§Ø¨Ø± ÛŒÚ© ',\n",
       " ' Ø§Ø¹ØªØ±Ø§Ø¶ ',\n",
       " ' ÙÙ„Ø³Ø·ÙŠÙ† ',\n",
       " ' Ø®Ø±Ø§Ø³Ø§Ù† Ø±Ø¶ÙˆÛŒ ',\n",
       " ' ÙØ±ÙˆØ¯Ú¯Ø§Ù‡ ',\n",
       " ' Ø®ÛŒØ± Ù…ÙˆØ«Ø± ',\n",
       " ' Ø³ÛŒØ§Ø³Øª Ú¯Ø°Ø§Ø±ÛŒ ',\n",
       " ' Ù…Ø¹Ø·Ù„ ',\n",
       " ' Ù…ÙˆØ³Ø§Ø¯ ',\n",
       " ' Ú†Ù‡Ø§Ø±Ù…Ø­Ø§Ù„ Ùˆ Ø¨Ø®ØªÛŒØ§Ø±ÛŒ ',\n",
       " ' Ø¯ÙŠÙ¾Ù„Ù…Ø§Ø³ÙŠ ',\n",
       " ' Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø§Ø³Ù„Ø§Ù…ÛŒ ',\n",
       " ' Ù‚Ø§Ø³Ù… Ø³Ù„ÛŒÙ…Ø§Ù†ÛŒ Ù‚Ù‡Ø±Ù…Ø§Ù† Ù…Ù„ÛŒ ',\n",
       " ' ØºÙ„Ø· ',\n",
       " ' Ø§Ù„Ù‚Ø¯Ø³ Ø§Ù‚Ø±Ø¨ ',\n",
       " ' Ø±Ø³Ø§Ù†Ù‡ Ø¨Ø§Ø´ÛŒØ¯ ',\n",
       " ' Ø±Ø³Ø§Ù†Ù‡ Ù‡Ø§ÛŒ Ø¬Ø±ÛŒØ§Ù† Ø§ØµÙ„ÛŒ ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø¯Ù‡Ù… ',\n",
       " ' EconomicTerrorism. ',\n",
       " ' Ø¯Ø®ØªØ± Ø¢Ø¨ÛŒ ',\n",
       " ' Ø¬Ø§Ø³ÙˆØ³ÛŒ ',\n",
       " ' Ø£Ù…Ø±ÙŠÙƒØ§ ',\n",
       " ' Ø§Ø¹Ù„Ø§Ù…ÛŒÙ‡ Ø¬Ù‡Ø§Ù†ÛŒ Ø­Ù‚ÙˆÙ‚ Ø¨Ø´Ø± ',\n",
       " ' ÙØ¶Ø§ÛŒ Ù…Ø¬Ø§Ø²ÛŒ ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø³Ø§ÛŒÙ‡ ',\n",
       " ' Ø³Ø±Ù¾Ù„ Ø°Ù‡Ø§Ø¨ ',\n",
       " ' ØºØ±Ø¨ ',\n",
       " ' ProphetMuhammad ',\n",
       " ' Ù…Ù† Ø±Ø§ÛŒ Ù…ÛŒØ¯Ù‡Ù… ',\n",
       " ' Ù…Ù‚Ø§ÙˆÙ…Øª Ù¾Ù†Ø¬Ø´ÛŒØ± ',\n",
       " ' Ø¨Ø­Ø±ÙŠÙ† ',\n",
       " ' ÙƒÙ„Ø§ Ù„Ù„Ø¥Ø¹Ø¯Ø§Ù… ',\n",
       " ' Ù…Ù†ÙØ¹Ù„ØŒ ',\n",
       " ' Ø­Ø§Ø¬ Ù‚Ø§Ø³Ù… Ø³Ù„ÛŒÙ…Ø§Ù†ÛŒ ',\n",
       " ' Ø¯Ø´Ù…Ù†Ø§Ù† ',\n",
       " ' Ø·Ø§Ù„Ø¨Ø§Ù† ',\n",
       " ' Ø§Ø³ØªØ¹Ù…Ø§Ø±Ú¯Ø± ',\n",
       " ' ÙˆØ§Ù„ÛŒØ¨Ø§Ù„ ',\n",
       " ' Ø¬Ù„ÛŒÙ„ÛŒ-Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ',\n",
       " ' Ù‡Ù… Ø§Ú©Ù†ÙˆÙ† ',\n",
       " ' Ù…ÙˆØ§ÙÙ‚Øª ',\n",
       " ' Ø­Ù…Ø§ÛŒØª ',\n",
       " ' ØµÙ„Ø­ØŒ ',\n",
       " ' Ø¯Ø§Ø¹Ø´ ',\n",
       " ' Ø±ÙˆØ² Ø¬Ù‡Ø§Ù†ÛŒ Ú©Ø§Ø±Ú¯Ø± ',\n",
       " ' Ø§ØµÙ„Ø§Ø­ Ø·Ù„Ø¨: ',\n",
       " ' Ø²Ù„Ø²Ù„Ù‡ ',\n",
       " '  Ú©Ø±ÙˆÙ†Ø§ ',\n",
       " ' Ù…Ù† Ø±ÙˆÛŒØ§ Ø¯Ø§Ø±Ù… Ù¾Ø³ Ù‡Ø³ØªÙ…. ',\n",
       " ' Ø§Ù†ØªÙ‚Ø§Ù… Ø³Ø®Øª ',\n",
       " ' Ø±Ú˜ÛŒÙ… ØµÙ‡ÛŒÙˆÙ†ÛŒØ³ØªÛŒ: ',\n",
       " ' ØªØ±Ø§Ù…Ù¾ ',\n",
       " ' Ø³ÙˆØ§Ø¨Ù‚ ØªØ­ØµÛŒÙ„ÛŒ ',\n",
       " ' Ø¨Ø±Ø§Ø¯Ø±Ú©Ø´ÛŒ ',\n",
       " ' Ø®Ø§ØªÙ…ÛŒ Ù…Ø¯ÛŒØ§ ',\n",
       " ' Ø³Ø§Ø²Ù…Ø§Ù† Ù†Ø¸Ø§Ù…ÛŒ Ø±Ø³Ù…ÛŒ ',\n",
       " ' Ø­Ù…Ø§ÛŒØª Ø§Ø² Ú©Ø§Ù„Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø¢ÛŒÙ†Ø¯Ù‡ ',\n",
       " ' Ø±ÙˆØ² Ù…Ù„ÛŒ Ø´ÙˆØ±Ø§Ù‡Ø§ ',\n",
       " ' Ø¨Ø§ÛŒØ¯Ù† ',\n",
       " ' Ø³Ø§Ø²Ù…Ø§Ù† Ø´Ø§Ù†Ú¯Ù‡Ø§ÛŒ ',\n",
       " ' Ø§Ù‡Ù„ Ø³Ù†Øª ',\n",
       " ' Ø¨Ø¹Ø«Øª ',\n",
       " ' Ù‡Ø± Ú¯ÙˆØ´ÛŒ ÛŒÚ© Ø³ØªØ§Ø¯ ',\n",
       " ' Ø¢Ø°Ø±Ø¨Ø§ÛŒØ¬Ø§Ù† Ø´Ø±Ù‚ÛŒ ',\n",
       " ' Ø­Ù‚ Ø§Ù†ØªÙ‚Ø§Ø¯ ',\n",
       " ' Ø´ÛŒÙ†Ø²Ùˆ Ø¢Ø¨Ù‡ ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø³ÙˆÙ… Ø®Ø§ØªÙ…ÛŒ ',\n",
       " ' Ù¾ÛŒÚ© Ú†Ù‡Ø§Ø±Ù… ',\n",
       " ' Ø¬Ù†Ø§Ø¨ ',\n",
       " ' Ø±Ú©ÙˆØ¯ ',\n",
       " ' Ù‡ÙØª ØªÙ¾Ù‡ ',\n",
       " ' Ø±Ø¦ÛŒØ³ Ø¬Ù…Ù‡ÙˆØ±ØŒ ',\n",
       " ' ØµØ§Ø­Ø¨ Ø§Ù„Ø²Ù…Ø§Ù† ',\n",
       " ' Ù‚Ø±Ø¢Ù† ÙƒØ±ÙŠÙ… ',\n",
       " ' ØºÛŒØ± Ù¾ÙˆÙ¾ÙˆÙ„ÛŒØ³ØªÛŒ ',\n",
       " ' Ù…Ø­Ø³Ù† Ø±Ø¶Ø§ÛŒÛŒ:Ø¨Ø§ ',\n",
       " ' Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒ ',\n",
       " ' Ø·Ø±Ø­ Ø§Ø¹Ø§Ø¯Ù‡ Ø§Ù…ÙˆØ§Ù„ Ù†Ø§Ù…Ø´Ø±ÙˆØ¹ Ù…Ø³Ø¦ÙˆÙ„Ø§Ù† ',\n",
       " ' Ù¾ÛŒØ§Ù…Ú© ÙˆØ­Ø´Øª ',\n",
       " ' Ø³Ù…Ù†Ø§Ù† ',\n",
       " ' ØªÙˆØ¦ÙŠØªØ± ',\n",
       " ' Ù…Ø±Øº ',\n",
       " ' Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Ø­Ø¯Ø§Ù‚Ù„ÛŒ ',\n",
       " ' Ù‡ÛŒØ§Øª Ù…Ù‚Ø±Ø±Ø§Øª Ø²Ø¯Ø§ÛŒÛŒ ',\n",
       " ' Ø§Ú©Ø«Ø±ÛŒØª ØºÛŒØ±Ø®ÙˆØ¯ÛŒ ',\n",
       " ' Ø§Ø´ØªØºØ§Ù„ ',\n",
       " ' Ø³ÛŒÙ ',\n",
       " ' Ø³Ø§Ø²Ù…Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡ ÙˆØ¨ÙˆØ¯Ø¬Ù‡ ',\n",
       " ' Ø¯ÙÙ† Ø²Ø¨Ø§Ù„Ù‡ ',\n",
       " ' Ù¾Ø§Ø³Ø® Ù…Ø¹ØªØ¨Ø± ',\n",
       " ' ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ',\n",
       " ' ØªÙ‡Ø±Ø§Ù†: ',\n",
       " ' Ú¯Ø´Ø§ÛŒØ´ Ø§Ù‚ØªØµØ§Ø¯ÛŒ ',\n",
       " ' ØªØ¬Ø§Ø±Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ ',\n",
       " ' Ø§Ø­Ù…Ø¯ Ú¯Ù„ Ù…Ø­Ù…Ø¯ÛŒ ',\n",
       " ' ØªØ±Ú©ÙŠØ§ ',\n",
       " ' Ø·Ø±Ø­ ØµÛŒØ§Ù†Øª ',\n",
       " ' Ù†ÛŒØ±ÙˆÛŒ Ø¯Ø±ÛŒØ§ÛŒÛŒ ',\n",
       " ' Ø¢Ù„ Ø³Ø¹ÙˆØ¯ ',\n",
       " ' Ø§ÙØºØ§Ù†Ø³ØªØ§Ù†ÛŒ ',\n",
       " ' Ø¬Ù†Ú¯ Ø§Ù‚ØªØµØ§Ø¯ÙŠ ',\n",
       " ' ØªÙˆÙ„ÛŒØ¯ ',\n",
       " ' ØªØ­Ø±ÛŒÙ…ØŒ ',\n",
       " ' Future Bank ',\n",
       " ' Ø¯ÙˆÙ„Øª Ø³ÛŒØ²Ø¯Ù‡Ù… ',\n",
       " ' Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„ ',\n",
       " ' Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø´ØªÙ‡ Ù‡Ø§ ',\n",
       " ' Ø¹Ø¯Ø§Ù„Øª ÙØ±Ù‡Ù†Ú¯ÛŒ ',\n",
       " ' Ø¹Ø°Ø±Ø®ÙˆØ§Ù‡ÛŒ ',\n",
       " ' Ø§ØµØºØ±Ø²Ø§Ø¯Ù‡ØŒ ',\n",
       " ' Ø´ÙˆØ±Ø§ÛŒ Ù†Ú¯Ù‡Ø¨Ø§Ù†: ',\n",
       " ' Ù„Ø¨Ø§Ø³ Ø´Ø®ØµÛŒ Ù‡Ø§ ',\n",
       " ' iranelections ',\n",
       " ' Ù…Ø­Ø§ØµØ±Ù‡ ',\n",
       " ' Ù‡Ø¯Ù… Ø¬Ù…Ù‡ÙˆØ±ÛŒØª ',\n",
       " ' Ù…Ø±ÛŒÙ… Ù…ÛŒØ±Ø²Ø§Ø®Ø§Ù†ÛŒ ',\n",
       " ' Ø¹Ø¯Ø§Ù„Øª Ø¬Ù†Ø³ÛŒØªÛŒ ',\n",
       " ' Ø¨Ø§Ù†Ú©ÛŒ ',\n",
       " ' Ù…Ø¨Ø§Ø±Ø²Ù‡ Ø¨Ø§ÙØ³Ø§Ø¯ ',\n",
       " ' ØµØ¯Ø§Ù‚Øª Ø¨Ø¬Ø§ÛŒ Ø¯Ø±ÙˆØº ',\n",
       " ' Ø§Ù†Ú¯Ù„ÛŒØ³ ',\n",
       " ' Ø±Ù‚Ø§Ø¨Øª Û¹Û¶ ',\n",
       " ' Ø¯Ø§Ø±Ø§ÛŒÛŒ Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒ ',\n",
       " ' Ø§Ø±Ø²Ù¾Ø§Ø´ÛŒ ',\n",
       " ' Ú¯Ù„ÙˆØ¨Ø§Ù„ Ù‡Ø§ÙˆÚ©ØŒ ',\n",
       " ' Ø¯Ù‡Ù‡ Ù†ÙˆØ¯ÛŒ Ù‡Ø§ ',\n",
       " ' ØªØµÙ…ÛŒÙ…Ø§Øª ',\n",
       " ' ÙØ±ØµØª Ù‡Ø§ÛŒ Ø§Ø²Ø¯Ø³Øª Ø±ÙØªÙ‡ ',\n",
       " ' Ø§ØµÙ„Ø§Ø­ Ø·Ù„Ø¨ ',\n",
       " ' Ù†Ù…Ø§Ø² Ø¬Ù…Ø¹Ù‡ ',\n",
       " ' ÙˆÛŒØ¯Ø¦Ùˆ ',\n",
       " ' Ø§Ù…Ù†ÛŒØª ØºØ°Ø§ÛŒÛŒ ',\n",
       " ' Ø§Ø³ØªØ§Ù† Ø³Ù…Ù†Ø§Ù† ',\n",
       " ' Ø§Ù…Ø§Ø±Ø§Øª ',\n",
       " ' Ø´ÙˆØ±Ø§ÛŒ Ù†Ú¯Ù‡Ø¨Ø§Ù† ',\n",
       " ' Ø¹Ø§Ø¯Ù„ Ú©ÛŒØ§Ù†Ù¾ÙˆØ± ',\n",
       " ' Ø¯ÛŒÚ©ØªØ§ØªÙˆØ±ÛŒ Ø¯Ù„Ø§Ø± ',\n",
       " ' Ø¢Ù…Ø±ÛŒÚ©Ø§ ',\n",
       " ' Ù¾ÙŠØ´Ù†Ù‡Ø§Ø¯ ',\n",
       " ' ØªÛŒÙ†Ø§ Ù¾Ø§Ú©Ø±ÙˆØ§Ù† ',\n",
       " ' ÙˆØ±Ø²Ø´Ú©Ø§Ø±Ø§Ù† ',\n",
       " ' ØªÙˆÙ‡ÛŒÙ† Ø¨Ù‡ Ø±Ø¦ÛŒØ³  Ø¬Ù…Ù‡ÙˆØ± ',\n",
       " ' Ø²Ø§Ú©Ø§Ù†ÛŒ ',\n",
       " ' Ø±Ù‚Ø§Ø¨Øª Ù†Ø§Ø³Ø§Ù„Ù… ',\n",
       " ' Ø´ÙˆØ±Ø§ÙŠ Ù†Ú¯Ù‡Ø¨Ø§Ù† ',\n",
       " ' Ø¨Ø±Ø§Ù†Ø¯Ø§Ø²ÛŒ ',\n",
       " ' ØªØ­Ø±ÛŒÙ… Ù‡Ø§ ',\n",
       " ' Ù‡Ù†Ø¯Ø¨Ø§Ù„ ',\n",
       " ' Ù‡ÙØª Ù†Ù‡ ',\n",
       " ' Ø´ÙˆØ±Ø§ÛŒ Ø§Ù…Ù†ÛŒØª ',\n",
       " ' Ù†ÙÙˆØ° ',\n",
       " ' ÙˆØ§Ú©Ø³Ù† Ú©Ø±ÙˆÙ†Ø§ ',\n",
       " ' Ø­Ú©ÛŒÙ… ',\n",
       " ' Ú©Ø´ÙˆØ± ',\n",
       " ' ÙØ±Ø¬ÛŒ Ø¯Ø§Ù†Ø§ ',\n",
       " ' Ù…Ø§Ø¯Ø±Ø§Ù† Ú†Ø´Ù… Ø§Ù†ØªØ¸Ø§Ø± ',\n",
       " ' Ø´ÛŒØ·Ø§Ù† ',\n",
       " ' Ø§ÙˆÚ©Ø±Ø§ÛŒÙ†: ',\n",
       " ' IranTalksVienna ',\n",
       " ' Ø­Ù‚ÙˆÙ‚ Ù…Ø¤Ù„Ù ',\n",
       " ' Ø®Ø¨Ø±Ø®ÙˆØ¨ØŒ ',\n",
       " ' Ø³ØªØ§Ø¯ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ØªØ¸Ø§Ù‡Ø±Ø§Øª ',\n",
       " ' Ù‚Ø·Ø¹Ù†Ø§Ù…Ù‡ ',\n",
       " ' Ø¢Ù„ Ø®Ù„ÙŠÙÙ‡ ',\n",
       " ' Ø§ØµÙˆÙ„ Ú¯Ø±Ø§ ',\n",
       " ' Ø­Ù‚ÙˆÙ‚ ',\n",
       " ' Ø­ØµØ± ',\n",
       " ' Ù†Ø±Ù…Ø´ Ù‚Ù‡Ø±Ù…Ø§Ù†Ø§Ù†Ù‡ ',\n",
       " ' Ø§ÛŒÙ† ÛŒØ¹Ù†ÛŒ ÛŒÚ© Ø§Ø´Ú©Ø§Ù„ÛŒ Ù‡Ø³Øª ',\n",
       " ' Ø²Ù‡Ø±Ø§ Ø´Ø¬Ø§Ø¹ÛŒ ',\n",
       " ' Ù‚Ø·Ø¹Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒ Ù…ØªØ¹Ø¯Ø¯ ',\n",
       " ' Ù…Ø³Ù„Ù…Ø§Ù†Ø§Ù† Ù…Ø¸Ù„ÙˆÙ… Ù‡Ù†Ø¯ ',\n",
       " ' ÙˆØ­Ø¯Øª ',\n",
       " ' Ù…ÙˆØ§Ø¶Ø¹ Ø®Ø·Ø±Ù†Ø§Ú© ',\n",
       " ' ÙŠÙ‡ÙˆØ¯ÙŠØ§Ù† ',\n",
       " ' Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø±Ù†Ú¯ÛŒ ',\n",
       " ' Ú†Ù¾ Ú¯Ø±Ø§ÛŒÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ ',\n",
       " ' Ø§Ø±Ø¨Ø¹ÛŒÙ† ',\n",
       " ' Ø§Ø³ØªØ§Ø¯ Ù¾Ù†Ø§Ù‡ÛŒØ§Ù† ',\n",
       " ' Ú¯ÙØªÚ¯Ùˆ ',\n",
       " ' Ø­Ø§ÙØ¸ ',\n",
       " ' Ø§ØµÙˆÙ„Ú¯Ø±Ø§ÛŒÛŒ ',\n",
       " ' Ø§Ø±ØªØ´ ',\n",
       " ' Ø¯Ù„Ø§Ø± Ø¬Ù‡Ø§Ù†Ú¯ÛŒØ±ÛŒ ',\n",
       " ' Ø¥ÙŠØ±Ø§Ù† ',\n",
       " ' Ø³ÙŠÙ„ÙŠ Ø§ÙˆÙ„Ø› ',\n",
       " ' ØªÙˆØ±Ù…  ',\n",
       " ' Ø¹Ù†Ø§Ø¨Ø³ØªØ§Ù†ÛŒ: ',\n",
       " ' ØªÙ‡Ø¯ÛŒØ¯ ',\n",
       " ' Ø§Ù„Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ ',\n",
       " ' Û±Û³ Ø¢Ø¨Ø§Ù† ',\n",
       " ' Ù†Ø¸Ø±Ø³Ù†Ø¬ÛŒ ',\n",
       " ' FreeIranianSoldiers ',\n",
       " ' Ø­Ø§Ú©Ù…ÛŒØª Ù‚Ø§Ù†ÙˆÙ†ØŒ ',\n",
       " ' Ø±ÙØ±Ø§Ù†Ø¯Ù… ',\n",
       " ' ÙˆØ²Ø§Ø±Øª ØµÙ…Øª ',\n",
       " ' election ',\n",
       " ' ÙØªÙ†Ù‡ ',\n",
       " ' IranTalks ',\n",
       " ' Ø§Ù„Ù†ØµØ± ',\n",
       " ' Ø§ÛŒØ±Ø§Ù† Ø¬Ø¯ÛŒØ¯ ',\n",
       " ' Ø®Ø§Ù†Ù‡ Ø¯Ø§Ø±ÛŒ Ø´ØºÙ„ Ø§Ø³Øª ',\n",
       " ' Ø±ÙˆØ­Ø§Ù†ÛŒØŒ ',\n",
       " ' ØªÙ†Ø¸ÛŒÙ… Ø¨Ø§Ø²Ø§Ø± ',\n",
       " ' Ø¨ÙˆØ¯Ø¬Ù‡ ',\n",
       " ' Ù…Ø¬Ù„Ø³ ÛŒØ§Ø²Ø¯Ù‡Ù… ',\n",
       " ' Ú©Ø§Ù†Ø¯ÛŒØ¯Ø§Ù‡Ø§ÛŒ ',\n",
       " ' ÙØ±Ù‡Ù†Ú¯ÛŒ ',\n",
       " ' Ø§Ø¹Ù…Ø§Ù„ÛŒ ',\n",
       " ' Ø¯ÙˆÙ…ÛŒÙ† Ù†Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ù…Ø¬Ø§Ø²ÛŒ Ú©ØªØ§Ø¨ ',\n",
       " ' ØªØ§Ø¬ÛŒÚ©Ø³ØªØ§Ù† ',\n",
       " ' ØªØ§Ù„Ø´ ',\n",
       " ' Ø®Ø±Ù…Ø´Ù‡Ø± ',\n",
       " ' Ø±Ø³ÙˆÙ„ ',\n",
       " ' Ù…Ø³Ù„Ù…Ø§Ù† Ø¸Ø§Ù„Ù… ',\n",
       " ' Ø±ÙˆØ§ÛŒØª ØªØ­Ø±ÛŒÙ… ',\n",
       " ' PS752 ',\n",
       " ' Ù…Ù„Øª ',\n",
       " ' Ø±ÙˆØ³ÛŒÙ‡ ',\n",
       " ' Ú©Ø§Ø± ',\n",
       " ' Ø®ÛŒØ§Ù†Øª ',\n",
       " ' Ø²Ù†Ø¯Ø§Ù† ',\n",
       " ' Ù…Ø°Ø§ÙƒØ±Ù‡ ',\n",
       " ' Ø²ÙˆØ± Ù…Ø§ Ù…ÛŒ Ø±Ø³Ø¯ ',\n",
       " ' Ø³Ø±Ù…Ø±Ø¨ÛŒ ØªÙˆØ§Ù†Ù…Ù†Ø¯ ',\n",
       " ' ØªØ±Ú© ØªØ§Ø¨Ø¹ÛŒØª ',\n",
       " ' Ø´Ø±ÛŒÚ© ',\n",
       " ' Ø®Ø§Ù…Ù†Ù‡ Ø§ÛŒ ',\n",
       " ' Ù‡Ø±Ù…Ø²Ú¯Ø§Ù† ',\n",
       " ' Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© ',\n",
       " ' ØªÙ‡Ù…Øª ',\n",
       " ' ÙˆÛŒÙ† ',\n",
       " ' Ø¬Ù…Ù‡ÙˆØ±ÛŒ ',\n",
       " ' Û±Û³Ø¢Ø¨Ø§Ù† ',\n",
       " ' IranTeamâ€™s ',\n",
       " ' Ø§Ù…Ø§Ù… Ù…ÙˆØ³ÛŒ ØµØ¯Ø± ',\n",
       " ' Ø­Ø°Ù ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ ',\n",
       " ' Ù„ÛŒØ¨Ø±Ø§Ù„ØŒ ',\n",
       " ' Ø§Ù„Ø¥ØªÙØ§Ù‚ Ø§Ù„Ù†ÙˆÙˆÙŠ. ',\n",
       " ' Ù„ÛŒØ³Øª Ø¬Ù…Ù‡ÙˆØ± ',\n",
       " ' Ú©Ù…Ø§Ù„ÙˆÙ†Ø¯ÛŒ ',\n",
       " ' Ù…Ø­Ø§Ú©Ù…Ù‡ ',\n",
       " ' Ø¨ÛŒØª Ø§Ù„Ù…Ø§Ù„ ',\n",
       " ' Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª ',\n",
       " ' Ø§Ù†Ø±Ú˜ÛŒ ',\n",
       " ' Ø§Ù‚ØªØµØ§Ø¯ÛŒ ',\n",
       " ' Ù„ØºÙˆ ',\n",
       " ' Ø²ÛŒØ± Ù…ÛŒØ² Ø§ÛŒØ±Ø§Ù† Ù‡Ø±Ø§Ø³ÛŒ ',\n",
       " ' Ø¨Ø´Ø§Ø± Ø§Ø³Ø¯ ',\n",
       " ' Ø­Ø¬ ',\n",
       " ' Ø§Ù…ØªØ¯Ø§Ø¯ Ø±ÙˆØ­Ø§Ù†ÛŒ ',\n",
       " ' Ú©Ø§Ø±Ù†Ø§Ù…Ù‡ Ø±ÙˆØ­Ø§Ù†ÛŒ Ø¯Ø± Ø­ÙˆØ²Ù‡ Ø²Ù†Ø§Ù† ',\n",
       " ' Ø¬Ù‡Ø±Ù…ÛŒ ',\n",
       " ' Ø±Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ ',\n",
       " ' Ù†Ù…ÛŒ ØªÙˆØ§Ù†ÛŒ ',\n",
       " ' Ø­Ø³Ø§Ø¨Ù‡Ø§ÛŒ Ø³Ù¾Ø±Ø¯Ù‡ Ù‚ÙˆÙ‡ Ù‚Ø¶Ø§ÛŒÛŒÙ‡ ',\n",
       " ' Ù…Ø³Ù‚Ø·ØŒ ',\n",
       " ' Ù…Ø·Ø¨ÙˆØ¹Ø§Øª ',\n",
       " ' Ø§Ù‚ØªØµØ§Ø¯ ØªØ­Ø±ÛŒÙ… ',\n",
       " ' Ù…Ø­Ø³Ù† Ø±Ø¶Ø§ÛŒÛŒ ',\n",
       " ' Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§ÛŒ Ø§ÙˆÚ©Ø±Ø§ÛŒÙ†ÛŒ ',\n",
       " ' ØµØ¯Ø§ÛŒ Ø¨ÛŒ Ù‚Ø¯Ø±ØªØ§Ù† ',\n",
       " ' Ù¾Ø¯Ø§ÙÙ†Ø¯ ØºÛŒØ±Ø¹Ø§Ù…Ù„ ',\n",
       " ' Ø­Ø°Ù Ø¯Ù„Ø§Ø± ',\n",
       " ' Ù…ÙØ³Ø¯Ø§Ù† Ø§Ù‚ØªØµØ§Ø¯ÛŒ ',\n",
       " ' Ù…Ø§ Ù…Ù„Øª Ø§Ù…Ø§Ù… Ø­Ø³ÛŒÙ†ÛŒÙ… ',\n",
       " ' ÙˆØ§Ú©Ø³Ù† Ø¢Ù„ÙˆØ¯Ù‡ Ø¢Ù…Ø±ÛŒÚ©Ø§ÛŒÛŒ ',\n",
       " ' Ø±Ø§Ø¦Û’ Ø´Ù…Ø§Ø±ÛŒ ',\n",
       " ' Ø®Ø´ÙˆÙ†Øª Ø¹Ù„ÛŒÙ‡ Ø²Ù†Ø§Ù† ',\n",
       " ' Ø¢ØªØ´ Ø¨Ù‡ Ø§Ø®ØªÛŒØ§Ø± ',\n",
       " ' Ø³ÛŒÙ„ÛŒ Ø³Ù¾Ø§Ù‡ ',\n",
       " ' Ú˜Ø§Ù¾Ù† ',\n",
       " ' Ù‡Ù…Ø±Ø²Ù…Ø§Ù† ',\n",
       " ' Ø³Ù†ØµÙ„ÙŠ ÙÛŒ Ø§Ù„Ù‚Ø¯Ø³ ',\n",
       " ' Ø®Ø§Ù…Ù†Ù‡ Ø§ÛŒ: ',\n",
       " ' Ù…Ø¬Ø§Ø²Ø§Øª ',\n",
       " ' Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ ',\n",
       " ' TokyoParalympics2020 ',\n",
       " ' Ø¯ÙˆÙ„Øª Ù†Ø¸Ø§Ù…ÛŒØ§Ù†ØŒ ',\n",
       " ' Ù„Ø¨Ù†Ø§Ù† ',\n",
       " ' Ø·Ø±Ø­ ÙˆØ§Ù† ',\n",
       " ' ÙØ³Ø§Ø¯ Ø®ÙˆØ¯ÛŒ ',\n",
       " ' Ú©ÛŒÙ Ú©Ø´! ',\n",
       " ' Ø§ÙˆØ±Ø§Ù‚ Ø¨Ø¯Ù‡ÛŒ ',\n",
       " ' Ø¬Ø±Ø§Ø­ÛŒ Ø¯Ø±ÙˆÙ† Ù†Ø¸Ø§Ù… ',\n",
       " ' Ø¢Ù„Ù…Ø§Ù† ',\n",
       " ' ØªÙ†Ú¯Ù‡ Ù‡Ø±Ù…Ø² ',\n",
       " ' Ø§Ø³ØªÙ‚Ø±Ø§Ø¶ ØºÛŒØ±Ù…Ø³ØªÙ‚ÛŒÙ… ',\n",
       " ' Ø±ÛŒØ§Ø³Øª Ø¬Ù…Ù‡ÙˆØ±ÛŒØŒ ',\n",
       " ' Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Û±Û´Û°Û° ',\n",
       " ' Ø§Ø´Ø¯ Ù…Ø¬Ø§Ø²Ø§Øª ',\n",
       " ' Ù‚Ù‡Ø±Ù…Ø§Ù† ',\n",
       " ' Ø³ÛŒØ³ØªØ§Ù† ÙˆØ¨Ù„ÙˆÚ†Ø³ØªØ§Ù† ',\n",
       " ' Ø²Ù†Ø¯Ø§Ù†ÛŒØ§Ù† ',\n",
       " ' Ø³ÙÛŒØ± ',\n",
       " ' Ø¨ÛŒ Ø§Ø«Ø±Ø³Ø§Ø²ÛŒ ØªØ­Ø±ÛŒÙ… Ù‡Ø§ ',\n",
       " ' Ø¯ÙˆØ´ØºÙ„Ù‡( ',\n",
       " ' Ù…Ø§Ù„ÛŒØ§ØªÛŒ ',\n",
       " ' Ú©Ù„Ø§Ø¨ Ù‡Ø§ÙˆØ³ ',\n",
       " ' Ø´ÙØ§Ù Ø³Ø§Ø²ÛŒ ',\n",
       " ' Ø¢Ù‚Ø§ Ø®ÙˆØ´Ø´ Ø¨ÛŒØ§ÛŒØ¯ØŒ ',\n",
       " ' Ø±Ø§Ø¨Ø·Ù‡ ',\n",
       " ' ØªØ­Ø±ÛŒÙ… Ø¯Ø§Ø®Ù„ÛŒ ',\n",
       " ' Ù†Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ ',\n",
       " ' Ø¹Ø²Øª ',\n",
       " ' Ù†Ø§Ù…Ù‡ Ø±Ø¦ÛŒØ³ Ù‚ÙˆÙ‡ Ù‚Ø¶Ø§Ø¦ÛŒÙ‡ ',\n",
       " ' ØªØ­Ø±ÙŠÙ… ',\n",
       " ' Ø³Ø¹Ø¯ÛŒ: ',\n",
       " ' Ú©Ø§Ù…ÛŒÙˆÙ†Ø¯Ø§Ø±Ø§Ù† ',\n",
       " ' Ø¶Ø¯ ØªØ¨Ø¹ÛŒØ¶ ',\n",
       " ' Ù…Ø±Ø§ Ø¯Ø±ÛŒØ§Ø¨ ',\n",
       " ' ØªØ±Ø§Ù…Ø¨! ',\n",
       " ' ÙØ³Ø§Ø¯ ',\n",
       " ' ÙˆØ²ÛŒØ± Ø§Ø·Ù„Ø§Ø¹Ø§Øª ',\n",
       " ' Ø­Ø§Ù…Ø¯ Ø§Ù…ÛŒØ±ÛŒ ',\n",
       " ' Ø¢Ø°Ø±Ø¨Ø§ÛŒØ¬Ø§Ù† ØºØ±Ø¨ÛŒ ',\n",
       " ' Ø­Ù…ÛŒØ¯ Ù‡ÙˆØ´Ù†Ú¯ÛŒ ',\n",
       " ' Ø¯Ù…ÙˆÚ©Ø±Ø§Ø³ÙŠ ',\n",
       " ' Ø¢Ø±Ø§Ù…Ø´ ',\n",
       " ' Ø³Ù‡Ø§Ù… Ø¹Ø¯Ø§Ù„Øª. ',\n",
       " ' HERO ',\n",
       " ' Ø±Ø§Ù†Øª Ùˆ Ø§Ù†Ø­ØµØ§Ø± ',\n",
       " ' Ù†Ø§Ù…Ø²Ø¯Ù‡Ø§ ',\n",
       " ' Ø§ÛŒÙ†Ø³ØªØ§Ú¯Ø±Ø§Ù… ',\n",
       " ' Ø§Ù„Ø²Ø§Ù…Ø§Øª Ø¬Ù…Ù‡ÙˆØ±ÛŒ ',\n",
       " ' Ø±ÙØ¹ ØªØ­Ø±ÛŒÙ… ',\n",
       " ' Ø±Ú˜ÛŒÙ… ØµÙ‡ÛŒÙˆÙ†ÛŒØ³ØªÛŒ ',\n",
       " ' Ø§ØµÙ„Ø§Ø­Ø§Øª Ø³Ø§Ø®ØªØ§Ø±ÛŒ ',\n",
       " ' ÙÙ„Ø³Ø·ÛŒÙ† Ø­Ù„= ',\n",
       " ' Ø´Ø±ÛŒÙÛŒ ',\n",
       " ' Ø¨ÙˆÙ„ØªÙˆÙ† ',\n",
       " ' Ø­Ø§Ø´ÛŒÙ‡ Ø³Ø§Ø²ÛŒ ',\n",
       " ' Ø¬Ù†Ú¯ Ù†Ø±Ù… ',\n",
       " ' Ø¹Ø¨Ø§Ø³ Ø¹Ø¨Ø¯ÛŒ ',\n",
       " ' Ø±Ù‡Ø§ÛŒÛŒ Ø¨Ø®Ø´ ',\n",
       " ' Ø±Ø§ÛŒ Ù…ÛŒØ¯Ù‡Ù… ',\n",
       " ' Ø±ÙˆØ­Ø§Ù†ÛŒØª ',\n",
       " ' Ø³ÛŒØ³ØªØ§Ù† Ùˆ Ø¨Ù„ÙˆÚ†Ø³ØªØ§Ù† ',\n",
       " ' Ø´ÙˆØ±Ø§ÛŒ Ø³Ù†Ø¬Ø´ ',\n",
       " ' Ú©Ø§Ø± Ø¢ÙØ±ÛŒÙ†ÛŒ ',\n",
       " ' Ù‚ÙˆÙ‡ Ù‚Ø¶Ø§Ø¦ÛŒÙ‡ ',\n",
       " ' Ù†Ù‡ Ø¨Ù‡ Ù¾Ù„Ø§Ø³ØªÛŒÚ© ',\n",
       " ' Ø§Ø±Ø¨Ø¹ÛŒÙ†ÛŒ Ù‡Ø§ ',\n",
       " ' ØªØµÙ…ÛŒÙ…Ø§Øª Ù…Ø­ÙÙ„ÛŒ ',\n",
       " ' Ù‡Ø¬Ø¯Ù‡ ØªÛŒØ± ',\n",
       " ' Ø±ÙØ¹ ØªØ­Ø±ÙŠÙ… ',\n",
       " ' Ø§Ø³ØªØ§Ù† Ú¯ÛŒÙ„Ø§Ù† ',\n",
       " ' Ø§Ø­Ù…Ø¯ÛŒ Ù†Ú˜Ø§Ø¯ØŒ ',\n",
       " ' Ø´Ø®ØµÛŒØª Ø³ÛŒØ§Ø³ÛŒ Ù…Ø°Ù‡Ø¨ÛŒ ',\n",
       " ' Ø±Ø¦ÛŒØ³ Ø¬Ù…Ù‡ÙˆØ± ',\n",
       " ' Ù¾Ø±ÙˆÚ˜Ù‡ Ù†ÙÙˆØ° ',\n",
       " ...]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert multihahstags to words\n",
    "hashtag = []\n",
    "for ht_list in data['hashtags']:\n",
    "    for ht in ht_list:\n",
    "        hashtag.append(ht)\n",
    "hash_set = set(hashtag)\n",
    "ht_list = []\n",
    "for h in hash_set:\n",
    "    ht_list.append(h)\n",
    "hash_list = []\n",
    "for h in hash_set:\n",
    "    hash_list.append(' '+ ' '.join(h[1:].split('_')) + ' ')\n",
    "hash_list2 = []\n",
    "for h in hash_list:\n",
    "    hash_list2.append(' '.join(h.split('\\u200c')))\n",
    "hash_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d2a985a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if data['tweet_num_hashtags'].iloc[i] > 0:\n",
    "        for h in data['hashtags'].iloc[i]:\n",
    "            ind = ht_list.index(h)\n",
    "            data['full_text'].iloc[i] = data['full_text'].iloc[i].replace(h, hash_list2[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c6140a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    stop_words = list()\n",
    "    with open(r'C:/PRIVATE/Metodata/Metodata-Files/stopwords_new.txt',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stop_words.append(line.strip())\n",
    "    exclusions = [\"ir\",\"#ff\", \"ff\", \"rt\",\"RT\", \"FF\",\"\\u200c\",\"\\n\",\"'s\",\"n't\",\"'re\",\"'m\",'#','@','&','?','.','+','-','*','/','â€™','...','â€¦','â€˜','â€œ','â€','â€“','ØŸ','ØŒ','.','\"',';','!',':','%','.',',']\n",
    "    stop_words.extend(exclusions)\n",
    "\n",
    "    junk_chars_regex = r'[^a-zA-Z\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF \\u200c]'\n",
    "    url_regex = r\"\"\"((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b\"\"\"\n",
    "    RFC_5322_COMPLIANT_EMAIL_REGEX = r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\"\n",
    "    compile_patterns = lambda patterns: [(re.compile(pattern), repl) for pattern, repl in patterns]\n",
    "    \n",
    "    remove_url = (url_regex, ' ')\n",
    "    remove_email = (RFC_5322_COMPLIANT_EMAIL_REGEX, ' ')\n",
    "    remove_junk_characters = (junk_chars_regex, ' ')\n",
    "    compiled_patterns_before = compile_patterns([remove_url, remove_email])\n",
    "    compiled_patterns_after = compile_patterns([remove_junk_characters])\n",
    "    \n",
    "    text = text.lower()\n",
    "    for pattern, repl in compiled_patterns_before:\n",
    "        text = pattern.sub(repl, text)\n",
    "    text = re.sub(r'[\\u200c\\s]*\\s[\\s\\u200c]*', ' ', text)\n",
    "    text = re.sub(r'[\\u200c]+', '\\u200c', text)\n",
    "\n",
    "    for pattern, repl in compiled_patterns_after:\n",
    "        text = pattern.sub(repl, text)\n",
    "\n",
    "    tokenized_words = tokenizer.tokenize(text)\n",
    "    tokenized_words = [word for word in tokenized_words if word not in stop_words]\n",
    "\n",
    "    return tokenized_words\n",
    "\n",
    "def remove_emoji(text):\n",
    "    junk_chars_regex = r'[^a-zA-Z\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF \\u200c]'\n",
    "    compile_patterns = lambda patterns: [(re.compile(pattern), repl) for pattern, repl in patterns]\n",
    "    remove_junk_characters = (junk_chars_regex, ' ')\n",
    "    compiled_patterns_after = compile_patterns([remove_junk_characters])\n",
    "    for pattern, repl in compiled_patterns_after:\n",
    "        text = pattern.sub(repl, text)\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def make_farsi_text(x):\n",
    "    reshaped_text = arabic_reshaper.reshape(x)\n",
    "    farsi_text = get_display(reshaped_text)\n",
    "    return farsi_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "65516bd5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 50)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_5080/2797418508.py:6: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_5080/2797418508.py:7: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "data['full_text']=data['full_text'].fillna('')\n",
    "print(data.shape)\n",
    "data['full_text'] = [str(i).lower() for i in data['full_text']]\n",
    "print('1')\n",
    "data['full_text']=data['full_text'].str.replace('\\d+', '')\n",
    "data['full_text'] = data['full_text'].str.replace('@[\\w\\-]+','')\n",
    "print('2')\n",
    "data['full_text']=data['full_text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "print('3')\n",
    "data['full_text']=data['full_text'].apply(lambda x:remove_emoji(x) )\n",
    "print('4')\n",
    "data['full_text']=data['full_text'].apply(lambda x:preprocess(x))\n",
    "print('5')\n",
    "data['full_text']=data['full_text'].apply(lambda x:' '.join([word for word in x]))\n",
    "print('6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce9516",
   "metadata": {},
   "source": [
    "# Negativity/Character Attack/Political Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bd5192ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Character Attack and Political Attack run following comment\n",
    "\n",
    "data = data[data['neg'] == 1]\n",
    "\n",
    "#for Character Attack change data['neg'] to data['char'] in code\n",
    "#for Political Attack change data['neg'] to data['pol'] in code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1197da",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e55a2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, data['neg'], test_size=0.2, random_state=0, stratify=data['neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2a8ce",
   "metadata": {},
   "source": [
    "## Undersmapling - Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "4d626faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 50)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_data = X_train[X_train['neg'] == 1]\n",
    "positive_data = X_train[X_train['neg'] == 0]\n",
    "'''\n",
    "#Undersampling\n",
    "cutting_point = min(len(negative_data), len(positive_data))\n",
    "\n",
    "if cutting_point <= len(negative_data):\n",
    "    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "if cutting_point <= len(positive_data):\n",
    "    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)'''\n",
    "\n",
    "#Oversampling\n",
    "'''if len(positive_data) > len(negative_data):\n",
    "    negative_data = negative_data.sample(n=len(positive_data), replace=True).reset_index(drop=True)\n",
    "\n",
    "if len(positive_data) < len(negative_data):\n",
    "    positive_data = positive_data.sample(n=len(negative_data), replace=True).reset_index(drop=True)'''\n",
    "\n",
    "X_train = pd.concat([negative_data, positive_data])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccf2fa",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e6212911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 300)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_n_train = []\n",
    "for w in X_train['full_text']:\n",
    "    vecs_n_train.append(ft.get_word_vector(w))\n",
    "vecs_n_train = np.array(vecs_n_train)\n",
    "vecs_n_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "320a28af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 300)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_n_test = []\n",
    "for w in X_test['full_text'].to_list():\n",
    "    vecs_n_test.append(ft.get_word_vector(w))\n",
    "vecs_n_test = np.array(vecs_n_test)\n",
    "vecs_n_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "4fa0d51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831,)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n_train = X_train['neg'].to_list()\n",
    "y_n_train = np.array(y_n_train)\n",
    "y_n_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d547a40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208,)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n_test = y_test\n",
    "y_n_test = np.array(y_n_test)\n",
    "y_n_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "088b4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf0af4",
   "metadata": {},
   "source": [
    "## Textual Features for Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "7cd4c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pd.DataFrame({'full_text':X_train['full_text'], 'neg':X_train['neg']})\n",
    "df_s1 = S[S['neg'] == 1]\n",
    "df_s0 = S[S['neg'] == 0]\n",
    "def Merge(D1,D2):\n",
    "    py = D1 | D2\n",
    "    return py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "accfab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sort_Dic = {}\n",
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l1 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l2 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l3 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "616ba2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 850)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_Top = [x[0] for x in l1[:400]] + [x[0] for x in l2[:100]] + [x[0] for x in l3[:50]]\n",
    "L = X_train['full_text'].to_list()\n",
    "top_dic = {}\n",
    "for i in range(len(dic_Top)):\n",
    "    k = []\n",
    "    for j in range(len(L)):\n",
    "        if(dic_Top[i] in L[j]):\n",
    "            k.append(1)\n",
    "        else:\n",
    "            k.append(0)\n",
    "    top_dic[dic_Top[i]] = k\n",
    "df_top_s = pd.DataFrame(top_dic)\n",
    "vecs_n_train = pd.DataFrame(vecs_n_train)\n",
    "df_top_s = df_top_s.reset_index()\n",
    "vecs_n_train = vecs_n_train.reset_index()\n",
    "dic_all_train = pd.concat([vecs_n_train, df_top_s], axis=1)\n",
    "dic_all_train = dic_all_train.drop({'index'}, axis=1)\n",
    "dic_all_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "634faa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_1 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_2 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_3 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_1 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_2 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_3 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "sort_dic0_n = Merge(sort_dic0_1, sort_dic0_2)\n",
    "sort_dic0 = Merge(sort_dic0_n, sort_dic0_3)\n",
    "\n",
    "sort_dic1_n = Merge(sort_dic1_1, sort_dic1_2)\n",
    "sort_dic1 = Merge(sort_dic1_n, sort_dic1_3)\n",
    "\n",
    "sort_dic0 = dict(sorted(sort_dic0.items(), key=lambda item: item[1] , reverse=True))\n",
    "sort_dic1 = dict(sorted(sort_dic1.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "Top0 = [x for x in list(sort_dic0_1.keys())][:500] + [x for x in list(sort_dic0_2.keys())][:100] + [x for x in list(sort_dic0_3.keys())][:50]\n",
    "Top1 = [x for x in list(sort_dic1_1.keys())][:500] + [x for x in list(sort_dic1_2.keys())][:100] + [x for x in list(sort_dic1_3.keys())][:50]\n",
    "\n",
    "Top01 = [x for x in list(sort_dic0_1.keys()) if x not in sort_dic1_1.keys()][:500] + [x for x in list(sort_dic0_2.keys()) if x not in sort_dic1_2.keys()][:100] + [x for x in list(sort_dic0_3.keys()) if x not in sort_dic1_3.keys()][:100]\n",
    "Top10 = [x for x in list(sort_dic1_1.keys()) if x not in sort_dic0_1.keys()][:500] + [x for x in list(sort_dic1_2.keys()) if x not in sort_dic0_2.keys()][:100] + [x for x in list(sort_dic1_3.keys()) if x not in sort_dic0_3.keys()][:100]\n",
    "\n",
    "Top0_w = [x for x in list(sort_dic0_1.keys())][:500] + [x for x in list(sort_dic0_2.keys())][:100] + [x for x in list(sort_dic0_3.keys())][:50]\n",
    "Top1_w = [x for x in list(sort_dic1_1.keys())][:500] + [x for x in list(sort_dic1_2.keys())][:100] + [x for x in list(sort_dic1_3.keys())][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "98636e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 831, 831, 831, 831, 831)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_Top0 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0):\n",
    "                count += 1\n",
    "        feature_Top0.append(count)\n",
    "    else:\n",
    "        feature_Top0.append(0)\n",
    "        \n",
    "feature_Top1 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1):\n",
    "                count += 1\n",
    "        feature_Top1.append(count)\n",
    "    else:\n",
    "        feature_Top1.append(0)\n",
    "        \n",
    "feature_Top01 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top01):\n",
    "                count += 1\n",
    "        feature_Top01.append(count)\n",
    "    else:\n",
    "        feature_Top01.append(0)\n",
    "        \n",
    "feature_Top10 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top10):\n",
    "                count += 1\n",
    "        feature_Top10.append(count)\n",
    "    else:\n",
    "        feature_Top10.append(0)\n",
    "        \n",
    "feature_weight1 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1_w):\n",
    "                count += sort_dic1[tweet[i]]\n",
    "        feature_weight1.append(count)\n",
    "    else:\n",
    "        feature_weight1.append(0)\n",
    "\n",
    "feature_weight0 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0_w):\n",
    "                count += sort_dic0[tweet[i]]\n",
    "        feature_weight0.append(count)\n",
    "    else:\n",
    "        feature_weight0.append(0)\n",
    "\n",
    "len(feature_Top0), len(feature_Top1), len(feature_Top01), len(feature_Top10), len(feature_weight1), len(feature_weight0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f17353ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((831, 868), 831)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train = pd.DataFrame(\n",
    "    {'50Top_0': feature_Top0,\n",
    "     '50Top_1': feature_Top1,\n",
    "     'Top_0_1': feature_Top01,\n",
    "     'Top_1_0': feature_Top10,\n",
    "     'Weight_0': feature_weight0,\n",
    "     'Weight_1': feature_weight1})\n",
    "features_train['tweet_like'] = X_train['tweet_like']\n",
    "features_train['tweet_retweet_count'] = X_train['tweet_retweet_count']\n",
    "features_train['tweet_length_word'] = X_train['tweet_length_word']\n",
    "features_train['tweet_length_characters'] = X_train['tweet_length_characters']\n",
    "features_train['tweet_num_hashtags'] = X_train['tweet_num_hashtags']\n",
    "features_train['tweet_num_mention'] = X_train['tweet_num_mention']\n",
    "features_train['tweet_num_urls'] = X_train['tweet_num_urls']\n",
    "features_train['tweet_num_emoji'] = X_train['tweet_num_emoji']\n",
    "features_train['tweet_num_punctuation'] = X_train['tweet_num_punctuation']\n",
    "features_train['person_names'] = X_train['person_names']\n",
    "features_train['organize_names'] = X_train['organize_names']\n",
    "features_train['swear_words'] = X_train['swear_words']\n",
    "\n",
    "X_train_new = pd.concat([dic_all_train , features_train], axis=1)\n",
    "X_train_new = X_train_new.fillna(0)\n",
    "y_train_new = X_train['neg'].to_list()\n",
    "scaler_train = Normalizer()\n",
    "scaler_train.fit(X_train_new)\n",
    "X_train_new = scaler_train.transform(X_train_new)\n",
    "X_train_new.shape, len(y_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91db43",
   "metadata": {},
   "source": [
    "## Textual Features for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "980a5857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 850)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_Top = [x[0] for x in l1[:400]] + [x[0] for x in l2[:100]] + [x[0] for x in l3[:50]]\n",
    "L = X_test['full_text'].to_list()\n",
    "top_dic = {}\n",
    "for i in range(len(dic_Top)):\n",
    "    k = []\n",
    "    for j in range(len(L)):\n",
    "        if(dic_Top[i] in L[j]):\n",
    "            k.append(1)\n",
    "        else:\n",
    "            k.append(0)\n",
    "    top_dic[dic_Top[i]] = k\n",
    "df_top_s = pd.DataFrame(top_dic)\n",
    "vecs_n_test = pd.DataFrame(vecs_n_test)\n",
    "df_top_s = df_top_s.reset_index()\n",
    "vecs_n_test = vecs_n_test.reset_index()\n",
    "dic_all_test = pd.concat([vecs_n_test, df_top_s], axis=1)\n",
    "dic_all_test = dic_all_test.drop({'index'}, axis=1)\n",
    "dic_all_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9b90281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(208, 208, 208, 208, 208, 208)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_Top0_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0):\n",
    "                count += 1\n",
    "        feature_Top0_t.append(count)\n",
    "    else:\n",
    "        feature_Top0_t.append(0)\n",
    "\n",
    "feature_Top1_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1):\n",
    "                count += 1\n",
    "        feature_Top1_t.append(count)\n",
    "    else:\n",
    "        feature_Top1_t.append(0)\n",
    "\n",
    "feature_Top01_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top01):\n",
    "                count += 1\n",
    "        feature_Top01_t.append(count)\n",
    "    else:\n",
    "        feature_Top01_t.append(0)\n",
    "        \n",
    "feature_Top10_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top10):\n",
    "                count += 1\n",
    "        feature_Top10_t.append(count)\n",
    "    else:\n",
    "        feature_Top10_t.append(0)\n",
    "        \n",
    "feature_weight1_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1_w):\n",
    "                count += sort_dic1[tweet[i]]\n",
    "        feature_weight1_t.append(count)\n",
    "    else:\n",
    "        feature_weight1_t.append(0)\n",
    "        \n",
    "feature_weight0_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0_w):\n",
    "                count += sort_dic0[tweet[i]]\n",
    "        feature_weight0_t.append(count)\n",
    "    else:\n",
    "        feature_weight0_t.append(0)\n",
    "        \n",
    "len(feature_Top0_t), len(feature_Top1_t), len(feature_Top01_t), len(feature_Top10_t), len(feature_weight1_t), len(feature_weight0_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "54d08b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(208, 868)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test = pd.DataFrame(\n",
    "    {'50Top_0': feature_Top0_t,\n",
    "     '50Top_1': feature_Top1_t,\n",
    "     'Top_0_1': feature_Top01_t,\n",
    "     'Top_1_0': feature_Top10_t,\n",
    "     'Weight_0': feature_weight0_t,\n",
    "     'Weight_1': feature_weight1_t\n",
    "      })\n",
    "features_test['tweet_like'] = X_test['tweet_like']\n",
    "features_test['tweet_retweet_count'] = X_test['tweet_retweet_count']\n",
    "features_test['tweet_length_word'] = X_test['tweet_length_word']\n",
    "features_test['tweet_length_characters'] = X_test['tweet_length_characters']\n",
    "features_test['tweet_num_hashtags'] = X_test['tweet_num_hashtags']\n",
    "features_test['tweet_num_mention'] = X_test['tweet_num_mention']\n",
    "features_test['tweet_num_urls'] = X_test['tweet_num_urls']\n",
    "features_test['tweet_num_emoji'] = X_test['tweet_num_emoji']\n",
    "features_test['tweet_num_punctuation'] = X_test['tweet_num_punctuation']\n",
    "features_test['person_names'] = X_test['person_names']\n",
    "features_test['organize_names'] = X_test['organize_names']\n",
    "features_test['swear_words'] = X_test['swear_words']\n",
    "\n",
    "X_test_new = pd.concat([dic_all_test , features_test], axis=1)\n",
    "X_test_new = X_test_new.fillna(0)\n",
    "scaler_test = Normalizer()\n",
    "scaler_test.fit(X_test_new)\n",
    "X_test_new = scaler_test.transform(X_test_new)\n",
    "X_test_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45f8bf",
   "metadata": {},
   "source": [
    "## Classical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ad067",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLP\n",
    "'''parameter_space = {\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive']}'''\n",
    "\n",
    "clf1 = MLPClassifier(random_state=0, max_iter=400)\n",
    "#clf1 = GridSearchCV(mlp, parameter_space, cv=5, n_jobs=1)\n",
    "clf1.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('****MLP classificaion report for test set****')\n",
    "predictions1 = clf1.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions1))\n",
    "print(metrics.roc_auc_score(y_test,predictions1))\n",
    "\n",
    "print('\\n****MLP classificaion report for train set****')\n",
    "predictions1_1 = clf1.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_1))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_1))\n",
    "\n",
    "#XGB\n",
    "'''parameter_space={'max_depth': range(3, 18),\n",
    "                    'gamma': range(1,9)}'''\n",
    "\n",
    "eval_set = [(X_test_new, y_test)]\n",
    "clf2 = XGBClassifier(random_state=0, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)\n",
    "#clf2 = GridSearchCV(xgb, parameter_space, cv=3)\n",
    "clf2.fit(X_train_new, y_train_new, verbose=True)\n",
    "\n",
    "print('\\n\\n\\n\\n****XGB classificaion report for test set****')\n",
    "predictions2 = clf2.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions2))\n",
    "print(metrics.roc_auc_score(y_test,predictions2))\n",
    "\n",
    "print('\\n****XGB classificaion report for train set****')\n",
    "predictions1_2 = clf2.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_2))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_2))\n",
    "\n",
    "#RF\n",
    "'''parameter_space={'max_depth' : list(range(1, 50))}'''\n",
    "\n",
    "clf3 = RandomForestClassifier(random_state=0)\n",
    "#clf3 = GridSearchCV(rf, parameter_space, cv=3, scoring=p_score)\n",
    "clf3.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****RF classificaion report for test set****')\n",
    "predictions3 = clf3.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions3))\n",
    "print(metrics.roc_auc_score(y_test,predictions3))\n",
    "\n",
    "print('\\n****RF classificaion report for train set****')\n",
    "predictions1_3 = clf3.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_3))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_3))\n",
    "\n",
    "#LR\n",
    "'''parameter_space={'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'penalty' : ['l2'],\n",
    "                'C' : [100, 10, 1.0, 0.1, 0.01]}'''\n",
    "\n",
    "clf4 = LogisticRegression(random_state=0)\n",
    "#clf4 = GridSearchCV(lr, parameter_space, cv=3, scoring=p_score)\n",
    "clf4.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****LR classificaion report for test set****')\n",
    "predictions4 = clf4.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions4))\n",
    "print(metrics.roc_auc_score(y_test,predictions4))\n",
    "\n",
    "print('\\n****LR classificaion report for train set****')\n",
    "predictions1_4 = clf4.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_4))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_4))\n",
    "\n",
    "#NB\n",
    "clf5 = GaussianNB()\n",
    "clf5.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****NB classificaion report for test set****')\n",
    "predictions5 = clf5.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions5))\n",
    "print(metrics.roc_auc_score(y_test,predictions5))\n",
    "\n",
    "print('\\n****NB classificaion report for train set****')\n",
    "predictions1_5 = clf5.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_5))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_5))\n",
    "\n",
    "#SVM\n",
    "clf6 = svm.SVC(random_state=0)\n",
    "clf6.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****SVM classificaion report for test set****')\n",
    "predictions6 = clf6.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions6))\n",
    "print(metrics.roc_auc_score(y_test,predictions6))\n",
    "\n",
    "print('\\n****SVM classificaion report for train set****')\n",
    "predictions1_6 = clf6.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_6))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_6))\n",
    "\n",
    "#SGD\n",
    "clf7 = SGDClassifier(max_iter=2000, tol=1e-3, random_state=0)\n",
    "clf7.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****SGD classificaion report for test set****')\n",
    "predictions7 = clf7.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions7))\n",
    "print(metrics.roc_auc_score(y_test,predictions7))\n",
    "\n",
    "print('\\n****SGD classificaion report for train set****')\n",
    "predictions1_7 = clf7.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_7))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_7))\n",
    "\n",
    "#KNN\n",
    "'''parameter_space = {'n_neighbors' : list(range(1,300))}'''\n",
    "clf8 = KNeighborsClassifier()\n",
    "#clf8 = GridSearchCV(kn, parameter_space, n_jobs=-1, cv=5, scoring='f1')\n",
    "clf8.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****KNN classificaion report for test set****')\n",
    "predictions8 = clf8.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions8))\n",
    "print(metrics.roc_auc_score(y_test,predictions8))\n",
    "\n",
    "print('\\n****KNN classificaion report for train set****')\n",
    "predictions1_8 = clf8.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_8))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1b5b8",
   "metadata": {},
   "source": [
    "## Feature Importance Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0a4d0b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#feature importances for MLP\n",
    "l1 = []\n",
    "l2 = clf1.coefs_[0]\n",
    "for i in l2:\n",
    "    l1.append(sum(i))\n",
    "importance_1 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_1.append(i)\n",
    "importance_1 = [(float(i)-min(importance_1))/(max(importance_1)-min(importance_1)) for i in importance_1]\n",
    "\n",
    "#feature importances for XGB\n",
    "l1 = []\n",
    "l2 = clf2.feature_importances_\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_2 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_2.append(i)\n",
    "importance_2\n",
    "importance_2 = [(float(i)-min(importance_2))/(max(importance_2)-min(importance_2)) for i in importance_2]\n",
    "\n",
    "#feature importances for RF\n",
    "l1 = []\n",
    "l2 = clf3.feature_importances_\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_3 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_3.append(i)\n",
    "importance_3 = [(float(i)-min(importance_3))/(max(importance_3)-min(importance_3)) for i in importance_3]\n",
    "\n",
    "#feature importances for LR\n",
    "l1 = []\n",
    "l2 = clf4.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_4 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_4.append(i)\n",
    "importance_4 = [(float(i)-min(importance_4))/(max(importance_4)-min(importance_4)) for i in importance_4]\n",
    "\n",
    "#feature importances for NB\n",
    "l1 = []\n",
    "l2 = permutation_importance(clf5, X_test_new, y_test)\n",
    "l2 = l2.importances_mean\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_5 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_5.append(i)\n",
    "importance_5 = [(float(i)-min(importance_5))/(max(importance_5)-min(importance_5)) for i in importance_5]\n",
    "\n",
    "#feature importances for SVM\n",
    "l1 = []\n",
    "clf6 = svm.SVC(random_state=0, kernel='linear')\n",
    "clf6.fit(X_train_new, y_train_new)\n",
    "l2 = clf6.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_6 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_6.append(i)\n",
    "importance_6 = [(float(i)-min(importance_6))/(max(importance_6)-min(importance_6)) for i in importance_6]\n",
    "\n",
    "#feature importances for SGD\n",
    "l1 = []\n",
    "l2 = clf7.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_7 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_7.append(i)\n",
    "importance_7 = [(float(i)-min(importance_7))/(max(importance_7)-min(importance_7)) for i in importance_7]\n",
    "\n",
    "#feature importances for KNN\n",
    "l1 = []\n",
    "l2 = permutation_importance(clf, X_test_new, y_test, scoring='neg_mean_squared_error')\n",
    "l2 = l2.importances_mean\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_8 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_8.append(i)\n",
    "importance_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "886d8674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "col_name = ['embedding','unigram','bigram','trigram','Top_0','Top_1','Top_0_1','Top_1_0','Weight_0',\n",
    "            'Weight_1','like','retweet','len_word','len_chars','hashtags','mention','urls','emoji',\n",
    "            'punctuation','person','organize','swear']\n",
    "N = 12\n",
    "ind = np.arange(N)\n",
    "width = 0.1\n",
    "\n",
    "xvals = importance_1[10:]\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'r')\n",
    "  \n",
    "yvals = importance_2[10:]\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='g')\n",
    "\n",
    "zvals = importance_3[10:]\n",
    "bar3 = plt.bar(ind+width*2, zvals, width, color = 'b')\n",
    "\n",
    "z1vals = importance_4[10:]\n",
    "bar4 = plt.bar(ind+width*3, z1vals, width, color = 'c')\n",
    "\n",
    "z2vals = importance_5[10:]\n",
    "bar5 = plt.bar(ind+width*4, z2vals, width, color = 'm')\n",
    "\n",
    "z3vals = importance_6[10:]\n",
    "bar6 = plt.bar(ind+width*5, z3vals, width, color = 'y')\n",
    "\n",
    "z4vals = importance_7[10:]\n",
    "bar7 = plt.bar(ind+width*6, z4vals, width, color = 'lime')\n",
    "\n",
    "z5vals = importance_8[10:]\n",
    "bar8 = plt.bar(ind+width*7, z5vals, width, color = 'violet')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(23, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=15)\n",
    "plt.title(\"Textual Feature Importance for Negativity Detection Algorithms(for 2382 tweets with underampling)\",fontsize=15)\n",
    "plt.xticks(ind+width,col_name[10:],fontsize=15)\n",
    "plt.legend((bar1, bar2, bar3 , bar4, bar5, bar6, bar7, bar8), ('MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_2.jpg' ,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "N = 10\n",
    "ind = np.arange(N)\n",
    "width = 0.1\n",
    "\n",
    "xvals = importance_1[:10]\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'r')\n",
    "  \n",
    "yvals = importance_2[:10]\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='g')\n",
    "\n",
    "zvals = importance_3[:10]\n",
    "bar3 = plt.bar(ind+width*2, zvals, width, color = 'b')\n",
    "\n",
    "z1vals = importance_4[:10]\n",
    "bar4 = plt.bar(ind+width*3, z1vals, width, color = 'c')\n",
    "\n",
    "z2vals = importance_5[:10]\n",
    "bar5 = plt.bar(ind+width*4, z2vals, width, color = 'm')\n",
    "\n",
    "z3vals = importance_6[:10]\n",
    "bar6 = plt.bar(ind+width*5, z3vals, width, color = 'y')\n",
    "\n",
    "z4vals = importance_7[:10]\n",
    "bar7 = plt.bar(ind+width*6, z4vals, width, color = 'lime')\n",
    "\n",
    "z5vals = importance_8[:10]\n",
    "bar8 = plt.bar(ind+width*7, z5vals, width, color = 'violet')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(23, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=15)\n",
    "plt.title(\"Textual Feature Importance for Negativity Detection Algorithms(for 2382 tweets with underampling)\",fontsize=15)\n",
    "plt.xticks(ind+width,col_name[:10],fontsize=15)\n",
    "plt.legend((bar1, bar2, bar3 , bar4, bar5, bar6, bar7, bar8), ('MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_2.jpg' ,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7e3a5",
   "metadata": {},
   "source": [
    "## Overfitting Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3eed2cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_f1_test_0 = [0.69, 0.67, 0.77, 0.63, 0.56, 0.65, 0.58, 0.70, 0.76]\n",
    "list_f1_train_0 = [0.90, 1, 1, 0.64, 0.87, 0.65, 0.61, 0.77, 0.89]\n",
    "list_f1_test_1 = [0.53, 0.49, 0.52, 0.51, 0.50, 0.52 ,0.51, 0.50, 0.63]\n",
    "list_f1_train_1 = [0.91, 1, 1, 0.74, 0.89, 0.74, 0.74, 0.79, 0.89]\n",
    "N = 9\n",
    "ind = np.arange(N)\n",
    "width = 0.3\n",
    "\n",
    "xvals = list_f1_test_0\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'lightcoral')\n",
    "  \n",
    "yvals = list_f1_train_0\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='brown')\n",
    "\n",
    "fig.set_size_inches(20, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=12)\n",
    "plt.title(\"F1-Score for Class 0 in Negativity Detection(for 2382 tweets)\",fontsize=12)\n",
    "plt.xticks(ind+width,['MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN' , 'P-BERT'], fontsize=10)\n",
    "plt.legend((bar1, bar2), ('test' , 'train'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_overfit.jpg' ,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55944d4",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e593c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_excel(r'../Files/5100_new.xlsx')\n",
    "data['full_text'] = data1['full_text']\n",
    "data['rate'] = data1['neg']\n",
    "\n",
    "data['rate'] = data['rate'].apply(lambda r: r if r < 2 else None)\n",
    "\n",
    "data = data.dropna(subset=['rate'])\n",
    "data = data.dropna(subset=['full_text'])\n",
    "data = data.drop_duplicates(subset=['full_text'], keep='first')\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print('data information')\n",
    "print(data.info(), '\\n')\n",
    "\n",
    "# print missing values information\n",
    "print('missing values stats')\n",
    "print(data.isnull().sum(), '\\n')\n",
    "\n",
    "# print some missing values\n",
    "print('some missing values')\n",
    "print(data[data['rate'].isnull()].iloc[:5], '\\n')\n",
    "\n",
    "data['comment_len_by_words'] = data['full_text'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "\n",
    "min_max_len = data[\"comment_len_by_words\"].min(), data[\"comment_len_by_words\"].max()\n",
    "print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')\n",
    "\n",
    "minlim, maxlim = 0, 47\n",
    "\n",
    "data['comment_len_by_words'] = data['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\n",
    "data = data.dropna(subset=['comment_len_by_words'])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "unique_rates = list(sorted(data['rate'].unique()))\n",
    "\n",
    "def rate_to_label(rate, threshold=0.0):\n",
    "    if rate <= threshold:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "data['label'] = data['rate'].apply(lambda t: rate_to_label(t, 0.0))\n",
    "labels = list(sorted(data['label'].unique()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea330db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "  \n",
    "    text = text.strip()\n",
    "    \n",
    "    # regular cleaning\n",
    "    text = clean(text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lower=True,\n",
    "        no_line_breaks=True,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "    )\n",
    "\n",
    "    # cleaning htmls\n",
    "    text = cleanhtml(text)\n",
    "    \n",
    "    # normalizing\n",
    "    normalizer = hazm.Normalizer()\n",
    "    text = normalizer.normalize(text)\n",
    "    \n",
    "    # removing wierd patterns\n",
    "    wierd_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u2069\"\n",
    "        u\"\\u2066\"\n",
    "        # u\"\\u200c\"\n",
    "        u\"\\u2068\"\n",
    "        u\"\\u2067\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = wierd_pattern.sub(r'', text)\n",
    "    \n",
    "    # removing extra spaces, hashtags\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning comments\n",
    "data['cleaned_comment'] = data['full_text'].apply(cleaning)\n",
    "\n",
    "#calculate the length of comments based on their words\n",
    "data['cleaned_comment_len_by_words'] = data['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "\n",
    "#remove comments with the length of fewer than three words\n",
    "data['cleaned_comment_len_by_words'] = data['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\n",
    "data = data.dropna(subset=['cleaned_comment_len_by_words'])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "data = data[['full_text', 'label']]\n",
    "data.columns = ['comment', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c60652",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_id'] = data['label'].apply(lambda t: 1-labels.index(t))\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=0, stratify=data['label'])\n",
    "train, valid = train_test_split(train, test_size=0.2, random_state=0, stratify=train['label'])\n",
    "negative_data = train[train['label'] == 'negative']\n",
    "positive_data = train[train['label'] == 'positive']\n",
    "\n",
    "cutting_point = min(len(negative_data), len(positive_data))\n",
    "\n",
    "if cutting_point <= len(negative_data):\n",
    "    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "if cutting_point <= len(positive_data):\n",
    "    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "if len(positive_data) > len(negative_data):\n",
    "    negative_data = negative_data.sample(n=len(positive_data), replace=True).reset_index(drop=True)\n",
    "\n",
    "if len(positive_data) < len(negative_data):\n",
    "    positive_data = positive_data.sample(n=len(negative_data), replace=True).reset_index(drop=True)'''\n",
    "train = pd.concat([negative_data, positive_data])\n",
    "#new_data = new_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "x_train, y_train = train['comment'].values.tolist(), train['label_id'].values.tolist()\n",
    "x_valid, y_valid = valid['comment'].values.tolist(), valid['label_id'].values.tolist()\n",
    "x_test, y_test = test['comment'].values.tolist(), test['label_id'].values.tolist()\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import TFBertModel, TFBertForSequenceClassification\n",
    "from transformers import glue_convert_examples_to_features\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general config\n",
    "MAX_LEN = 79\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "\n",
    "EPOCHS = 4\n",
    "EEVERY_EPOCH = 500\n",
    "LEARNING_RATE = 2e-5\n",
    "CLIP = 0.0\n",
    "dropout_rate = 0.4\n",
    "\n",
    "MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'\n",
    "#MODEL_NAME_OR_PATH = 'distilbert-base-uncased'\n",
    "#MODEL_NAME_OR_PATH = 'bert-base-multilingual-cased'\n",
    "#MODEL_NAME_OR_PATH = 'distilbert-base-multilingual-cased'\n",
    "OUTPUT_PATH = r'./pytorch_model.bin'\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ccd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: 1-i for i, label in enumerate(labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "config = BertConfig.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH, **{\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label,\n",
    "    })\n",
    "config.hidden_dropout_prob = 0.5\n",
    "config.attention_probs_dropout_prob = 0.5\n",
    "print(config.to_json_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27cd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\" A single example for simple sequence classification. \"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\" Constructs a InputExample. \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def make_examples(tokenizer, x, y=None, maxlen=128, output_mode=\"classification\", is_tf_dataset=True):\n",
    "    examples = []\n",
    "    y = y if isinstance(y, list) or isinstance(y, np.ndarray) else [None] * len(x)\n",
    "\n",
    "    for i, (_x, _y) in tqdm(enumerate(zip(x, y)), position=0, total=len(x)):\n",
    "        guid = \"%s\" % i\n",
    "        label = int(_y)\n",
    "        \n",
    "        if isinstance(_x, str):\n",
    "            text_a = _x\n",
    "            text_b = None\n",
    "        else:\n",
    "            assert len(_x) == 2\n",
    "            text_a = _x[0]\n",
    "            text_b = _x[1]\n",
    "        \n",
    "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    \n",
    "    features = glue_convert_examples_to_features(\n",
    "        examples, \n",
    "        tokenizer, \n",
    "        maxlen, \n",
    "        output_mode=output_mode, \n",
    "        label_list=list(np.unique(y)))\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_token_type_ids = []\n",
    "    all_labels = []\n",
    "\n",
    "    for f in tqdm(features, position=0, total=len(examples)):\n",
    "        if is_tf_dataset:\n",
    "            all_input_ids.append(tf.constant(f.input_ids))\n",
    "            all_attention_masks.append(tf.constant(f.attention_mask))\n",
    "            all_token_type_ids.append(tf.constant(f.token_type_ids))\n",
    "            all_labels.append(tf.constant(f.label))\n",
    "        else:\n",
    "            all_input_ids.append(f.input_ids)\n",
    "            all_attention_masks.append(f.attention_mask)\n",
    "            all_token_type_ids.append(f.token_type_ids)\n",
    "            all_labels.append(f.label)\n",
    "\n",
    "    if is_tf_dataset:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "            'input_ids': all_input_ids,\n",
    "            'attention_mask': all_attention_masks,\n",
    "            'token_type_ids': all_token_type_ids\n",
    "        }, all_labels))\n",
    "\n",
    "        return dataset, features\n",
    "    \n",
    "    xdata = [np.array(all_input_ids), np.array(all_attention_masks), np.array(all_token_type_ids)]\n",
    "    ydata = all_labels\n",
    "\n",
    "    return [xdata, ydata], features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_base, train_examples = make_examples(tokenizer, x_train, y_train, maxlen=256)\n",
    "valid_dataset_base, valid_examples = make_examples(tokenizer, x_valid, y_valid, maxlen=256)\n",
    "\n",
    "test_dataset_base, test_examples = make_examples(tokenizer, x_test, y_test, maxlen=256)\n",
    "[xtest, ytest], test_examples = make_examples(tokenizer, x_test, y_test, maxlen=256, is_tf_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset(dataset, batch_size):\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(dataset, batch_size):\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_training_dataset(train_dataset_base, TRAIN_BATCH_SIZE)\n",
    "valid_dataset = get_training_dataset(valid_dataset_base, VALID_BATCH_SIZE)\n",
    "\n",
    "train_steps = len(train_examples) // TRAIN_BATCH_SIZE\n",
    "valid_steps = len(valid_examples) // VALID_BATCH_SIZE\n",
    "\n",
    "train_steps, valid_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "def build_model(model_name, config, learning_rate=3e-5):\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(MODEL_NAME_OR_PATH, config, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1)\n",
    "\n",
    "final_accuracy = r.history['val_accuracy']\n",
    "print('FINAL ACCURACY MEAN: ', np.mean(final_accuracy))\n",
    "\n",
    "model.save_pretrained(os.path.dirname(OUTPUT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d326397",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = model.evaluate(test_dataset_base.batch(TEST_BATCH_SIZE))\n",
    "print()\n",
    "print(f'Evaluation: {ev}')\n",
    "print()\n",
    "\n",
    "predictions = model.predict(xtest)\n",
    "ypred = predictions[0].argmax(axis=-1).tolist()\n",
    "\n",
    "print()\n",
    "print(classification_report(ytest, ypred, target_names=labels))\n",
    "print()\n",
    "\n",
    "print(f'F1: {f1_score(ytest, ypred, average=\"weighted\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
