{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83a56ec",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io, json\n",
    "import csv\n",
    "import requests\n",
    "import stanza\n",
    "import emojis\n",
    "import collections\n",
    "import math\n",
    "import twitter\n",
    "import fnmatch\n",
    "import pickle\n",
    "import urllib\n",
    "import re, string, ast, emoji, json, urlexpander, sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files  \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.corpus import words\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy.stats import kurtosis, skew, entropy\n",
    "from textstat.textstat import textstat\n",
    "from textblob import TextBlob\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from persiantools.jdatetime import JalaliDate\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import hazm\n",
    "from hazm import Normalizer, Lemmatizer, WordTokenizer\n",
    "from cleantext import clean\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import imblearn\n",
    "from numpy import mean\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from googletrans import Translator\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "#fasttext.util.download_model('fa', if_exists='ignore')\n",
    "#!pip install googletrans==4.0.0-rc1\n",
    "ft = fasttext.load_model('cc.fa.300.bin')\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b07b2",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3c114c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>neg</th>\n",
       "      <th>pol</th>\n",
       "      <th>char</th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MohajerMr غذای چرب ! سالم محسوب نمیشه</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Oct 11 15:24:43 +0000 2021</td>\n",
       "      <td>[11, 38]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>نمونه اش همین وزیر پیشنهادی آموزش و پرورش که د...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wed Nov 17 09:20:50 +0000 2021</td>\n",
       "      <td>[0, 90]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [{'dis...</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'contributors': None, 'coordinates': None, 'c...</td>\n",
       "      <td>1.460879e+18</td>\n",
       "      <td>1.460879e+18</td>\n",
       "      <td>{'display': 'twitter.com/mah_sadeghi/st…', 'ex...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mah_sadeghi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>رئیس سازمان بازرسی کل کشور با بررسی دقیق وظایف...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thu Nov 11 13:56:42 +0000 2021</td>\n",
       "      <td>[0, 183]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>1660</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ejei_com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اقدام ایثارگرانه و قهرمانانه #علی_لندی نوجوان ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Sep 25 18:05:50 +0000 2021</td>\n",
       "      <td>[0, 253]</td>\n",
       "      <td>{'hashtags': [{'indices': [29, 38], 'text': 'ع...</td>\n",
       "      <td>1149</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GhazizadehSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>تشکر از بازیکنان تیم ملی #فوتبال که با #پیروزی...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thu Oct 07 18:49:04 +0000 2021</td>\n",
       "      <td>[0, 199]</td>\n",
       "      <td>{'hashtags': [{'indices': [25, 32], 'text': 'ف...</td>\n",
       "      <td>374</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'media': [{'display_url': 'pic.twitter.com/8w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hosseini_social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>@azarijahromi فرهاد مجیدی با بی اخلاقی این کل‌...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri May 14 20:24:25 +0000 2021</td>\n",
       "      <td>[14, 97]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>268</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>@zoheirmousavi جای لگد، بازی میکردن الان وضعشو...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri May 14 18:00:56 +0000 2021</td>\n",
       "      <td>[15, 56]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>@fahimehtabaee1 @YeganehKhodami سلام واکسن کوب...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri May 14 12:58:24 +0000 2021</td>\n",
       "      <td>[32, 130]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>@YeganehKhodami سلام، من منکر اینکه ممکن است ت...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri May 14 12:56:36 +0000 2021</td>\n",
       "      <td>[16, 170]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>@tahamohammadi_ @drjahanpur سلام، ما کاره‌ای ن...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri May 14 09:55:52 +0000 2021</td>\n",
       "      <td>[28, 179]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'contributors_enabled': False, 'created_at': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vahabzadeh_ali</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5100 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              full_text  neg  pol  char   \n",
       "0                @MohajerMr غذای چرب ! سالم محسوب نمیشه    0    0     0  \\\n",
       "1     نمونه اش همین وزیر پیشنهادی آموزش و پرورش که د...    1    0     1   \n",
       "2     رئیس سازمان بازرسی کل کشور با بررسی دقیق وظایف...    0    0     0   \n",
       "3     اقدام ایثارگرانه و قهرمانانه #علی_لندی نوجوان ...    0    0     0   \n",
       "4     تشکر از بازیکنان تیم ملی #فوتبال که با #پیروزی...    0    0     0   \n",
       "...                                                 ...  ...  ...   ...   \n",
       "5095  @azarijahromi فرهاد مجیدی با بی اخلاقی این کل‌...    1    0     1   \n",
       "5096  @zoheirmousavi جای لگد، بازی میکردن الان وضعشو...    1    1     0   \n",
       "5097  @fahimehtabaee1 @YeganehKhodami سلام واکسن کوب...    0    0     0   \n",
       "5098  @YeganehKhodami سلام، من منکر اینکه ممکن است ت...    0    0     0   \n",
       "5099  @tahamohammadi_ @drjahanpur سلام، ما کاره‌ای ن...    1    0     1   \n",
       "\n",
       "      contributors  coordinates                      created_at   \n",
       "0              NaN          NaN  Mon Oct 11 15:24:43 +0000 2021  \\\n",
       "1              NaN          NaN  Wed Nov 17 09:20:50 +0000 2021   \n",
       "2              NaN          NaN  Thu Nov 11 13:56:42 +0000 2021   \n",
       "3              NaN          NaN  Sat Sep 25 18:05:50 +0000 2021   \n",
       "4              NaN          NaN  Thu Oct 07 18:49:04 +0000 2021   \n",
       "...            ...          ...                             ...   \n",
       "5095           NaN          NaN  Fri May 14 20:24:25 +0000 2021   \n",
       "5096           NaN          NaN  Fri May 14 18:00:56 +0000 2021   \n",
       "5097           NaN          NaN  Fri May 14 12:58:24 +0000 2021   \n",
       "5098           NaN          NaN  Fri May 14 12:56:36 +0000 2021   \n",
       "5099           NaN          NaN  Fri May 14 09:55:52 +0000 2021   \n",
       "\n",
       "     display_text_range                                           entities   \n",
       "0              [11, 38]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...  \\\n",
       "1               [0, 90]  {'hashtags': [], 'symbols': [], 'urls': [{'dis...   \n",
       "2              [0, 183]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "3              [0, 253]  {'hashtags': [{'indices': [29, 38], 'text': 'ع...   \n",
       "4              [0, 199]  {'hashtags': [{'indices': [25, 32], 'text': 'ف...   \n",
       "...                 ...                                                ...   \n",
       "5095           [14, 97]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "5096           [15, 56]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "5097          [32, 130]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "5098          [16, 170]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "5099          [28, 179]  {'hashtags': [], 'symbols': [], 'urls': [], 'u...   \n",
       "\n",
       "      favorite_count  ...  truncated   \n",
       "0                  1  ...      False  \\\n",
       "1                 82  ...      False   \n",
       "2               1660  ...      False   \n",
       "3               1149  ...      False   \n",
       "4                374  ...      False   \n",
       "...              ...  ...        ...   \n",
       "5095             268  ...      False   \n",
       "5096               6  ...      False   \n",
       "5097               2  ...      False   \n",
       "5098               9  ...      False   \n",
       "5099               3  ...      False   \n",
       "\n",
       "                                                   user  possibly_sensitive   \n",
       "0     {'contributors_enabled': False, 'created_at': ...                 NaN  \\\n",
       "1     {'contributors_enabled': False, 'created_at': ...                 0.0   \n",
       "2     {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "3     {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "4     {'contributors_enabled': False, 'created_at': ...                 0.0   \n",
       "...                                                 ...                 ...   \n",
       "5095  {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "5096  {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "5097  {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "5098  {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "5099  {'contributors_enabled': False, 'created_at': ...                 NaN   \n",
       "\n",
       "                                          quoted_status quoted_status_id   \n",
       "0                                                   NaN              NaN  \\\n",
       "1     {'contributors': None, 'coordinates': None, 'c...     1.460879e+18   \n",
       "2                                                   NaN              NaN   \n",
       "3                                                   NaN              NaN   \n",
       "4                                                   NaN              NaN   \n",
       "...                                                 ...              ...   \n",
       "5095                                                NaN              NaN   \n",
       "5096                                                NaN              NaN   \n",
       "5097                                                NaN              NaN   \n",
       "5098                                                NaN              NaN   \n",
       "5099                                                NaN              NaN   \n",
       "\n",
       "      quoted_status_id_str                            quoted_status_permalink   \n",
       "0                      NaN                                                NaN  \\\n",
       "1             1.460879e+18  {'display': 'twitter.com/mah_sadeghi/st…', 'ex...   \n",
       "2                      NaN                                                NaN   \n",
       "3                      NaN                                                NaN   \n",
       "4                      NaN                                                NaN   \n",
       "...                    ...                                                ...   \n",
       "5095                   NaN                                                NaN   \n",
       "5096                   NaN                                                NaN   \n",
       "5097                   NaN                                                NaN   \n",
       "5098                   NaN                                                NaN   \n",
       "5099                   NaN                                                NaN   \n",
       "\n",
       "                                      extended_entities  retweeted_status   \n",
       "0                                                   NaN               NaN  \\\n",
       "1                                                   NaN               NaN   \n",
       "2                                                   NaN               NaN   \n",
       "3                                                   NaN               NaN   \n",
       "4     {'media': [{'display_url': 'pic.twitter.com/8w...               NaN   \n",
       "...                                                 ...               ...   \n",
       "5095                                                NaN               NaN   \n",
       "5096                                                NaN               NaN   \n",
       "5097                                                NaN               NaN   \n",
       "5098                                                NaN               NaN   \n",
       "5099                                                NaN               NaN   \n",
       "\n",
       "             username  \n",
       "0      vahabzadeh_ali  \n",
       "1         mah_sadeghi  \n",
       "2            Ejei_com  \n",
       "3        GhazizadehSA  \n",
       "4     hosseini_social  \n",
       "...               ...  \n",
       "5095   vahabzadeh_ali  \n",
       "5096   vahabzadeh_ali  \n",
       "5097   vahabzadeh_ali  \n",
       "5098   vahabzadeh_ali  \n",
       "5099   vahabzadeh_ali  \n",
       "\n",
       "[5100 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(r'../Files/5100_new.xlsx')\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac0739",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "organizations = pd.read_excel(r'../Files/organizations.xlsx')\n",
    "organizations['name2'] = organizations['name2'].astype(str)\n",
    "name_of_organizations = organizations['name2'].iloc[:130].to_list()\n",
    "organization = []\n",
    "for n in name_of_organizations:\n",
    "    organization.append(n.rstrip())\n",
    "organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c0e81",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swears = pd.read_json(r'../Files/data_swear.json', encoding='utf-8')\n",
    "swear = swears['word'].to_list()\n",
    "swear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46830b73",
   "metadata": {},
   "source": [
    "# Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_like'] = [like for like in data['favorite_count']]\n",
    "\n",
    "data['tweet_retweet_count'] = [retweet for retweet in data['retweet_count']]\n",
    "\n",
    "data['tweet_length_word'] = [len(word_tokenize(t)) for t in data['full_text']]\n",
    "\n",
    "data['tweet_length_characters'] = [len(t) for t in data['full_text']]\n",
    "\n",
    "data['hashtags'] = data['full_text'].str.findall(\"(#[^#\\s]+)\")\n",
    "\n",
    "pattern = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)'\n",
    "data['urls']= data['full_text'].str.findall(pattern)\n",
    "\n",
    "data['tweet_num_hashtags'] = [len(h) for h in data['hashtags']]\n",
    "\n",
    "def extract_mention_set(text):\n",
    "    mention_list = re.findall(\"@([a-zA-Z0-9]{1,15})\", text)\n",
    "    return len(mention_list)\n",
    "\n",
    "data['tweet_num_mention'] = data['full_text'].apply(extract_mention_set)\n",
    "\n",
    "data['tweet_num_urls'] = [len(h) for h in data['urls']]\n",
    "\n",
    "def extract_emojis(s):\n",
    "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "data['tweet_num_emoji'] = [len(extract_emojis(t)) for t in data['full_text']]\n",
    "\n",
    "data['tweet_emoji'] = [extract_emojis(t) for t in data['full_text']]\n",
    "\n",
    "persian_punctuation = '.،؛:()«»؟![]-/'\n",
    "def extract_punctuation(s):\n",
    "  return ''.join(c for c in s if c in list(persian_punctuation))\n",
    "\n",
    "data['tweet_num_punctuation'] = [len(extract_punctuation(t)) for t in data['full_text']]\n",
    "\n",
    "list_o = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_o.append(0)\n",
    "    for name in organization:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_o[i] += 1\n",
    "data['organize_names'] = list_o\n",
    "\n",
    "list_sw = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_sw.append(0)\n",
    "    for name in swear:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_sw[i] += 1\n",
    "data['swear_words'] = list_sw\n",
    "\n",
    "cn = ['رئیسی' , 'جلیلی' , 'رییسی' , 'همتی' , 'مهرعلیزاده' , 'مهر علیزاده' , 'محسن رضایی' , 'قاضی زاده' , 'قاضی‌زاده',\n",
    "      'زاکانی' , 'وهاب‌زاده' , 'وهاب زاده' , 'بطحایی' , 'حناچی' , 'مولاوردی' , 'تاج زاده' , 'تاج‌زاده' ,\n",
    "      'تاجزاده' , 'قالیباف' , 'قالی‌باف' , 'قالی باف' , 'محمود صادقی' , 'مهدی اسماعیلی' ,\n",
    "      'محمد اسماعیلی' , 'کواکبیان' , 'جواد اوجی' , 'خاتمی' , 'خامنه‌ای' , 'خامنه ای' , 'محمد حسینی', \n",
    "      'حمید سجادی' , 'معصومه ابتکار' , 'ساداتی نژاد' , 'ساداتی‌نژاد' , 'محمد دهقان' , 'جهرمی' , 'عبدالملکی' , 'لاریجانی',\n",
    "      'عراقچی' , 'ضرغامی' , 'روحانی' , 'رستم قاسمی' , 'مرتضوی' , 'نوبخت' , 'میرکاظمی' , 'مخبر' , 'ظریف' , \n",
    "      'عراق‌چی' , 'عراق چی' , 'زارع پور' , 'زارع‌پور' , 'عین اللهی' , 'عین‌اللهی' , 'جهانگیری' , 'جهان‌گیری' ,\n",
    "      'جهان گیری' , 'خزعلی' , 'اژه‌ای' , 'واعظی' , 'باقری کنی' , 'عبداللهیان' , 'احمدی‌نژاد' , 'احمدی نژاد' , 'آشتیانی' ,\n",
    "     'آقا' , 'آقای' , 'خانم' , 'مسئول' , 'وزیر' , 'آخوند' , 'رهبر' , 'رهبری' , 'آخوند' , 'استاندار' , 'شهردار']\n",
    "list_cn = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_cn.append(0)\n",
    "    for name in cn:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_cn[i] += 1\n",
    "data['person_names'] = list_cn\n",
    "\n",
    "'''def get_domain_from_url(url):\n",
    "  return urllib.parse.urlparse(url).netloc\n",
    "\n",
    "data['urls_dom'] = data['urls'].apply(lambda X: [get_domain_from_url(x) for x in X])\n",
    "url_list= []\n",
    "for i in range(len(data['urls_dom'])):\n",
    "    for url in data['urls_dom'].iloc[i]:\n",
    "        url_list.append(url)\n",
    "url_set = set(url_list)\n",
    "urls = []\n",
    "for url in url_set:\n",
    "    urls.append(url)\n",
    "if(len(urls) >= 10):\n",
    "    U = urls[:10]\n",
    "else:\n",
    "    U = urls\n",
    "for j in range(len(U)):\n",
    "    list_u = [0 for i in range(len(data))]\n",
    "    for i in range(len(data)):\n",
    "        if data['tweet_num_urls'].iloc[i] > 0:\n",
    "            if U[j] in data['full_text'].iloc[i]:\n",
    "                list_u[i] = 1\n",
    "    data[U[j]] = list_u.copy()'''\n",
    "\n",
    "data['tweet_emoji'] = [extract_emojis(t) for t in data['full_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2e5fb",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji_to_text(text, delimiters=(' ', ' ')):\n",
    "    \"\"\" Convert emojis to something readable by the vocab and model \"\"\"\n",
    "    text = emoji.demojize(text, delimiters=delimiters)\n",
    "    return text\n",
    "emo_list = []\n",
    "emoji_set = set(data[data['tweet_num_emoji']>0]['tweet_emoji'])\n",
    "for e in emoji_set:\n",
    "    for ee in e.strip():\n",
    "        emo_list.append(ee)\n",
    "emo_set = set(emo_list)\n",
    "emo_pic = []\n",
    "for e in emo_set:\n",
    "    emo_pic.append(e)\n",
    "emo = [convert_emoji_to_text(e) for e in emo_set]\n",
    "emo = [e.split('_') for e in emo]\n",
    "emo = [' '.join(e) for e in emo]\n",
    "emo_fa = []\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "for i in range(len(emo)):\n",
    "    trans = translator.translate(emo[i], dest='fa')\n",
    "    emo_fa.append(trans)\n",
    "emo_farsi = []\n",
    "for e in emo_fa:\n",
    "    emo_farsi.append(e.text)\n",
    "emo_farsi = [' ' + e + ' ' for e in emo_farsi]\n",
    "Emojis = dict(zip(emo_pic, emo_farsi))\n",
    "Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe3565e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if data['tweet_num_emoji'].iloc[i] > 0:\n",
    "        for e in emo_pic:\n",
    "            if e in data['full_text'].iloc[i]:\n",
    "                data['full_text'].iloc[i] = data['full_text'].iloc[i].replace(e, Emojis[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5132ab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert multihahstags to words\n",
    "hashtag = []\n",
    "for ht_list in data['hashtags']:\n",
    "    for ht in ht_list:\n",
    "        hashtag.append(ht)\n",
    "hash_set = set(hashtag)\n",
    "ht_list = []\n",
    "for h in hash_set:\n",
    "    ht_list.append(h)\n",
    "hash_list = []\n",
    "for h in hash_set:\n",
    "    hash_list.append(' '+ ' '.join(h[1:].split('_')) + ' ')\n",
    "hash_list2 = []\n",
    "for h in hash_list:\n",
    "    hash_list2.append(' '.join(h.split('\\u200c')))\n",
    "hash_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d2a985a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if data['tweet_num_hashtags'].iloc[i] > 0:\n",
    "        for h in data['hashtags'].iloc[i]:\n",
    "            ind = ht_list.index(h)\n",
    "            data['full_text'].iloc[i] = data['full_text'].iloc[i].replace(h, hash_list2[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c6140a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    stop_words = list()\n",
    "    with open(r'C:/PRIVATE/Metodata/Metodata-Files/stopwords_new.txt',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stop_words.append(line.strip())\n",
    "    exclusions = [\"ir\",\"#ff\", \"ff\", \"rt\",\"RT\", \"FF\",\"\\u200c\",\"\\n\",\"'s\",\"n't\",\"'re\",\"'m\",'#','@','&','?','.','+','-','*','/','’','...','…','‘','“','”','–','؟','،','.','\"',';','!',':','%','.',',']\n",
    "    stop_words.extend(exclusions)\n",
    "\n",
    "    junk_chars_regex = r'[^a-zA-Z\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF \\u200c]'\n",
    "    url_regex = r\"\"\"((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b\"\"\"\n",
    "    RFC_5322_COMPLIANT_EMAIL_REGEX = r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\"\n",
    "    compile_patterns = lambda patterns: [(re.compile(pattern), repl) for pattern, repl in patterns]\n",
    "    \n",
    "    remove_url = (url_regex, ' ')\n",
    "    remove_email = (RFC_5322_COMPLIANT_EMAIL_REGEX, ' ')\n",
    "    remove_junk_characters = (junk_chars_regex, ' ')\n",
    "    compiled_patterns_before = compile_patterns([remove_url, remove_email])\n",
    "    compiled_patterns_after = compile_patterns([remove_junk_characters])\n",
    "    \n",
    "    text = text.lower()\n",
    "    for pattern, repl in compiled_patterns_before:\n",
    "        text = pattern.sub(repl, text)\n",
    "    text = re.sub(r'[\\u200c\\s]*\\s[\\s\\u200c]*', ' ', text)\n",
    "    text = re.sub(r'[\\u200c]+', '\\u200c', text)\n",
    "\n",
    "    for pattern, repl in compiled_patterns_after:\n",
    "        text = pattern.sub(repl, text)\n",
    "\n",
    "    tokenized_words = tokenizer.tokenize(text)\n",
    "    tokenized_words = [word for word in tokenized_words if word not in stop_words]\n",
    "\n",
    "    return tokenized_words\n",
    "\n",
    "def remove_emoji(text):\n",
    "    junk_chars_regex = r'[^a-zA-Z\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF \\u200c]'\n",
    "    compile_patterns = lambda patterns: [(re.compile(pattern), repl) for pattern, repl in patterns]\n",
    "    remove_junk_characters = (junk_chars_regex, ' ')\n",
    "    compiled_patterns_after = compile_patterns([remove_junk_characters])\n",
    "    for pattern, repl in compiled_patterns_after:\n",
    "        text = pattern.sub(repl, text)\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def make_farsi_text(x):\n",
    "    reshaped_text = arabic_reshaper.reshape(x)\n",
    "    farsi_text = get_display(reshaped_text)\n",
    "    return farsi_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "65516bd5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 50)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_5080/2797418508.py:6: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_5080/2797418508.py:7: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "data['full_text']=data['full_text'].fillna('')\n",
    "print(data.shape)\n",
    "data['full_text'] = [str(i).lower() for i in data['full_text']]\n",
    "print('1')\n",
    "data['full_text']=data['full_text'].str.replace('\\d+', '')\n",
    "data['full_text'] = data['full_text'].str.replace('@[\\w\\-]+','')\n",
    "print('2')\n",
    "data['full_text']=data['full_text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "print('3')\n",
    "data['full_text']=data['full_text'].apply(lambda x:remove_emoji(x) )\n",
    "print('4')\n",
    "data['full_text']=data['full_text'].apply(lambda x:preprocess(x))\n",
    "print('5')\n",
    "data['full_text']=data['full_text'].apply(lambda x:' '.join([word for word in x]))\n",
    "print('6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce9516",
   "metadata": {},
   "source": [
    "# Negativity/Character Attack/Political Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bd5192ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Character Attack and Political Attack run following comment\n",
    "\n",
    "data = data[data['neg'] == 1]\n",
    "\n",
    "#for Character Attack change data['neg'] to data['char'] in code\n",
    "#for Political Attack change data['neg'] to data['pol'] in code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1197da",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e55a2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, data['neg'], test_size=0.2, random_state=0, stratify=data['neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2a8ce",
   "metadata": {},
   "source": [
    "## Undersmapling - Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "4d626faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 50)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_data = X_train[X_train['neg'] == 1]\n",
    "positive_data = X_train[X_train['neg'] == 0]\n",
    "'''\n",
    "#Undersampling\n",
    "cutting_point = min(len(negative_data), len(positive_data))\n",
    "\n",
    "if cutting_point <= len(negative_data):\n",
    "    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "if cutting_point <= len(positive_data):\n",
    "    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)'''\n",
    "\n",
    "#Oversampling\n",
    "'''if len(positive_data) > len(negative_data):\n",
    "    negative_data = negative_data.sample(n=len(positive_data), replace=True).reset_index(drop=True)\n",
    "\n",
    "if len(positive_data) < len(negative_data):\n",
    "    positive_data = positive_data.sample(n=len(negative_data), replace=True).reset_index(drop=True)'''\n",
    "\n",
    "X_train = pd.concat([negative_data, positive_data])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccf2fa",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e6212911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 300)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_n_train = []\n",
    "for w in X_train['full_text']:\n",
    "    vecs_n_train.append(ft.get_word_vector(w))\n",
    "vecs_n_train = np.array(vecs_n_train)\n",
    "vecs_n_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "320a28af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 300)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_n_test = []\n",
    "for w in X_test['full_text'].to_list():\n",
    "    vecs_n_test.append(ft.get_word_vector(w))\n",
    "vecs_n_test = np.array(vecs_n_test)\n",
    "vecs_n_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "4fa0d51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831,)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n_train = X_train['neg'].to_list()\n",
    "y_n_train = np.array(y_n_train)\n",
    "y_n_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d547a40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208,)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n_test = y_test\n",
    "y_n_test = np.array(y_n_test)\n",
    "y_n_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "088b4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf0af4",
   "metadata": {},
   "source": [
    "## Textual Features for Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "7cd4c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pd.DataFrame({'full_text':X_train['full_text'], 'neg':X_train['neg']})\n",
    "df_s1 = S[S['neg'] == 1]\n",
    "df_s0 = S[S['neg'] == 0]\n",
    "def Merge(D1,D2):\n",
    "    py = D1 | D2\n",
    "    return py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "accfab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sort_Dic = {}\n",
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l1 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l2 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l3 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "616ba2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 850)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_Top = [x[0] for x in l1[:400]] + [x[0] for x in l2[:100]] + [x[0] for x in l3[:50]]\n",
    "L = X_train['full_text'].to_list()\n",
    "top_dic = {}\n",
    "for i in range(len(dic_Top)):\n",
    "    k = []\n",
    "    for j in range(len(L)):\n",
    "        if(dic_Top[i] in L[j]):\n",
    "            k.append(1)\n",
    "        else:\n",
    "            k.append(0)\n",
    "    top_dic[dic_Top[i]] = k\n",
    "df_top_s = pd.DataFrame(top_dic)\n",
    "vecs_n_train = pd.DataFrame(vecs_n_train)\n",
    "df_top_s = df_top_s.reset_index()\n",
    "vecs_n_train = vecs_n_train.reset_index()\n",
    "dic_all_train = pd.concat([vecs_n_train, df_top_s], axis=1)\n",
    "dic_all_train = dic_all_train.drop({'index'}, axis=1)\n",
    "dic_all_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "634faa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_1 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_2 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_3 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_1 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_2 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_3 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "sort_dic0_n = Merge(sort_dic0_1, sort_dic0_2)\n",
    "sort_dic0 = Merge(sort_dic0_n, sort_dic0_3)\n",
    "\n",
    "sort_dic1_n = Merge(sort_dic1_1, sort_dic1_2)\n",
    "sort_dic1 = Merge(sort_dic1_n, sort_dic1_3)\n",
    "\n",
    "sort_dic0 = dict(sorted(sort_dic0.items(), key=lambda item: item[1] , reverse=True))\n",
    "sort_dic1 = dict(sorted(sort_dic1.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "Top0 = [x for x in list(sort_dic0_1.keys())][:500] + [x for x in list(sort_dic0_2.keys())][:100] + [x for x in list(sort_dic0_3.keys())][:50]\n",
    "Top1 = [x for x in list(sort_dic1_1.keys())][:500] + [x for x in list(sort_dic1_2.keys())][:100] + [x for x in list(sort_dic1_3.keys())][:50]\n",
    "\n",
    "Top01 = [x for x in list(sort_dic0_1.keys()) if x not in sort_dic1_1.keys()][:500] + [x for x in list(sort_dic0_2.keys()) if x not in sort_dic1_2.keys()][:100] + [x for x in list(sort_dic0_3.keys()) if x not in sort_dic1_3.keys()][:100]\n",
    "Top10 = [x for x in list(sort_dic1_1.keys()) if x not in sort_dic0_1.keys()][:500] + [x for x in list(sort_dic1_2.keys()) if x not in sort_dic0_2.keys()][:100] + [x for x in list(sort_dic1_3.keys()) if x not in sort_dic0_3.keys()][:100]\n",
    "\n",
    "Top0_w = [x for x in list(sort_dic0_1.keys())][:500] + [x for x in list(sort_dic0_2.keys())][:100] + [x for x in list(sort_dic0_3.keys())][:50]\n",
    "Top1_w = [x for x in list(sort_dic1_1.keys())][:500] + [x for x in list(sort_dic1_2.keys())][:100] + [x for x in list(sort_dic1_3.keys())][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "98636e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 831, 831, 831, 831, 831)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_Top0 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0):\n",
    "                count += 1\n",
    "        feature_Top0.append(count)\n",
    "    else:\n",
    "        feature_Top0.append(0)\n",
    "        \n",
    "feature_Top1 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1):\n",
    "                count += 1\n",
    "        feature_Top1.append(count)\n",
    "    else:\n",
    "        feature_Top1.append(0)\n",
    "        \n",
    "feature_Top01 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top01):\n",
    "                count += 1\n",
    "        feature_Top01.append(count)\n",
    "    else:\n",
    "        feature_Top01.append(0)\n",
    "        \n",
    "feature_Top10 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top10):\n",
    "                count += 1\n",
    "        feature_Top10.append(count)\n",
    "    else:\n",
    "        feature_Top10.append(0)\n",
    "        \n",
    "feature_weight1 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1_w):\n",
    "                count += sort_dic1[tweet[i]]\n",
    "        feature_weight1.append(count)\n",
    "    else:\n",
    "        feature_weight1.append(0)\n",
    "\n",
    "feature_weight0 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0_w):\n",
    "                count += sort_dic0[tweet[i]]\n",
    "        feature_weight0.append(count)\n",
    "    else:\n",
    "        feature_weight0.append(0)\n",
    "\n",
    "len(feature_Top0), len(feature_Top1), len(feature_Top01), len(feature_Top10), len(feature_weight1), len(feature_weight0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f17353ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((831, 868), 831)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train = pd.DataFrame(\n",
    "    {'50Top_0': feature_Top0,\n",
    "     '50Top_1': feature_Top1,\n",
    "     'Top_0_1': feature_Top01,\n",
    "     'Top_1_0': feature_Top10,\n",
    "     'Weight_0': feature_weight0,\n",
    "     'Weight_1': feature_weight1})\n",
    "features_train['tweet_like'] = X_train['tweet_like']\n",
    "features_train['tweet_retweet_count'] = X_train['tweet_retweet_count']\n",
    "features_train['tweet_length_word'] = X_train['tweet_length_word']\n",
    "features_train['tweet_length_characters'] = X_train['tweet_length_characters']\n",
    "features_train['tweet_num_hashtags'] = X_train['tweet_num_hashtags']\n",
    "features_train['tweet_num_mention'] = X_train['tweet_num_mention']\n",
    "features_train['tweet_num_urls'] = X_train['tweet_num_urls']\n",
    "features_train['tweet_num_emoji'] = X_train['tweet_num_emoji']\n",
    "features_train['tweet_num_punctuation'] = X_train['tweet_num_punctuation']\n",
    "features_train['person_names'] = X_train['person_names']\n",
    "features_train['organize_names'] = X_train['organize_names']\n",
    "features_train['swear_words'] = X_train['swear_words']\n",
    "\n",
    "X_train_new = pd.concat([dic_all_train , features_train], axis=1)\n",
    "X_train_new = X_train_new.fillna(0)\n",
    "y_train_new = X_train['neg'].to_list()\n",
    "scaler_train = Normalizer()\n",
    "scaler_train.fit(X_train_new)\n",
    "X_train_new = scaler_train.transform(X_train_new)\n",
    "X_train_new.shape, len(y_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91db43",
   "metadata": {},
   "source": [
    "## Textual Features for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "980a5857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 850)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_Top = [x[0] for x in l1[:400]] + [x[0] for x in l2[:100]] + [x[0] for x in l3[:50]]\n",
    "L = X_test['full_text'].to_list()\n",
    "top_dic = {}\n",
    "for i in range(len(dic_Top)):\n",
    "    k = []\n",
    "    for j in range(len(L)):\n",
    "        if(dic_Top[i] in L[j]):\n",
    "            k.append(1)\n",
    "        else:\n",
    "            k.append(0)\n",
    "    top_dic[dic_Top[i]] = k\n",
    "df_top_s = pd.DataFrame(top_dic)\n",
    "vecs_n_test = pd.DataFrame(vecs_n_test)\n",
    "df_top_s = df_top_s.reset_index()\n",
    "vecs_n_test = vecs_n_test.reset_index()\n",
    "dic_all_test = pd.concat([vecs_n_test, df_top_s], axis=1)\n",
    "dic_all_test = dic_all_test.drop({'index'}, axis=1)\n",
    "dic_all_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9b90281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(208, 208, 208, 208, 208, 208)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_Top0_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0):\n",
    "                count += 1\n",
    "        feature_Top0_t.append(count)\n",
    "    else:\n",
    "        feature_Top0_t.append(0)\n",
    "\n",
    "feature_Top1_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1):\n",
    "                count += 1\n",
    "        feature_Top1_t.append(count)\n",
    "    else:\n",
    "        feature_Top1_t.append(0)\n",
    "\n",
    "feature_Top01_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top01):\n",
    "                count += 1\n",
    "        feature_Top01_t.append(count)\n",
    "    else:\n",
    "        feature_Top01_t.append(0)\n",
    "        \n",
    "feature_Top10_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top10):\n",
    "                count += 1\n",
    "        feature_Top10_t.append(count)\n",
    "    else:\n",
    "        feature_Top10_t.append(0)\n",
    "        \n",
    "feature_weight1_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1_w):\n",
    "                count += sort_dic1[tweet[i]]\n",
    "        feature_weight1_t.append(count)\n",
    "    else:\n",
    "        feature_weight1_t.append(0)\n",
    "        \n",
    "feature_weight0_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0_w):\n",
    "                count += sort_dic0[tweet[i]]\n",
    "        feature_weight0_t.append(count)\n",
    "    else:\n",
    "        feature_weight0_t.append(0)\n",
    "        \n",
    "len(feature_Top0_t), len(feature_Top1_t), len(feature_Top01_t), len(feature_Top10_t), len(feature_weight1_t), len(feature_weight0_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "54d08b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(208, 868)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test = pd.DataFrame(\n",
    "    {'50Top_0': feature_Top0_t,\n",
    "     '50Top_1': feature_Top1_t,\n",
    "     'Top_0_1': feature_Top01_t,\n",
    "     'Top_1_0': feature_Top10_t,\n",
    "     'Weight_0': feature_weight0_t,\n",
    "     'Weight_1': feature_weight1_t\n",
    "      })\n",
    "features_test['tweet_like'] = X_test['tweet_like']\n",
    "features_test['tweet_retweet_count'] = X_test['tweet_retweet_count']\n",
    "features_test['tweet_length_word'] = X_test['tweet_length_word']\n",
    "features_test['tweet_length_characters'] = X_test['tweet_length_characters']\n",
    "features_test['tweet_num_hashtags'] = X_test['tweet_num_hashtags']\n",
    "features_test['tweet_num_mention'] = X_test['tweet_num_mention']\n",
    "features_test['tweet_num_urls'] = X_test['tweet_num_urls']\n",
    "features_test['tweet_num_emoji'] = X_test['tweet_num_emoji']\n",
    "features_test['tweet_num_punctuation'] = X_test['tweet_num_punctuation']\n",
    "features_test['person_names'] = X_test['person_names']\n",
    "features_test['organize_names'] = X_test['organize_names']\n",
    "features_test['swear_words'] = X_test['swear_words']\n",
    "\n",
    "X_test_new = pd.concat([dic_all_test , features_test], axis=1)\n",
    "X_test_new = X_test_new.fillna(0)\n",
    "scaler_test = Normalizer()\n",
    "scaler_test.fit(X_test_new)\n",
    "X_test_new = scaler_test.transform(X_test_new)\n",
    "X_test_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45f8bf",
   "metadata": {},
   "source": [
    "## Classical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ad067",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLP\n",
    "'''parameter_space = {\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive']}'''\n",
    "\n",
    "clf1 = MLPClassifier(random_state=0, max_iter=400)\n",
    "#clf1 = GridSearchCV(mlp, parameter_space, cv=5, n_jobs=1)\n",
    "clf1.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('****MLP classificaion report for test set****')\n",
    "predictions1 = clf1.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions1))\n",
    "print(metrics.roc_auc_score(y_test,predictions1))\n",
    "\n",
    "print('\\n****MLP classificaion report for train set****')\n",
    "predictions1_1 = clf1.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_1))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_1))\n",
    "\n",
    "#XGB\n",
    "'''parameter_space={'max_depth': range(3, 18),\n",
    "                    'gamma': range(1,9)}'''\n",
    "\n",
    "eval_set = [(X_test_new, y_test)]\n",
    "clf2 = XGBClassifier(random_state=0, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)\n",
    "#clf2 = GridSearchCV(xgb, parameter_space, cv=3)\n",
    "clf2.fit(X_train_new, y_train_new, verbose=True)\n",
    "\n",
    "print('\\n\\n\\n\\n****XGB classificaion report for test set****')\n",
    "predictions2 = clf2.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions2))\n",
    "print(metrics.roc_auc_score(y_test,predictions2))\n",
    "\n",
    "print('\\n****XGB classificaion report for train set****')\n",
    "predictions1_2 = clf2.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_2))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_2))\n",
    "\n",
    "#RF\n",
    "'''parameter_space={'max_depth' : list(range(1, 50))}'''\n",
    "\n",
    "clf3 = RandomForestClassifier(random_state=0)\n",
    "#clf3 = GridSearchCV(rf, parameter_space, cv=3, scoring=p_score)\n",
    "clf3.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****RF classificaion report for test set****')\n",
    "predictions3 = clf3.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions3))\n",
    "print(metrics.roc_auc_score(y_test,predictions3))\n",
    "\n",
    "print('\\n****RF classificaion report for train set****')\n",
    "predictions1_3 = clf3.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_3))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_3))\n",
    "\n",
    "#LR\n",
    "'''parameter_space={'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'penalty' : ['l2'],\n",
    "                'C' : [100, 10, 1.0, 0.1, 0.01]}'''\n",
    "\n",
    "clf4 = LogisticRegression(random_state=0)\n",
    "#clf4 = GridSearchCV(lr, parameter_space, cv=3, scoring=p_score)\n",
    "clf4.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****LR classificaion report for test set****')\n",
    "predictions4 = clf4.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions4))\n",
    "print(metrics.roc_auc_score(y_test,predictions4))\n",
    "\n",
    "print('\\n****LR classificaion report for train set****')\n",
    "predictions1_4 = clf4.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_4))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_4))\n",
    "\n",
    "#NB\n",
    "clf5 = GaussianNB()\n",
    "clf5.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****NB classificaion report for test set****')\n",
    "predictions5 = clf5.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions5))\n",
    "print(metrics.roc_auc_score(y_test,predictions5))\n",
    "\n",
    "print('\\n****NB classificaion report for train set****')\n",
    "predictions1_5 = clf5.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_5))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_5))\n",
    "\n",
    "#SVM\n",
    "clf6 = svm.SVC(random_state=0)\n",
    "clf6.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****SVM classificaion report for test set****')\n",
    "predictions6 = clf6.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions6))\n",
    "print(metrics.roc_auc_score(y_test,predictions6))\n",
    "\n",
    "print('\\n****SVM classificaion report for train set****')\n",
    "predictions1_6 = clf6.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_6))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_6))\n",
    "\n",
    "#SGD\n",
    "clf7 = SGDClassifier(max_iter=2000, tol=1e-3, random_state=0)\n",
    "clf7.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****SGD classificaion report for test set****')\n",
    "predictions7 = clf7.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions7))\n",
    "print(metrics.roc_auc_score(y_test,predictions7))\n",
    "\n",
    "print('\\n****SGD classificaion report for train set****')\n",
    "predictions1_7 = clf7.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_7))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_7))\n",
    "\n",
    "#KNN\n",
    "'''parameter_space = {'n_neighbors' : list(range(1,300))}'''\n",
    "clf8 = KNeighborsClassifier()\n",
    "#clf8 = GridSearchCV(kn, parameter_space, n_jobs=-1, cv=5, scoring='f1')\n",
    "clf8.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****KNN classificaion report for test set****')\n",
    "predictions8 = clf8.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions8))\n",
    "print(metrics.roc_auc_score(y_test,predictions8))\n",
    "\n",
    "print('\\n****KNN classificaion report for train set****')\n",
    "predictions1_8 = clf8.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_8))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1b5b8",
   "metadata": {},
   "source": [
    "## Feature Importance Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0a4d0b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#feature importances for MLP\n",
    "l1 = []\n",
    "l2 = clf1.coefs_[0]\n",
    "for i in l2:\n",
    "    l1.append(sum(i))\n",
    "importance_1 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_1.append(i)\n",
    "importance_1 = [(float(i)-min(importance_1))/(max(importance_1)-min(importance_1)) for i in importance_1]\n",
    "\n",
    "#feature importances for XGB\n",
    "l1 = []\n",
    "l2 = clf2.feature_importances_\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_2 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_2.append(i)\n",
    "importance_2\n",
    "importance_2 = [(float(i)-min(importance_2))/(max(importance_2)-min(importance_2)) for i in importance_2]\n",
    "\n",
    "#feature importances for RF\n",
    "l1 = []\n",
    "l2 = clf3.feature_importances_\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_3 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_3.append(i)\n",
    "importance_3 = [(float(i)-min(importance_3))/(max(importance_3)-min(importance_3)) for i in importance_3]\n",
    "\n",
    "#feature importances for LR\n",
    "l1 = []\n",
    "l2 = clf4.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_4 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_4.append(i)\n",
    "importance_4 = [(float(i)-min(importance_4))/(max(importance_4)-min(importance_4)) for i in importance_4]\n",
    "\n",
    "#feature importances for NB\n",
    "l1 = []\n",
    "l2 = permutation_importance(clf5, X_test_new, y_test)\n",
    "l2 = l2.importances_mean\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_5 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_5.append(i)\n",
    "importance_5 = [(float(i)-min(importance_5))/(max(importance_5)-min(importance_5)) for i in importance_5]\n",
    "\n",
    "#feature importances for SVM\n",
    "l1 = []\n",
    "clf6 = svm.SVC(random_state=0, kernel='linear')\n",
    "clf6.fit(X_train_new, y_train_new)\n",
    "l2 = clf6.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_6 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_6.append(i)\n",
    "importance_6 = [(float(i)-min(importance_6))/(max(importance_6)-min(importance_6)) for i in importance_6]\n",
    "\n",
    "#feature importances for SGD\n",
    "l1 = []\n",
    "l2 = clf7.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_7 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_7.append(i)\n",
    "importance_7 = [(float(i)-min(importance_7))/(max(importance_7)-min(importance_7)) for i in importance_7]\n",
    "\n",
    "#feature importances for KNN\n",
    "l1 = []\n",
    "l2 = permutation_importance(clf, X_test_new, y_test, scoring='neg_mean_squared_error')\n",
    "l2 = l2.importances_mean\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_8 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_8.append(i)\n",
    "importance_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "886d8674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "col_name = ['embedding','unigram','bigram','trigram','Top_0','Top_1','Top_0_1','Top_1_0','Weight_0',\n",
    "            'Weight_1','like','retweet','len_word','len_chars','hashtags','mention','urls','emoji',\n",
    "            'punctuation','person','organize','swear']\n",
    "N = 12\n",
    "ind = np.arange(N)\n",
    "width = 0.1\n",
    "\n",
    "xvals = importance_1[10:]\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'r')\n",
    "  \n",
    "yvals = importance_2[10:]\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='g')\n",
    "\n",
    "zvals = importance_3[10:]\n",
    "bar3 = plt.bar(ind+width*2, zvals, width, color = 'b')\n",
    "\n",
    "z1vals = importance_4[10:]\n",
    "bar4 = plt.bar(ind+width*3, z1vals, width, color = 'c')\n",
    "\n",
    "z2vals = importance_5[10:]\n",
    "bar5 = plt.bar(ind+width*4, z2vals, width, color = 'm')\n",
    "\n",
    "z3vals = importance_6[10:]\n",
    "bar6 = plt.bar(ind+width*5, z3vals, width, color = 'y')\n",
    "\n",
    "z4vals = importance_7[10:]\n",
    "bar7 = plt.bar(ind+width*6, z4vals, width, color = 'lime')\n",
    "\n",
    "z5vals = importance_8[10:]\n",
    "bar8 = plt.bar(ind+width*7, z5vals, width, color = 'violet')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(23, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=15)\n",
    "plt.title(\"Textual Feature Importance for Negativity Detection Algorithms(for 2382 tweets with underampling)\",fontsize=15)\n",
    "plt.xticks(ind+width,col_name[10:],fontsize=15)\n",
    "plt.legend((bar1, bar2, bar3 , bar4, bar5, bar6, bar7, bar8), ('MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_2.jpg' ,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "N = 10\n",
    "ind = np.arange(N)\n",
    "width = 0.1\n",
    "\n",
    "xvals = importance_1[:10]\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'r')\n",
    "  \n",
    "yvals = importance_2[:10]\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='g')\n",
    "\n",
    "zvals = importance_3[:10]\n",
    "bar3 = plt.bar(ind+width*2, zvals, width, color = 'b')\n",
    "\n",
    "z1vals = importance_4[:10]\n",
    "bar4 = plt.bar(ind+width*3, z1vals, width, color = 'c')\n",
    "\n",
    "z2vals = importance_5[:10]\n",
    "bar5 = plt.bar(ind+width*4, z2vals, width, color = 'm')\n",
    "\n",
    "z3vals = importance_6[:10]\n",
    "bar6 = plt.bar(ind+width*5, z3vals, width, color = 'y')\n",
    "\n",
    "z4vals = importance_7[:10]\n",
    "bar7 = plt.bar(ind+width*6, z4vals, width, color = 'lime')\n",
    "\n",
    "z5vals = importance_8[:10]\n",
    "bar8 = plt.bar(ind+width*7, z5vals, width, color = 'violet')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(23, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=15)\n",
    "plt.title(\"Textual Feature Importance for Negativity Detection Algorithms(for 2382 tweets with underampling)\",fontsize=15)\n",
    "plt.xticks(ind+width,col_name[:10],fontsize=15)\n",
    "plt.legend((bar1, bar2, bar3 , bar4, bar5, bar6, bar7, bar8), ('MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_2.jpg' ,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7e3a5",
   "metadata": {},
   "source": [
    "## Overfitting Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3eed2cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_f1_test_0 = [0.69, 0.67, 0.77, 0.63, 0.56, 0.65, 0.58, 0.70, 0.76]\n",
    "list_f1_train_0 = [0.90, 1, 1, 0.64, 0.87, 0.65, 0.61, 0.77, 0.89]\n",
    "list_f1_test_1 = [0.53, 0.49, 0.52, 0.51, 0.50, 0.52 ,0.51, 0.50, 0.63]\n",
    "list_f1_train_1 = [0.91, 1, 1, 0.74, 0.89, 0.74, 0.74, 0.79, 0.89]\n",
    "N = 9\n",
    "ind = np.arange(N)\n",
    "width = 0.3\n",
    "\n",
    "xvals = list_f1_test_0\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'lightcoral')\n",
    "  \n",
    "yvals = list_f1_train_0\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='brown')\n",
    "\n",
    "fig.set_size_inches(20, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=12)\n",
    "plt.title(\"F1-Score for Class 0 in Negativity Detection(for 2382 tweets)\",fontsize=12)\n",
    "plt.xticks(ind+width,['MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN' , 'P-BERT'], fontsize=10)\n",
    "plt.legend((bar1, bar2), ('test' , 'train'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_overfit.jpg' ,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55944d4",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e593c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_excel(r'../Files/5100_new.xlsx')\n",
    "data['full_text'] = data1['full_text']\n",
    "data['rate'] = data1['neg']\n",
    "\n",
    "data['rate'] = data['rate'].apply(lambda r: r if r < 2 else None)\n",
    "\n",
    "data = data.dropna(subset=['rate'])\n",
    "data = data.dropna(subset=['full_text'])\n",
    "data = data.drop_duplicates(subset=['full_text'], keep='first')\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print('data information')\n",
    "print(data.info(), '\\n')\n",
    "\n",
    "# print missing values information\n",
    "print('missing values stats')\n",
    "print(data.isnull().sum(), '\\n')\n",
    "\n",
    "# print some missing values\n",
    "print('some missing values')\n",
    "print(data[data['rate'].isnull()].iloc[:5], '\\n')\n",
    "\n",
    "data['comment_len_by_words'] = data['full_text'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "\n",
    "min_max_len = data[\"comment_len_by_words\"].min(), data[\"comment_len_by_words\"].max()\n",
    "print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')\n",
    "\n",
    "minlim, maxlim = 0, 47\n",
    "\n",
    "data['comment_len_by_words'] = data['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\n",
    "data = data.dropna(subset=['comment_len_by_words'])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "unique_rates = list(sorted(data['rate'].unique()))\n",
    "\n",
    "def rate_to_label(rate, threshold=0.0):\n",
    "    if rate <= threshold:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "data['label'] = data['rate'].apply(lambda t: rate_to_label(t, 0.0))\n",
    "labels = list(sorted(data['label'].unique()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea330db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "  \n",
    "    text = text.strip()\n",
    "    \n",
    "    # regular cleaning\n",
    "    text = clean(text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lower=True,\n",
    "        no_line_breaks=True,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "    )\n",
    "\n",
    "    # cleaning htmls\n",
    "    text = cleanhtml(text)\n",
    "    \n",
    "    # normalizing\n",
    "    normalizer = hazm.Normalizer()\n",
    "    text = normalizer.normalize(text)\n",
    "    \n",
    "    # removing wierd patterns\n",
    "    wierd_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u2069\"\n",
    "        u\"\\u2066\"\n",
    "        # u\"\\u200c\"\n",
    "        u\"\\u2068\"\n",
    "        u\"\\u2067\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = wierd_pattern.sub(r'', text)\n",
    "    \n",
    "    # removing extra spaces, hashtags\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning comments\n",
    "data['cleaned_comment'] = data['full_text'].apply(cleaning)\n",
    "\n",
    "#calculate the length of comments based on their words\n",
    "data['cleaned_comment_len_by_words'] = data['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "\n",
    "#remove comments with the length of fewer than three words\n",
    "data['cleaned_comment_len_by_words'] = data['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\n",
    "data = data.dropna(subset=['cleaned_comment_len_by_words'])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "data = data[['full_text', 'label']]\n",
    "data.columns = ['comment', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c60652",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_id'] = data['label'].apply(lambda t: 1-labels.index(t))\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=0, stratify=data['label'])\n",
    "train, valid = train_test_split(train, test_size=0.2, random_state=0, stratify=train['label'])\n",
    "negative_data = train[train['label'] == 'negative']\n",
    "positive_data = train[train['label'] == 'positive']\n",
    "\n",
    "cutting_point = min(len(negative_data), len(positive_data))\n",
    "\n",
    "if cutting_point <= len(negative_data):\n",
    "    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "if cutting_point <= len(positive_data):\n",
    "    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "if len(positive_data) > len(negative_data):\n",
    "    negative_data = negative_data.sample(n=len(positive_data), replace=True).reset_index(drop=True)\n",
    "\n",
    "if len(positive_data) < len(negative_data):\n",
    "    positive_data = positive_data.sample(n=len(negative_data), replace=True).reset_index(drop=True)'''\n",
    "train = pd.concat([negative_data, positive_data])\n",
    "#new_data = new_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "x_train, y_train = train['comment'].values.tolist(), train['label_id'].values.tolist()\n",
    "x_valid, y_valid = valid['comment'].values.tolist(), valid['label_id'].values.tolist()\n",
    "x_test, y_test = test['comment'].values.tolist(), test['label_id'].values.tolist()\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import TFBertModel, TFBertForSequenceClassification\n",
    "from transformers import glue_convert_examples_to_features\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general config\n",
    "MAX_LEN = 79\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "\n",
    "EPOCHS = 4\n",
    "EEVERY_EPOCH = 500\n",
    "LEARNING_RATE = 2e-5\n",
    "CLIP = 0.0\n",
    "dropout_rate = 0.4\n",
    "\n",
    "MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'\n",
    "#MODEL_NAME_OR_PATH = 'distilbert-base-uncased'\n",
    "#MODEL_NAME_OR_PATH = 'bert-base-multilingual-cased'\n",
    "#MODEL_NAME_OR_PATH = 'distilbert-base-multilingual-cased'\n",
    "OUTPUT_PATH = r'./pytorch_model.bin'\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ccd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: 1-i for i, label in enumerate(labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "config = BertConfig.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH, **{\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label,\n",
    "    })\n",
    "config.hidden_dropout_prob = 0.5\n",
    "config.attention_probs_dropout_prob = 0.5\n",
    "print(config.to_json_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27cd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\" A single example for simple sequence classification. \"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\" Constructs a InputExample. \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def make_examples(tokenizer, x, y=None, maxlen=128, output_mode=\"classification\", is_tf_dataset=True):\n",
    "    examples = []\n",
    "    y = y if isinstance(y, list) or isinstance(y, np.ndarray) else [None] * len(x)\n",
    "\n",
    "    for i, (_x, _y) in tqdm(enumerate(zip(x, y)), position=0, total=len(x)):\n",
    "        guid = \"%s\" % i\n",
    "        label = int(_y)\n",
    "        \n",
    "        if isinstance(_x, str):\n",
    "            text_a = _x\n",
    "            text_b = None\n",
    "        else:\n",
    "            assert len(_x) == 2\n",
    "            text_a = _x[0]\n",
    "            text_b = _x[1]\n",
    "        \n",
    "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    \n",
    "    features = glue_convert_examples_to_features(\n",
    "        examples, \n",
    "        tokenizer, \n",
    "        maxlen, \n",
    "        output_mode=output_mode, \n",
    "        label_list=list(np.unique(y)))\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_token_type_ids = []\n",
    "    all_labels = []\n",
    "\n",
    "    for f in tqdm(features, position=0, total=len(examples)):\n",
    "        if is_tf_dataset:\n",
    "            all_input_ids.append(tf.constant(f.input_ids))\n",
    "            all_attention_masks.append(tf.constant(f.attention_mask))\n",
    "            all_token_type_ids.append(tf.constant(f.token_type_ids))\n",
    "            all_labels.append(tf.constant(f.label))\n",
    "        else:\n",
    "            all_input_ids.append(f.input_ids)\n",
    "            all_attention_masks.append(f.attention_mask)\n",
    "            all_token_type_ids.append(f.token_type_ids)\n",
    "            all_labels.append(f.label)\n",
    "\n",
    "    if is_tf_dataset:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "            'input_ids': all_input_ids,\n",
    "            'attention_mask': all_attention_masks,\n",
    "            'token_type_ids': all_token_type_ids\n",
    "        }, all_labels))\n",
    "\n",
    "        return dataset, features\n",
    "    \n",
    "    xdata = [np.array(all_input_ids), np.array(all_attention_masks), np.array(all_token_type_ids)]\n",
    "    ydata = all_labels\n",
    "\n",
    "    return [xdata, ydata], features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_base, train_examples = make_examples(tokenizer, x_train, y_train, maxlen=256)\n",
    "valid_dataset_base, valid_examples = make_examples(tokenizer, x_valid, y_valid, maxlen=256)\n",
    "\n",
    "test_dataset_base, test_examples = make_examples(tokenizer, x_test, y_test, maxlen=256)\n",
    "[xtest, ytest], test_examples = make_examples(tokenizer, x_test, y_test, maxlen=256, is_tf_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset(dataset, batch_size):\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(dataset, batch_size):\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_training_dataset(train_dataset_base, TRAIN_BATCH_SIZE)\n",
    "valid_dataset = get_training_dataset(valid_dataset_base, VALID_BATCH_SIZE)\n",
    "\n",
    "train_steps = len(train_examples) // TRAIN_BATCH_SIZE\n",
    "valid_steps = len(valid_examples) // VALID_BATCH_SIZE\n",
    "\n",
    "train_steps, valid_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "def build_model(model_name, config, learning_rate=3e-5):\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(MODEL_NAME_OR_PATH, config, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1)\n",
    "\n",
    "final_accuracy = r.history['val_accuracy']\n",
    "print('FINAL ACCURACY MEAN: ', np.mean(final_accuracy))\n",
    "\n",
    "model.save_pretrained(os.path.dirname(OUTPUT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d326397",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = model.evaluate(test_dataset_base.batch(TEST_BATCH_SIZE))\n",
    "print()\n",
    "print(f'Evaluation: {ev}')\n",
    "print()\n",
    "\n",
    "predictions = model.predict(xtest)\n",
    "ypred = predictions[0].argmax(axis=-1).tolist()\n",
    "\n",
    "print()\n",
    "print(classification_report(ytest, ypred, target_names=labels))\n",
    "print()\n",
    "\n",
    "print(f'F1: {f1_score(ytest, ypred, average=\"weighted\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
