{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83a56ec",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io, json\n",
    "import csv\n",
    "import requests\n",
    "import stanza\n",
    "import emojis\n",
    "import collections\n",
    "import math\n",
    "import twitter\n",
    "import fnmatch\n",
    "import pickle\n",
    "import urllib\n",
    "import re, string, ast, emoji, json, urlexpander, sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files  \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.corpus import words\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy.stats import kurtosis, skew, entropy\n",
    "from textstat.textstat import textstat\n",
    "from textblob import TextBlob\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from persiantools.jdatetime import JalaliDate\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import hazm\n",
    "from hazm import Normalizer, Lemmatizer, WordTokenizer\n",
    "from cleantext import clean\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import imblearn\n",
    "from numpy import mean\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from googletrans import Translator\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "#fasttext.util.download_model('fa', if_exists='ignore')\n",
    "#!pip install googletrans==4.0.0-rc1\n",
    "ft = fasttext.load_model('cc.fa.300.bin')\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b07b2",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3c114c",
   "metadata": {},
  },
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'../Files/5100_new.xlsx')\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac0739",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "organizations = pd.read_excel(r'../Files/organizations.xlsx')\n",
    "organizations['name2'] = organizations['name2'].astype(str)\n",
    "name_of_organizations = organizations['name2'].iloc[:130].to_list()\n",
    "organization = []\n",
    "for n in name_of_organizations:\n",
    "    organization.append(n.rstrip())\n",
    "organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c0e81",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swears = pd.read_json(r'../Files/data_swear.json', encoding='utf-8')\n",
    "swear = swears['word'].to_list()\n",
    "swear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46830b73",
   "metadata": {},
   "source": [
    "# Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_like'] = [like for like in data['favorite_count']]\n",
    "\n",
    "data['tweet_retweet_count'] = [retweet for retweet in data['retweet_count']]\n",
    "\n",
    "data['tweet_length_word'] = [len(word_tokenize(t)) for t in data['full_text']]\n",
    "\n",
    "data['tweet_length_characters'] = [len(t) for t in data['full_text']]\n",
    "\n",
    "data['hashtags'] = data['full_text'].str.findall(\"(#[^#\\s]+)\")\n",
    "\n",
    "pattern = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)'\n",
    "data['urls']= data['full_text'].str.findall(pattern)\n",
    "\n",
    "data['tweet_num_hashtags'] = [len(h) for h in data['hashtags']]\n",
    "\n",
    "def extract_mention_set(text):\n",
    "    mention_list = re.findall(\"@([a-zA-Z0-9]{1,15})\", text)\n",
    "    return len(mention_list)\n",
    "\n",
    "data['tweet_num_mention'] = data['full_text'].apply(extract_mention_set)\n",
    "\n",
    "data['tweet_num_urls'] = [len(h) for h in data['urls']]\n",
    "\n",
    "def extract_emojis(s):\n",
    "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "data['tweet_num_emoji'] = [len(extract_emojis(t)) for t in data['full_text']]\n",
    "\n",
    "data['tweet_emoji'] = [extract_emojis(t) for t in data['full_text']]\n",
    "\n",
    "persian_punctuation = '.ุุ:()ยซยปุ![]-/'\n",
    "def extract_punctuation(s):\n",
    "  return ''.join(c for c in s if c in list(persian_punctuation))\n",
    "\n",
    "data['tweet_num_punctuation'] = [len(extract_punctuation(t)) for t in data['full_text']]\n",
    "\n",
    "list_o = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_o.append(0)\n",
    "    for name in organization:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_o[i] += 1\n",
    "data['organize_names'] = list_o\n",
    "\n",
    "list_sw = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_sw.append(0)\n",
    "    for name in swear:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_sw[i] += 1\n",
    "data['swear_words'] = list_sw\n",
    "\n",
    "cn = ['ุฑุฆุณ' , 'ุฌูู' , 'ุฑุณ' , 'ููุช' , 'ููุฑุนูุฒุงุฏู' , 'ููุฑ ุนูุฒุงุฏู' , 'ูุญุณู ุฑุถุง' , 'ูุงุถ ุฒุงุฏู' , 'ูุงุถโุฒุงุฏู',\n",
    "      'ุฒุงฺฉุงู' , 'ููุงุจโุฒุงุฏู' , 'ููุงุจ ุฒุงุฏู' , 'ุจุทุญุง' , 'ุญูุงฺ' , 'ูููุงูุฑุฏ' , 'ุชุงุฌ ุฒุงุฏู' , 'ุชุงุฌโุฒุงุฏู' ,\n",
    "      'ุชุงุฌุฒุงุฏู' , 'ูุงูุจุงู' , 'ูุงูโุจุงู' , 'ูุงู ุจุงู' , 'ูุญููุฏ ุตุงุฏู' , 'ููุฏ ุงุณูุงุนู' ,\n",
    "      'ูุญูุฏ ุงุณูุงุนู' , 'ฺฉูุงฺฉุจุงู' , 'ุฌูุงุฏ ุงูุฌ' , 'ุฎุงุชู' , 'ุฎุงูููโุง' , 'ุฎุงููู ุง' , 'ูุญูุฏ ุญุณู', \n",
    "      'ุญูุฏ ุณุฌุงุฏ' , 'ูุนุตููู ุงุจุชฺฉุงุฑ' , 'ุณุงุฏุงุช ูฺุงุฏ' , 'ุณุงุฏุงุชโูฺุงุฏ' , 'ูุญูุฏ ุฏููุงู' , 'ุฌูุฑู' , 'ุนุจุฏุงูููฺฉ' , 'ูุงุฑุฌุงู',\n",
    "      'ุนุฑุงูฺ' , 'ุถุฑุบุงู' , 'ุฑูุญุงู' , 'ุฑุณุชู ูุงุณู' , 'ูุฑุชุถู' , 'ููุจุฎุช' , 'ูุฑฺฉุงุธู' , 'ูุฎุจุฑ' , 'ุธุฑู' , \n",
    "      'ุนุฑุงูโฺ' , 'ุนุฑุงู ฺ' , 'ุฒุงุฑุน ูพูุฑ' , 'ุฒุงุฑุนโูพูุฑ' , 'ุนู ุงููู' , 'ุนูโุงููู' , 'ุฌูุงูฺฏุฑ' , 'ุฌูุงูโฺฏุฑ' ,\n",
    "      'ุฌูุงู ฺฏุฑ' , 'ุฎุฒุนู' , 'ุงฺูโุง' , 'ูุงุนุธ' , 'ุจุงูุฑ ฺฉู' , 'ุนุจุฏุงูููุงู' , 'ุงุญูุฏโูฺุงุฏ' , 'ุงุญูุฏ ูฺุงุฏ' , 'ุขุดุชุงู' ,\n",
    "     'ุขูุง' , 'ุขูุง' , 'ุฎุงูู' , 'ูุณุฆูู' , 'ูุฒุฑ' , 'ุขุฎููุฏ' , 'ุฑูุจุฑ' , 'ุฑูุจุฑ' , 'ุขุฎููุฏ' , 'ุงุณุชุงูุฏุงุฑ' , 'ุดูุฑุฏุงุฑ']\n",
    "list_cn = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_cn.append(0)\n",
    "    for name in cn:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_cn[i] += 1\n",
    "data['person_names'] = list_cn\n",
    "\n",
    "'''def get_domain_from_url(url):\n",
    "  return urllib.parse.urlparse(url).netloc\n",
    "\n",
    "data['urls_dom'] = data['urls'].apply(lambda X: [get_domain_from_url(x) for x in X])\n",
    "url_list= []\n",
    "for i in range(len(data['urls_dom'])):\n",
    "    for url in data['urls_dom'].iloc[i]:\n",
    "        url_list.append(url)\n",
    "url_set = set(url_list)\n",
    "urls = []\n",
    "for url in url_set:\n",
    "    urls.append(url)\n",
    "if(len(urls) >= 10):\n",
    "    U = urls[:10]\n",
    "else:\n",
    "    U = urls\n",
    "for j in range(len(U)):\n",
    "    list_u = [0 for i in range(len(data))]\n",
    "    for i in range(len(data)):\n",
    "        if data['tweet_num_urls'].iloc[i] > 0:\n",
    "            if U[j] in data['full_text'].iloc[i]:\n",
    "                list_u[i] = 1\n",
    "    data[U[j]] = list_u.copy()'''\n",
    "\n",
    "data['tweet_emoji'] = [extract_emojis(t) for t in data['full_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2e5fb",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji_to_text(text, delimiters=(' ', ' ')):\n",
    "    \"\"\" Convert emojis to something readable by the vocab and model \"\"\"\n",
    "    text = emoji.demojize(text, delimiters=delimiters)\n",
    "    return text\n",
    "emo_list = []\n",
    "emoji_set = set(data[data['tweet_num_emoji']>0]['tweet_emoji'])\n",
    "for e in emoji_set:\n",
    "    for ee in e.strip():\n",
    "        emo_list.append(ee)\n",
    "emo_set = set(emo_list)\n",
    "emo_pic = []\n",
    "for e in emo_set:\n",
    "    emo_pic.append(e)\n",
    "emo = [convert_emoji_to_text(e) for e in emo_set]\n",
    "emo = [e.split('_') for e in emo]\n",
    "emo = [' '.join(e) for e in emo]\n",
    "emo_fa = []\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "for i in range(len(emo)):\n",
    "    trans = translator.translate(emo[i], dest='fa')\n",
    "    emo_fa.append(trans)\n",
    "emo_farsi = []\n",
    "for e in emo_fa:\n",
    "    emo_farsi.append(e.text)\n",
    "emo_farsi = [' ' + e + ' ' for e in emo_farsi]\n",
    "Emojis = dict(zip(emo_pic, emo_farsi))\n",
    "Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ffe3565e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    if data['tweet_num_emoji'].iloc[i] > 0:\n",
    "        for e in emo_pic:\n",
    "            if e in data['full_text'].iloc[i]:\n",
    "                data['full_text'].iloc[i] = data['full_text'].iloc[i].replace(e, Emojis[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6e5132ab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ุงูุชุตุงุฏ ุงุณูุงู ',\n",
       " ' ุนุจุฑุช ุงูฺฉุฑุงู ',\n",
       " ' ุชูุฑู ',\n",
       " ' ุงูุฏฺฉุง ',\n",
       " ' ุณูุฑู ููุดูู ',\n",
       " ' ุญุถุฑุช ูุญูุฏ ',\n",
       " ' ูุงุฏุฑุงู ุชุญูู ุณุงุฒ ',\n",
       " ' ุนุฑุจุณุชุงู ',\n",
       " ' ุณุฏูุญูุฏุฎุงุชู: ',\n",
       " ' ูุงูุง ูุงุฑุฏุงุช ',\n",
       " ' ุฏุงูุดฺฏุงู ุนููู ู ุชุญููุงุช ',\n",
       " ' ูุฏุฑุช ุณูุฌุงู ',\n",
       " ' ูู ูุนูุชุงู ุฒูุงูุฏุงุฑุงู ',\n",
       " ' ูุงุณู ุณููุงู ',\n",
       " ' ุฑูููุฑุฏ ',\n",
       " ' ุขุฒููู ุนุฏุงูุช ',\n",
       " ' ุงูุช ุชุฑู ',\n",
       " ' ุจูุง ',\n",
       " ' ุฑูุญุงู ',\n",
       " ' ุงุณูุงูุดูุฑ ',\n",
       " ' ฺฏุฒุงุฑุด ุจูุฏุฌู ',\n",
       " ' ุดูุงูุช ุขุฑุง ููุงูุฏฺฏุงู ',\n",
       " ' ุงูุฏุงู ู ',\n",
       " ' ุขุช ุงููู ุฑุฆุณ ',\n",
       " ' ุฎุงู ููุชู ',\n",
       " ' ูุฑู ุฌููุจู ',\n",
       " ' ุฌููุง ',\n",
       " ' ุฎุณุงุฑุช ูุญุถ ',\n",
       " ' ูุดุฑ ุญุฏุงฺฉุซุฑ ',\n",
       " ' ุฎูุฒุณุชุงู ',\n",
       " ' ุจุงูฺฉ ูุง ',\n",
       " ' ุฎุฑุฏ ููฺฉ ',\n",
       " ' ุงููุช ',\n",
       " ' ูุฏูู ูุฑุฏู ',\n",
       " ' ููุฑูุฒฑดฐฐ ',\n",
       " ' ูุณุงุฏ. ',\n",
       " ' ุงูุงู ุฎุงููู ุง ',\n",
       " ' ุดุนู ุณู ',\n",
       " ' ูพุฑุณุชู ',\n",
       " ' ุณูฺฏ ูุจุฑ ',\n",
       " ' ุทูุง ',\n",
       " ' ุฑูุจุฑ ',\n",
       " ' ูุณููุงูุ ',\n",
       " ' ูุฑุฏู ุณุงุฒ ',\n",
       " ' ุขูุฑฺฉุง! ',\n",
       " ' ุฌูุงุช ุชุฑูุฑุณุช ุงููุงุฒ ',\n",
       " ' ููุดฺฉ ูุง ุจุงูุณุชฺฉ ',\n",
       " ' ุฏููุช ูพุงุณุฎฺฏู ',\n",
       " ' ุงูุจุญุฑูู ',\n",
       " ' ูุธููู ',\n",
       " ' ุขุชุด ุณูุฒ ุฒุงูุฏุงู ',\n",
       " ' ูุธุงู ุณูุทู ',\n",
       " ' ุจุงุฒูฺฏุฑ ูุงููู ุงุณุงุณ ',\n",
       " ' ูุณูู ',\n",
       " ' ุฑุฃ ',\n",
       " ' ุนูุฑุถุง ุฒุงฺฉุงู ',\n",
       " ' ุขุดุช ูู ',\n",
       " ' ุจุฑุง ุณุฑุจุงุฒ ',\n",
       " ' ุดุนุงุฑ ',\n",
       " ' ฺฉุฑุจูุง. ',\n",
       " ' ุฌุฑุงู ุชุญุฑู ',\n",
       " ' ุชุญูู ',\n",
       " ' ุจ ุงูุตุงู ',\n",
       " ' ุฌูุด ุงูุชุตุงุฏ ',\n",
       " ' ุงุจุชฺฉุงุฑ ',\n",
       " ' ุชูุงููุ ',\n",
       " ' ุงูุชุฎุงุจุงุช ฑดฐฐ ',\n",
       " ' ูุฌูุน ุชุดุฎุต ูุตูุญุชุ ',\n",
       " ' ุณุงูุงูู  ',\n",
       " ' ุตุฏุง ูุฑุฏู ',\n",
       " ' ููุณุทู! ',\n",
       " ' ุฑุดุช ',\n",
       " ' ฺฉูุฏฺฉ ููุณุฑ ',\n",
       " ' ูุญุณู ุงฺู ุง ',\n",
       " ' ูพุงุณุฎฺฏููู ',\n",
       " ' ุงููุงุฒ ',\n",
       " ' ุฑุญูุช ',\n",
       " ' ุงููุฏฺฏ ููุง ',\n",
       " ' ููุถ ุจุฑุฌุงู ',\n",
       " ' ุฏุงุฏฺฏุงู ุนูู ',\n",
       " ' ุฏุฎุชุฑ ',\n",
       " ' ุขูุฑููุง ',\n",
       " ' ุฑุณ ุฌูููุฑ ',\n",
       " ' ุญุณู ุฒุฏุงู ',\n",
       " ' ุงุฑุงู ููุ ',\n",
       " ' ููุงููุช ',\n",
       " ' ุจุณุฌ ',\n",
       " ' ูุงุฑุฏุงุช ',\n",
       " ' ุฌูููุฑ ุงุณูุงู ',\n",
       " ' ูขูจ ูุฑุฏุงุฏ ',\n",
       " ' ุงุฒุฏูุงุฌ ',\n",
       " ' ุฏุฑูุบ ููููุน ',\n",
       " ' ูุญููุฏู ',\n",
       " ' ูุฑุฎุต ',\n",
       " ' ูุฐุงฺฉุฑู ',\n",
       " ' ุฎูู ',\n",
       " ' ุดุนู ',\n",
       " ' ูุงูุน ุฒุฏุง ',\n",
       " ' ูุธุงู ุงุณูุงูุ ',\n",
       " ' ุณุงุญู ',\n",
       " ' ุณููุง ',\n",
       " ' ุฑูุงู ',\n",
       " ' ุชุงุซุฑ ูุทุน ูุนุฏู ',\n",
       " ' ุทุจุฑ ',\n",
       " ' ุงูฺฉุฑุงู ',\n",
       " ' ุฏุฑุณูุง ุชุงุฑุฎ ',\n",
       " ' ุตุฑุจุณุชุงูุ ',\n",
       " ' ุฌูุงูฺฏุฑ ',\n",
       " ' ุฌุจุฑุงู ',\n",
       " ' ุณุงุฒูุงู ุจุฑูุงูู ุจูุฏุฌู ',\n",
       " ' ุฑุฌุงู ',\n",
       " ' ููุงุธุฑูฑดฐฐ ',\n",
       " ' ุงุนูุญุถุฑุช ',\n",
       " ' ุชูฑฑููุฑู ',\n",
       " ' ุฌูุงูุงู ',\n",
       " ' ุงูุฑุงู ููุดุนุจ ุจุตูุฉ ',\n",
       " ' ูุงุดู ุฑูุณูุฌุงู ',\n",
       " ' iran ',\n",
       " ' ุชุฌุฑุจู ุฏููุช ุฏูุงุฒุฏูู ',\n",
       " ' ฺฉูุฏุชุง  ุนูู ุฌูููุฑุช ',\n",
       " ' ุขู ุณุนูุฏ ',\n",
       " ' ุญฺฉููุช ',\n",
       " ' UNGA ',\n",
       " ' ุฒุจุงู ุงูฺฏูุณ ',\n",
       " ' ุฅุณุฑุงุฆูู ',\n",
       " ' ุขุจุงูนธ ',\n",
       " ' ุชูุจู ',\n",
       " ' ุขุฒุงุฏ ',\n",
       " ' ููุช! ',\n",
       " ' ุณูพุงู ',\n",
       " ' ุงุณุชุงู ูุงุฑุณ ',\n",
       " ' ูุฑุฏู ฺฏูู ููุฏูุฏ ',\n",
       " ' ุงุฑุงูู ',\n",
       " ' ฺฉุฑูุงู ',\n",
       " ' Riyadh ',\n",
       " ' ุงุทูุงุนู ',\n",
       " ' ุณุฏูุญูุฏุฎุงุชูุ ',\n",
       " ' ฺฉุงูุฑ ุนุงุฏู ',\n",
       " ' ูุฑูุณ ',\n",
       " ' ูุงุฑุฌุงู ',\n",
       " ' ุชูุฒุน ุนุงุฏูุงูู ',\n",
       " ' ุฑูุฒูุงูู ูฺฏุงุฑุงูุ ',\n",
       " ' ุงูุชุตุงุจุงุช ุฑูุจุฑ ',\n",
       " ' ุณุงุฒูุงู ูพุฒุดฺฉุงู ุจุฏูู ูุฑุฒ ',\n",
       " ' ุงุตููุงู ',\n",
       " ' ุฏุงูุดุฌู ',\n",
       " ' ูพุงูุจุฑ ุงฺฉุฑู(ุต) ',\n",
       " ' ุฌูุง ',\n",
       " ' ุงุตูุงุญ ',\n",
       " ' ุณุฑูุงู ุฏุงุฑ ',\n",
       " ' ุงูุฏูู ุงูุนุฑุจุฉ ',\n",
       " ' ุงูุชุฎุงุจุงุชนด ',\n",
       " ' ุจุฑูุงูู ูุง ุจุฏูู ุจูุฏุฌู ',\n",
       " ' ูุงฺุงู ',\n",
       " ' ุทุฑุญ ุตุงูุชุ ',\n",
       " ' ุชุฑุฏุฏุ ',\n",
       " ' ุฑูุณูุง ',\n",
       " ' ุงุฑุงู ูู ',\n",
       " ' ุดูุฑุงูุง ุดูุฑ ',\n",
       " ' ุงูุงู ุฎูู ',\n",
       " ' ุงุนุชุฑุงู ',\n",
       " ' ุญู ุงููุงุณ ',\n",
       " ' ุณุณุชุงู ',\n",
       " ' 16Day ',\n",
       " ' ุจุงฺฉุฑ) ',\n",
       " ' ุณุงุณุช ุฒุฏฺฏ ',\n",
       " ' ุฌูุด ',\n",
       " ' islam ',\n",
       " ' ูุณุคูุช ฺฉูุฑ ',\n",
       " ' ุงูุฑุงุถ ',\n",
       " ' ุตุงูุญู ',\n",
       " ' ูุฒุฑ ฺฉุดูุฑ: ',\n",
       " ' ุขู ',\n",
       " ' ุฏุงููุณ ',\n",
       " ' ูุนุดุช ',\n",
       " ' ููุฏุจุงู ุฒูุงู ',\n",
       " ' ูุดุงุท ุงุฌุชูุงุน ',\n",
       " ' ูุงู ุฎุฑุฏ ',\n",
       " ' ุบุฒุฉ ',\n",
       " ' ุทุงููุงู ',\n",
       " ' ูู ุฌูฺฏ ูู ูุฐุงฺฉุฑู ',\n",
       " ' ุดุฑู ',\n",
       " ' ฺฏุฑุงู ุดุจ ุนุฏ ',\n",
       " ' ุชุจูุช ',\n",
       " ' ุณุนูุฏ ',\n",
       " ' ุฎุงูุฑูุงูู ',\n",
       " ' ูู ',\n",
       " ' Iranelection ',\n",
       " ' ูุฏุฆู ',\n",
       " ' ุงูุตูู ',\n",
       " ' ุฏูู60ุ ',\n",
       " ' ุงูุฑฺฉุง ',\n",
       " ' ุฑุดุชู ',\n",
       " ' ูุทุงุฑ ',\n",
       " ' ุตุฏุฑุงูุณุงุฏุงุช ',\n",
       " ' ุญุตุฑ ุบุฑูุงููู ',\n",
       " ' ุชุญูู: ',\n",
       " ' ุงุชูฺฉุดุฏู ',\n",
       " ' ุชุญุฑู  ',\n",
       " ' ูุงููู ุงุณุงุณ ',\n",
       " ' ุฌูููุฑ ุงุณูุงู ุงุฑุงู ',\n",
       " ' ูู ุชุฑู ',\n",
       " ' ุจู ุณููุงู ',\n",
       " ' ุฌูฺฏ ูุงุจุช ',\n",
       " ' ุงูุงูุช ฺฉููุฏฺฏุงู ',\n",
       " ' NCAA ',\n",
       " ' ุฏููุงูพุณุงู ',\n",
       " ' ุฑุฆุณ ุณุชุงุฏุง   ู ุงูุงู)ู ุฎูุงูู ',\n",
       " ' ุฎุจุฑุฎูุจ! ',\n",
       " ' ฺฉุงูุง ',\n",
       " ' ุขุฑูุงู ',\n",
       " ' ุขุฒุงุฏ ูุฎุงูู ',\n",
       " ' ุทุฑุญ ูู ุจูู ุงุฌุชูุงุน ',\n",
       " ' ููุงุจ ูุงูุณ ',\n",
       " ' ุทุฑุญ ุชุณูู ุตุฏูุฑ ูุฌูุฒูุง ฺฉุณุจ ูฺฉุงุฑ ',\n",
       " ' ุฏุฑูุบ ',\n",
       " ' ุฑุญูู ููุนุงููู ',\n",
       " ' ุงูููุงุจ ',\n",
       " ' ูููุง ',\n",
       " ' ุฎูุฒุณุชุงู ุชููุง ูุณุช ',\n",
       " ' ุชุฎุฑุจ ',\n",
       " ' ุฏุงุฏฺฏุงู ',\n",
       " ' ุขู ฺูุงู ',\n",
       " ' ุฎุฑุงุณุงู ุดูุงู ',\n",
       " ' ุจุฏุงุฎูุงู ',\n",
       " ' ฺฉูุฏุชุง ุงูุชุฎุงุจุงุช ',\n",
       " ' ูุฌุงุฒ ',\n",
       " ' ุงูุบุงูุณุชุงู ',\n",
       " ' ุงุฎูุงู ',\n",
       " ' ุงุฑุงู\\u2066๐ฎ๐ท ',\n",
       " ' ุงุญูุฏ ูุณุนูุฏ ',\n",
       " ' ุงูุชุตุงุฏ ูู ',\n",
       " ' ฺฏุฑุณูฺฏ ',\n",
       " ' ุงูุฅูุฑฺฉุฉ ',\n",
       " ' ุตุฏู!ุจู ',\n",
       " ' ฺู ',\n",
       " ' ุฌูุงูฺฏุฑุ ',\n",
       " ' ฺฏุงุฑุงูุช ',\n",
       " ' ุฑุณ ุฌูููุฑ ุงูุชุตุงุฏุฏุงู ',\n",
       " ' ุดุจู ุนูุฏ) ',\n",
       " ' ุดูุฑุฏุงุฑ ุชูุฑุงู ',\n",
       " ' ุฏูฺฏุงูู ูุชุนุงุฑุถ ',\n",
       " ' ุชูฺฉุฑุงุฑ ',\n",
       " ' ุณุฑุฏุงุฑ ',\n",
       " ' ุญูุจฺู ',\n",
       " ' ุฑุงุถู ',\n",
       " ' ุญุฐู ',\n",
       " ' ูุงุฏุฑ ',\n",
       " ' ุฌูุงู ',\n",
       " ' ุชุงุฑุฎ ',\n",
       " ' ุญุงุฌ ุงุญูุฏ ฺฉุงุธู ',\n",
       " ' ุงุณุชุงู ูุฑูุฒฺฏุงู ',\n",
       " ' fatf ',\n",
       " ' ุฑุฆุณ ุฌูููุฑ ',\n",
       " ' ุฌุฑู ุณุงุณ ',\n",
       " ' ุฒูุฌุงู ',\n",
       " ' ุชููุฏฺฉููุฏฺฏุงู ',\n",
       " ' ุฒู ',\n",
       " ' ูุฌูุณ ',\n",
       " ' ุจู ุนูู ฺฉุงุฑ ุจุฑุขุฏ. ',\n",
       " ' ุงููุฏ ',\n",
       " ' ุญู ุงูุชุฎุงุจ ',\n",
       " ' ุฏุฒุฏุงู ุฏุฑุง ',\n",
       " ' ุดุฑูุท ุถูู ุนูุฏ ',\n",
       " ' ุชูุงูู ',\n",
       " ' ุญฺฉูุฑุงู ุงูุชุตุงุฏ ',\n",
       " ' ูุญุณู ูุฎุฑ ุฒุงุฏู ',\n",
       " ' ุฏูุงูพูุงุณ ',\n",
       " ' ุณุฑุฏุงุฑ ุฏููุง ',\n",
       " ' ุฌูุณุช ',\n",
       " ' ุญูุธ ุงููุช ุฑูุงู ุฌุงูุนู ',\n",
       " ' ูุฑูุฒุฌุง ',\n",
       " ' ูุฌูุณุ ',\n",
       " ' ูุฏุฑุงู ุฒู ',\n",
       " ' ุฑุฆุณุ ',\n",
       " ' ูุณุนูุฏุดุฌุงุน ',\n",
       " ' ุฏููุช ุฌูุงู ุญุฒุจ ุงููู ',\n",
       " ' NeverThreatenAnIranian. ',\n",
       " ' ุนููฺฉุฑุฏูุง ',\n",
       " ' ุงุฑ ฺฉูฺฉ ',\n",
       " ' ุจุฒูู ุฒุฑ ูุฒ ',\n",
       " ' ฺฉุงูุง ุจุฑฺฏ ',\n",
       " ' ุขุจุฒุงู ',\n",
       " ' ุฑุฏุตูุงุญุช ',\n",
       " ' ุชูุณุนู ุณูุงุฏ ูุญุท ุฒุณุช ',\n",
       " ' ุฎูุงุณุช ุนููู ',\n",
       " ' ฺฉุงุณุจุงู ุฑุง ',\n",
       " ' ูุฌู ',\n",
       " ' ุญุฑู ูุฑุฏู ',\n",
       " ' ุงูุฑฺฉุจุฑ:ุญุฑฺฉุช ',\n",
       " ' ูพุงุงู ',\n",
       " ' ุชุบูุฑุงุช ููููุณ ',\n",
       " ' ูุจุงุฑฺฉ ',\n",
       " ' ูุงุดู ',\n",
       " ' ุฏููุช ุณูู ุฑูุญุงู ',\n",
       " ' ูุตูุญุช  ',\n",
       " ' letter4u ',\n",
       " ' ุฑูุณุชุง ุฑุง ุญุฐู ูฺฉูุฏ ',\n",
       " ' ุฏฺฉุชุฑ ุทุญุงู ูุธู ',\n",
       " ' ุดูุฏ ฺฏูุฌ ',\n",
       " ' ุชููุฏ ูู ',\n",
       " ' ฺฉุงุฑฺฏุฑุงู ',\n",
       " ' ุญููู ุจุณุชู ',\n",
       " ' ุงูุชุฎุงุจุงุชฑดฐฐ ',\n",
       " ' ุขูู ูุงุฑุฌุงู ',\n",
       " ' ุชุงุฌุฒุงุฏู ',\n",
       " ' ูฺฉูู ูฺฏุงู ุขูุฑฺฉุง ',\n",
       " ' ุงูุฑุงู ',\n",
       " ' ุณู ',\n",
       " ' ุงูุงู ุตุงุฏู(ุน) ',\n",
       " ' ุณูุฑูุฉ ',\n",
       " ' ุงูููุงุจ ',\n",
       " ' ูุตู ููุชู ',\n",
       " ' ุถุฏ ุงูุชุตุงุฏ ููุงููุช ',\n",
       " ' ุงุนุชุฑุงุถุงุช ูุณุงููุช ุขูุฒ ',\n",
       " ' ูพุงุงู ูุณ ',\n",
       " ' ูุฑุฏุง ุจุฑุง ุดูุงุณุช ',\n",
       " ' ููุงุธุฑู ุณูู ',\n",
       " ' ูพุงุณุฎฺฏู ',\n",
       " ' ุจุงูฺฉ ',\n",
       " ' ุฎูุฏุฑูุณุงุฒุงู ุฌูุงู ',\n",
       " ' ุงูุชุฎุงุจุงุชุ ',\n",
       " ' ูู ุงฺฉููู ',\n",
       " ' ูุงุตุฑ ููฺฉ ูุทุน ',\n",
       " ' election94 ',\n",
       " ' ุณูู ุฎุฑุฏุงุฏ ',\n",
       " ' ุณุงุฒุด ',\n",
       " ' ุจููุงุงู ',\n",
       " ' ุขุฒุงุฏ ',\n",
       " ' ุฏุฑูุบุ ',\n",
       " ' ุจูุง ุจุฒุฑฺฏ ',\n",
       " ' ุงุตูุงุญ ุทูุจุ ',\n",
       " ' ุตุงู ฺฏููพุงฺฏุงู ',\n",
       " ' ุงููุช ูู ',\n",
       " ' Nowruz ',\n",
       " ' ูููุฏุณ ุงูุชุฎุงุจุงุช: ',\n",
       " ' ูุฑ ููุฑ ุฏู ููุฑ ',\n",
       " ' ฑถ ุขุฐุฑ ',\n",
       " ' ุงูุญุตุงุฑ ุงููุช ',\n",
       " ' ูู ุฌูฺฏ ูู ุชุญุฑู\\u2069 ',\n",
       " ' ุตุงูุญุงู ',\n",
       " ' ููุงููู ',\n",
       " ' ุงูุฑุงููุงู ',\n",
       " ' ุฏููุช ุณุงู\" ',\n",
       " ' ุชููุฏ ูพุดุชุจุงู ูุง ูุงูุน ุฒุฏุง ูุง ',\n",
       " ' ุงูููุงุจ ุงูุชุตุงุฏ ฑดูู ',\n",
       " ' ุฒูุฺฉ ',\n",
       " ' ูุฎุงุทุฑู ',\n",
       " ' ฺฉุฏุฎุฏุง ',\n",
       " ' ุฎุงุชู ุญุฐู ุดุฏู ูุณุช ',\n",
       " ' ูุชุงูุงูู ',\n",
       " ' ุงููุงู ุชููฺฉ ',\n",
       " ' ุงุฑุชุฌุงุน ',\n",
       " ' ุฌูฺฏ ุฏุงุฎู ',\n",
       " ' ุจูุจุงุฑุงู ุดูุง ',\n",
       " ' ุทูุจฺฉุงุฑ ',\n",
       " ' ุณุฏูุญูุฏุฎุงุชู ',\n",
       " ' ุธูู ',\n",
       " ' ููุงุฌุฑ ',\n",
       " ' ุฌููุงุช ฺฉูุฏ ',\n",
       " ' ูุณุฆููุงู ฺฉุงุฎ ูุดู ',\n",
       " ' ูพุฑููุฏู ูุณุชู ุง ',\n",
       " ' ูุญุฑู ',\n",
       " ' ุงุณูุงู ุฏู ฺฉุงูู ',\n",
       " ' ุงูุชุตุงุฏ ููุงููุช ',\n",
       " ' ูุณุชู ',\n",
       " ' ูพุฑูุฒ ',\n",
       " ' ููุดููุฏุณุงุฒ ',\n",
       " ' ููู ูพุฑุณ ',\n",
       " ' womensday2021 ',\n",
       " ' ููุฏูฺฏ ',\n",
       " ' ุนุดู ',\n",
       " ' ุงูุงู ',\n",
       " ' ูุนุฏู ูุงููฺฉู ูุฏูู ',\n",
       " ' ุฑูุบู ',\n",
       " ' ุฑูุงุถ ',\n",
       " ' ุงุณุชุงู ฺฉุฑูุงู ',\n",
       " ' ุงููุฑูุง ',\n",
       " ' ุฏููุช ุงุฒุฏูู ',\n",
       " ' ุธูู ุขุดฺฉุงุฑ ',\n",
       " ' ูุงุชุญุงู ุฎุจุฑ ',\n",
       " ' ุงุณุชุงู ุดุฏู ุงูุชุฎุงุจุงุช ',\n",
       " ' ูุง ุฏุงุฆุฑ ูุฏุงุฑ ููุงูุนู ',\n",
       " ' ุงุญูู ูุจู ',\n",
       " ' ุณุงุณุช ุฎุงุฑุฌ ',\n",
       " ' ูุญุฏุช ูู ',\n",
       " ' ุฎูุฏุฑู ',\n",
       " ' ุญฺฉููุชุ ',\n",
       " ' ฺฉุฑุฏุณุชุงู ุนุฑุงู ',\n",
       " ' ุญููู ูุฑุฏู ',\n",
       " ' ุชุณูุช ',\n",
       " ' ุจูุงุฑ ',\n",
       " ' ุญุฐู ฺฏุฒูู ',\n",
       " ' ุฑูุน ุชุจุนุถ ',\n",
       " ' ุฑุงุณุช ุฌูููุฑุ ',\n",
       " ' ูฺฉุชุจ ุดูุฏ ุณููุงู ',\n",
       " ' ฺฉุงุฑ ููุงุด ',\n",
       " ' ุธุฑู! ',\n",
       " ' ุฒูุฑุง ูุนูุช: ',\n",
       " ' ฺฉูุณูู ฺฉุดุงูุฑุฒุ ',\n",
       " ' ุงุฏุฏุงุดุช ',\n",
       " ' ุขุฒุงุฏ ุฎูุงู ',\n",
       " ' ูุฑุงุฌุน ุชููุฏ ',\n",
       " ' ฺฏูุชฺฏู ูู ',\n",
       " ' ุฒุงฺฏุฑุณ ุฏุฑ ุขุชุด ',\n",
       " ' ุฑูุจุฑ ',\n",
       " ' ุงูููุช ุณุฑูพุฑุณุช ',\n",
       " ' ูุงฺฉุณู ุจุฑฺฉุช ',\n",
       " ' ูุฅูุฑุงู ',\n",
       " ' ุญุถุฑุช ุฒูุฑุง(ุณ) ',\n",
       " ' ุญฺฉู ุฑูุจุฑูุนุธู ',\n",
       " ' ุงุณุชุงู ุฎูุฒุณุชุงู ',\n",
       " ' ูู ุจู ุฎุดููุช ุนูู ุฒูุงู ',\n",
       " ' ูู ุจู ุญุฑู ุฏุฑูุงู ',\n",
       " ' ุฑุฏ ุตูุงุญุช ',\n",
       " ' ุชุบุฑ ุฑู ุจูุฏุฌู ',\n",
       " ' ุฒูุงู ',\n",
       " ' ุนููุฏุงุฑ ',\n",
       " ' ุญุณู ุฎุฑุงุฒ ',\n",
       " ' ุฎุฏูุช ุณุฑุจุงุฒ ',\n",
       " ' ุงูููู. ',\n",
       " ' ุฏููฺฉุฑุงุณ ',\n",
       " ' ุงุณุชุงู ูุฑฺฉุฒ ',\n",
       " ' ุจุญุฑุงู ุขุจ ',\n",
       " ' ุงุญูุฏ ูฺุงุฏ ',\n",
       " ' ุจุฑูุงูู ',\n",
       " ' ูุธุฑุณูุฌ: ',\n",
       " ' ุดูุฏุ ',\n",
       " ' ุจุฏูู ุงุณุชุซูุง ',\n",
       " ' ูุณู ฺูุงุฑู ',\n",
       " ' ุงุญูุฏ ูฺุงุฏ ',\n",
       " ' MiddleEast. ',\n",
       " ' ุชููุฏฺฉููุฏฺฏุงูุ ',\n",
       " ' ุงุณุชุงู ฺฉุฑุฏุณุชุงู ',\n",
       " ' ุตุฏู ',\n",
       " ' ุญู ูุฑุฏู ุฏุฑ ุฌุจ ูุฑุฏู ',\n",
       " ' ฺฏุฒุงุฑุด ุจู ูุฑุฏู ',\n",
       " ' ุฏฺฉุชุฑ ุฌุจู ',\n",
       " ' ููุงููุช ุงุฑุงูุงู ',\n",
       " ' ูููุฑฺฉ ุชุงูุฒ:ููุงูุงุช ',\n",
       " ' ุฑุงุณุช ุขุฒูุง ูุงูุน ',\n",
       " ' ุบุฑุจุงูฺฏุฑ ฺูุชฺฉ ',\n",
       " ' ุขููุฒุด ู ูพุฑูุฑุด ',\n",
       " ' ฺฏุฑุงู ',\n",
       " ' ุฌูููุฑุ ',\n",
       " ' ูุทุน ุจุฑู ',\n",
       " ' ุงุณุชุงู ูุฑุณุชุงู ',\n",
       " ' ุฌูุฑุฌ ูููุฏ ',\n",
       " ' ุชุฑูุฑูุณู ',\n",
       " ' ุจูุฏุฌูฑดฐฐ ',\n",
       " ' ูุณุชุถุนูู ',\n",
       " ' ุณูพุงูุ ',\n",
       " ' ูุฑุฏู ุจ ูพูุงู ',\n",
       " ' ุญุฏุงูู ุฏุณุชูุฒุฏ ',\n",
       " ' ุฒุฏู ุฒุฑ ูุฒ ',\n",
       " ' ุนุฑุงู: ',\n",
       " ' ฺฉุฑุฏุณุชุงู ',\n",
       " ' Iran ',\n",
       " ' ุจูฺฉูุญุฑุงู ',\n",
       " ' ูุฌูุณ ูู ',\n",
       " ' ูุญุงูุธู ฺฉุงุฑ ',\n",
       " ' ฺฉุงูุด ุชุจุนุถ ',\n",
       " ' ุงุฑุงูุ ',\n",
       " ' ุตููุฉ ุงููุฑูยป ',\n",
       " ' ุดฺฏูุช ',\n",
       " ' ูุงุฌุนู ูู ',\n",
       " ' ุฎุงููุงุฏู ',\n",
       " ' ูุง ูุชูุงูู ',\n",
       " ' ุฌูุงุฏ ',\n",
       " ' ุงูุชุตุงุฏ ุถุนู ',\n",
       " ' ุฌูู ',\n",
       " ' ฺฉ ูููู ูุงุญุฏ ูุณฺฉู ',\n",
       " ' ููุณู ',\n",
       " ' ุงุตูุงุญุงุช. ',\n",
       " ' ุงุณุชุงู ุงูุจุฑุฒ ',\n",
       " ' ูุนูููุงู ',\n",
       " ' ุฏููุช ูุฑุฏู ',\n",
       " ' ุนููุงู ',\n",
       " ' ุณูพุงู ูุฏุณ ',\n",
       " ' ุงุฑุฒ ',\n",
       " ' ุฏููุช ุณูุงู ',\n",
       " ' ุงุฑุงู ูุงู ',\n",
       " ' ุดุงูฺฉุงุฑ ุฑูุญุงู ',\n",
       " ' ุฏููุช ุงูุฏุงู ู ุชุญูู ',\n",
       " ' ุฑุงูุช ',\n",
       " ' ุณุฑฺฉูุจ ุฏุฑูุงู ',\n",
       " ' ูุนููุงุช ุจุงูฺฉ ',\n",
       " ' ฺฉุฑููุง ุฑุง ุดฺฉุณุช ูุฏูู. ',\n",
       " ' ููฺฉู ุงูุฒุงุจุช ',\n",
       " ' ูุธุฑูุฑุฏู ',\n",
       " ' ุณฺฉูุช ',\n",
       " ' ุชุฑูุฑ ',\n",
       " ' ุจุงุฑุฒุงู ',\n",
       " ' ูุฑุงุฑ ูุงูุงุช ',\n",
       " ' ุญุฑู ุฎุตูุต ',\n",
       " ' ุจูุฑุณ ',\n",
       " ' ุดุฌุฑุงูุ ',\n",
       " ' ุนู ููุฏ ',\n",
       " ' ุงุฑุงู  ุงูุชุฑูุดูุงู: ',\n",
       " ' ุนุฏุงูุช ',\n",
       " ' ุญููู ุฎุงูู ุฏุงุฑ ',\n",
       " ' IranElection2021 ',\n",
       " ' ุณุนุฏ ุฌูู ',\n",
       " ' ุงูุงู ุฎูู(ุฑู)ุ ',\n",
       " ' ููุงูุนุช ',\n",
       " ' ฺฉุงุฑ ุจุฑุง ูุฑุฏู ',\n",
       " ' ูุจูุงู ',\n",
       " ' ูุฑฺฏ ุจุฑุฌุงู ',\n",
       " ' ุฏููุช ุฌูุงู ู ุญุฒุจ ุงููู ',\n",
       " ' ุจุงููุฑ ',\n",
       " ' ุจูุดูุฑ ',\n",
       " ' ุตุฏุง ู ุณูุง ',\n",
       " ' ุณุงุณ ',\n",
       " ' ุฑูููุง ุงุดุฑู ',\n",
       " ' ุญุงุฌ ูุงุณู ',\n",
       " ' ุฏุงูุดุฌูุงู ',\n",
       " ' ุฏููุช ',\n",
       " ' ูุณุชู ุง ',\n",
       " ' ุญุฒุจ ุงููู ',\n",
       " ' ุฒูุงุฑุฉ ุงุฑุจุนูู ',\n",
       " ' ุชุฑุงุดู ',\n",
       " ' ุบ ',\n",
       " ' ุตูููุณุช ',\n",
       " ' ุฌูุงูฺฏุฑุง ',\n",
       " ' ุญุฌุงุจ ',\n",
       " ' ุจูุงุฏ ุดูุฏ ',\n",
       " ' ุบู ุณุงุฒ ',\n",
       " ' ุดูุฑ ฺฉุดูุฑ ',\n",
       " ' ุชุฑุงูุจ ',\n",
       " ' ุจุตุฑุชุ ',\n",
       " ' ฑดฐฐ ',\n",
       " ' ุจุญุฑุงู ุณุงุฒ ',\n",
       " ' ุงูุชุตุงุจุงุช ',\n",
       " ' ุชู ููุช ุงุฑุงู ',\n",
       " ' 3YearsOfWarOnYemen ',\n",
       " ' ุณููุง ุจุนุฏ ุงุฒ ุงูููุงุจ ',\n",
       " ' ุณุงุณุช ูุง ุงูุชุตุงุฏ ุฏุงูุด ',\n",
       " ' ุงุชุงู ุจุงุฒุฑฺฏุงูุ ',\n",
       " ' ุงุตูุงุญุงุช ',\n",
       " ' ุถุนู ุงูุงู ',\n",
       " ' ุตุงุฏู ูุงุฑุฌุงู ',\n",
       " ' ุฏุณุชฺฏุงู ูุถุง ',\n",
       " ' ุงูุชุฎุงุจ ุฏุฑุณุช ฺฉุงุฑ ุฏุฑุณุช ',\n",
       " ' ุงูููู ุนุฌู ููููู ุงููุฑุฌ ',\n",
       " ' ูุตุงุฏุฑู ',\n",
       " ' ุชููุง ฺฏุฑู ฺฉู ',\n",
       " ' ุณุฑุงุฌ ',\n",
       " ' ุจู ุตุฏุฑ ',\n",
       " ' ุธูู ู ุฌูุง ',\n",
       " ' ููุฏ ',\n",
       " ' ูพุฏุฏู   ',\n",
       " ' ูพูุฌุดูุฑ ',\n",
       " ' ุญุฏุงูุซุฑู ',\n",
       " ' ุบุฒู ',\n",
       " ' ฺฏุงูุฏู  ',\n",
       " ' ุดุฑูู ',\n",
       " ' ุฌูุงุช ุบุฑุนูุฏ ',\n",
       " ' ููุชุฑ ',\n",
       " ' ุญุตุฑ ุบุฑูุงูููุ ',\n",
       " ' ุฏู ุชูฺฏ ',\n",
       " ' ูุทุน ุจุฑู ',\n",
       " ' ูุดุงุฑฺฉุช ุญุฏุงฺฉุซุฑ ',\n",
       " ' ุงูุชุงุฒุงุช ',\n",
       " ' ูุงูุง ุฎูุฏุฑู ',\n",
       " ' ุทุจุนุช ',\n",
       " ' ฺฉุงูุฏุฏุง ูพูุดุด ',\n",
       " ' ฺฏุงู ุฏูู ',\n",
       " ' Arbaeen2020 ',\n",
       " ' ุฏููุทุจ ',\n",
       " ' ุฑุงุณุช ุฌูููุฑ ',\n",
       " ' ุฌุฑู ',\n",
       " ' ูุฏุงู ',\n",
       " ' ูุดุงูู ',\n",
       " ' ุญุจุณ ',\n",
       " ' ุบุฑุจฺฏุฑุง ',\n",
       " ' ุฌูุด ุชููุฏ ',\n",
       " ' ุงูุงู ุฌูุนู ุงุตููุงูุ ',\n",
       " ' ูุงุญู  ุดูุงูุช ',\n",
       " ' ูููุฉ ุงููุฏุฑ ',\n",
       " ' ูพูุณ ',\n",
       " ' ุดูุงูุช. ',\n",
       " ' ฺฉุชุงุจ ',\n",
       " ' 24ุฎุฑุฏุงุฏ ',\n",
       " ' ูพุดุฑูุช ููู ุฌุงูุจู ',\n",
       " ' ุฏุงุนุดุ ',\n",
       " ' ุญููุฏ ุจูุง ',\n",
       " ' ุงููุช ูู ุงุฑุงู ',\n",
       " ' ุงูุฑฺุงูุณ ุงุฌุชูุงุน ',\n",
       " ' ุฏุงูุดฺฏุงู ',\n",
       " ' ูุธุงุฑุช ุงุณุชุตูุงุจ ',\n",
       " ' ุจุงุฒฺฏุดุช ุจู ูุฑุฏู ',\n",
       " ' ุงุณูุงู ',\n",
       " ' ุฎูุฏ ุบุฑุฎูุฏุ ',\n",
       " ' ฺฏููุฑ ุนุดู ',\n",
       " ' ุฑุงุณุช ุขุฒูุง ูุงูุน ',\n",
       " ' ุงุณุชุนูุง ',\n",
       " ' ุฏููุช ุชูุงูุง ุฏุฑ ุฌุงูุนู ุชูุงูุง ',\n",
       " ' ุดุจฺฉู ฺฉ ',\n",
       " ' ุชุงุฑุฎ ฺฉุฑููุง ',\n",
       " ' ูุฑุฏู ',\n",
       " ' ูุฑุฒุจุงู ',\n",
       " ' ุฌุงูุนู ุงูุณุงู ',\n",
       " ' ูุณููุงู ',\n",
       " ' ููุช ',\n",
       " ' ุดูุฏุงุ ',\n",
       " ' ุฏููุช ',\n",
       " ' ุงูุงุฑุงุช\\u2069 ',\n",
       " ' ุจุงูุงู ูุถุน ููุฌูุฏ ',\n",
       " ' ุขุช ุงููู ูุงุดู ุฑูุณูุฌุงู ',\n",
       " ' ุงุณุชุงู ุฒุฏ ',\n",
       " ' ุจูุฒู ',\n",
       " ' ูพูุงู ุฌุจู ',\n",
       " ' ูพ ',\n",
       " ' ุงุฑูู ',\n",
       " ' ฺฏุงู ุฏูู ุงูููุงุจ ',\n",
       " ' ฺูู ุณุงูฺฏ ุงูููุงุจ ',\n",
       " ' ุณุนุฏ ุดุฑุงุฒ ',\n",
       " ' ูุฑุฏูู ',\n",
       " ' ฺฉูุงูุ ',\n",
       " ' ุชุฎู ูุฑุฎ ',\n",
       " ' ุชูุฌู ',\n",
       " ' EconomicTerrorism ',\n",
       " ' ฺฏูุด ููุดููุฏ ',\n",
       " ' ุฎุงูู ูุง ุฎุงู ',\n",
       " ' ุฑูุจุฑ ุงูููุงุจ ',\n",
       " ' ุงุชุญุงุฏ ูู ',\n",
       " ' ุฏูฺฏุงูู ุทุจุน ',\n",
       " ' ูุฑูุด ุงูุฑุงู ููุช ',\n",
       " ' ูุฑููุฑฑฐููุฑ ',\n",
       " ' ุฏูพููุงุณ ',\n",
       " ' ุฎุฒุด ูุณุชุถุนูุงู ',\n",
       " ' ฺฏูุงู ',\n",
       " ' ฺูุฏุณุงู ',\n",
       " ' ูุณฺฉู ',\n",
       " ' ููุทูู ',\n",
       " ' ูุฑฺฉู ',\n",
       " ' ูุฑูู ูุณุท ',\n",
       " ' ุฌูุด ุจุฒุฑฺฏ ',\n",
       " ' ููุงุธุฑู  ูุง ',\n",
       " ' ูพุงุฏฺฏุงู ',\n",
       " ' ฺฉูุฏุชุง ',\n",
       " ' ุจุงุฏ ',\n",
       " ' ุงุฑุจุนู ุจุฏูู ุฏูุงุฑ ',\n",
       " ' ุงูุฒูุง ',\n",
       " ' ุตุฏุงูุณููุง ',\n",
       " ' ุงูุชูุงุฏ ',\n",
       " ' ูุฑุฌุน ุจุตุฑ ',\n",
       " ' ูพุดุฑูุช ฺฉุดูุฑ ',\n",
       " ' ูพูุฌ ุฏุฑุจุฑุงุจุฑ ฺฉ ',\n",
       " ' ุงุนุชุฑุงุถ ',\n",
       " ' ููุณุทูู ',\n",
       " ' ุฎุฑุงุณุงู ุฑุถู ',\n",
       " ' ูุฑูุฏฺฏุงู ',\n",
       " ' ุฎุฑ ููุซุฑ ',\n",
       " ' ุณุงุณุช ฺฏุฐุงุฑ ',\n",
       " ' ูุนุทู ',\n",
       " ' ููุณุงุฏ ',\n",
       " ' ฺูุงุฑูุญุงู ู ุจุฎุชุงุฑ ',\n",
       " ' ุฏููพููุงุณู ',\n",
       " ' ุงูููุงุจ ุงุณูุงู ',\n",
       " ' ูุงุณู ุณููุงู ููุฑูุงู ูู ',\n",
       " ' ุบูุท ',\n",
       " ' ุงููุฏุณ ุงูุฑุจ ',\n",
       " ' ุฑุณุงูู ุจุงุดุฏ ',\n",
       " ' ุฑุณุงูู ูุง ุฌุฑุงู ุงุตู ',\n",
       " ' ุฏููุช ุฏูู ',\n",
       " ' EconomicTerrorism. ',\n",
       " ' ุฏุฎุชุฑ ุขุจ ',\n",
       " ' ุฌุงุณูุณ ',\n",
       " ' ุฃูุฑููุง ',\n",
       " ' ุงุนูุงูู ุฌูุงู ุญููู ุจุดุฑ ',\n",
       " ' ูุถุง ูุฌุงุฒ ',\n",
       " ' ุฏููุช ุณุงู ',\n",
       " ' ุณุฑูพู ุฐูุงุจ ',\n",
       " ' ุบุฑุจ ',\n",
       " ' ProphetMuhammad ',\n",
       " ' ูู ุฑุง ูุฏูู ',\n",
       " ' ููุงููุช ูพูุฌุดุฑ ',\n",
       " ' ุจุญุฑูู ',\n",
       " ' ููุง ููุฅุนุฏุงู ',\n",
       " ' ูููุนูุ ',\n",
       " ' ุญุงุฌ ูุงุณู ุณููุงู ',\n",
       " ' ุฏุดููุงู ',\n",
       " ' ุทุงูุจุงู ',\n",
       " ' ุงุณุชุนูุงุฑฺฏุฑ ',\n",
       " ' ูุงูุจุงู ',\n",
       " ' ุฌูู-ุฏุงูุดฺฏุงู ',\n",
       " ' ูู ุงฺฉููู ',\n",
       " ' ููุงููุช ',\n",
       " ' ุญูุงุช ',\n",
       " ' ุตูุญุ ',\n",
       " ' ุฏุงุนุด ',\n",
       " ' ุฑูุฒ ุฌูุงู ฺฉุงุฑฺฏุฑ ',\n",
       " ' ุงุตูุงุญ ุทูุจ: ',\n",
       " ' ุฒูุฒูู ',\n",
       " '  ฺฉุฑููุง ',\n",
       " ' ูู ุฑูุง ุฏุงุฑู ูพุณ ูุณุชู. ',\n",
       " ' ุงูุชูุงู ุณุฎุช ',\n",
       " ' ุฑฺู ุตูููุณุช: ',\n",
       " ' ุชุฑุงููพ ',\n",
       " ' ุณูุงุจู ุชุญุตู ',\n",
       " ' ุจุฑุงุฏุฑฺฉุด ',\n",
       " ' ุฎุงุชู ูุฏุง ',\n",
       " ' ุณุงุฒูุงู ูุธุงู ุฑุณู ',\n",
       " ' ุญูุงุช ุงุฒ ฺฉุงูุง ุงุฑุงู ',\n",
       " ' ุฏููุช ุขูุฏู ',\n",
       " ' ุฑูุฒ ูู ุดูุฑุงูุง ',\n",
       " ' ุจุงุฏู ',\n",
       " ' ุณุงุฒูุงู ุดุงูฺฏูุง ',\n",
       " ' ุงูู ุณูุช ',\n",
       " ' ุจุนุซุช ',\n",
       " ' ูุฑ ฺฏูุด ฺฉ ุณุชุงุฏ ',\n",
       " ' ุขุฐุฑุจุงุฌุงู ุดุฑู ',\n",
       " ' ุญู ุงูุชูุงุฏ ',\n",
       " ' ุดูุฒู ุขุจู ',\n",
       " ' ุฏููุช ุณูู ุฎุงุชู ',\n",
       " ' ูพฺฉ ฺูุงุฑู ',\n",
       " ' ุฌูุงุจ ',\n",
       " ' ุฑฺฉูุฏ ',\n",
       " ' ููุช ุชูพู ',\n",
       " ' ุฑุฆุณ ุฌูููุฑุ ',\n",
       " ' ุตุงุญุจ ุงูุฒูุงู ',\n",
       " ' ูุฑุขู ูุฑูู ',\n",
       " ' ุบุฑ ูพููพููุณุช ',\n",
       " ' ูุญุณู ุฑุถุง:ุจุง ',\n",
       " ' ุฑุง ุงูู ',\n",
       " ' ุทุฑุญ ุงุนุงุฏู ุงููุงู ูุงูุดุฑูุน ูุณุฆููุงู ',\n",
       " ' ูพุงูฺฉ ูุญุดุช ',\n",
       " ' ุณููุงู ',\n",
       " ' ุชูุฆูุชุฑ ',\n",
       " ' ูุฑุบ ',\n",
       " ' ุงูุชุฎุงุจุงุช ุญุฏุงูู ',\n",
       " ' ูุงุช ููุฑุฑุงุช ุฒุฏุง ',\n",
       " ' ุงฺฉุซุฑุช ุบุฑุฎูุฏ ',\n",
       " ' ุงุดุชุบุงู ',\n",
       " ' ุณู ',\n",
       " ' ุณุงุฒูุงู ุจุฑูุงูู ูุจูุฏุฌู ',\n",
       " ' ุฏูู ุฒุจุงูู ',\n",
       " ' ูพุงุณุฎ ูุนุชุจุฑ ',\n",
       " ' ุชุฌุฑุจุงุช ุฏุงูุดฺฏุงู ',\n",
       " ' ุชูุฑุงู: ',\n",
       " ' ฺฏุดุงุด ุงูุชุตุงุฏ ',\n",
       " ' ุชุฌุงุฑุช ุขููุฒุด ',\n",
       " ' ุงุญูุฏ ฺฏู ูุญูุฏ ',\n",
       " ' ุชุฑฺฉูุง ',\n",
       " ' ุทุฑุญ ุตุงูุช ',\n",
       " ' ูุฑู ุฏุฑุง ',\n",
       " ' ุขู ุณุนูุฏ ',\n",
       " ' ุงูุบุงูุณุชุงู ',\n",
       " ' ุฌูฺฏ ุงูุชุตุงุฏู ',\n",
       " ' ุชููุฏ ',\n",
       " ' ุชุญุฑูุ ',\n",
       " ' Future Bank ',\n",
       " ' ุฏููุช ุณุฒุฏูู ',\n",
       " ' ุงุณุฑุงุฆู ',\n",
       " ' ูุฏุฑุช ุฏุงุดุชู ูุง ',\n",
       " ' ุนุฏุงูุช ูุฑููฺฏ ',\n",
       " ' ุนุฐุฑุฎูุงู ',\n",
       " ' ุงุตุบุฑุฒุงุฏูุ ',\n",
       " ' ุดูุฑุง ูฺฏูุจุงู: ',\n",
       " ' ูุจุงุณ ุดุฎุต ูุง ',\n",
       " ' iranelections ',\n",
       " ' ูุญุงุตุฑู ',\n",
       " ' ูุฏู ุฌูููุฑุช ',\n",
       " ' ูุฑู ูุฑุฒุงุฎุงู ',\n",
       " ' ุนุฏุงูุช ุฌูุณุช ',\n",
       " ' ุจุงูฺฉ ',\n",
       " ' ูุจุงุฑุฒู ุจุงูุณุงุฏ ',\n",
       " ' ุตุฏุงูุช ุจุฌุง ุฏุฑูุบ ',\n",
       " ' ุงูฺฏูุณ ',\n",
       " ' ุฑูุงุจุช นถ ',\n",
       " ' ุฏุงุฑุง ูุง ุงุฑุฒ ',\n",
       " ' ุงุฑุฒูพุงุด ',\n",
       " ' ฺฏููุจุงู ูุงูฺฉุ ',\n",
       " ' ุฏูู ููุฏ ูุง ',\n",
       " ' ุชุตููุงุช ',\n",
       " ' ูุฑุตุช ูุง ุงุฒุฏุณุช ุฑูุชู ',\n",
       " ' ุงุตูุงุญ ุทูุจ ',\n",
       " ' ููุงุฒ ุฌูุนู ',\n",
       " ' ูุฏุฆู ',\n",
       " ' ุงููุช ุบุฐุง ',\n",
       " ' ุงุณุชุงู ุณููุงู ',\n",
       " ' ุงูุงุฑุงุช ',\n",
       " ' ุดูุฑุง ูฺฏูุจุงู ',\n",
       " ' ุนุงุฏู ฺฉุงููพูุฑ ',\n",
       " ' ุฏฺฉุชุงุชูุฑ ุฏูุงุฑ ',\n",
       " ' ุขูุฑฺฉุง ',\n",
       " ' ูพูุดููุงุฏ ',\n",
       " ' ุชูุง ูพุงฺฉุฑูุงู ',\n",
       " ' ูุฑุฒุดฺฉุงุฑุงู ',\n",
       " ' ุชููู ุจู ุฑุฆุณ  ุฌูููุฑ ',\n",
       " ' ุฒุงฺฉุงู ',\n",
       " ' ุฑูุงุจุช ูุงุณุงูู ',\n",
       " ' ุดูุฑุงู ูฺฏูุจุงู ',\n",
       " ' ุจุฑุงูุฏุงุฒ ',\n",
       " ' ุชุญุฑู ูุง ',\n",
       " ' ููุฏุจุงู ',\n",
       " ' ููุช ูู ',\n",
       " ' ุดูุฑุง ุงููุช ',\n",
       " ' ูููุฐ ',\n",
       " ' ูุงฺฉุณู ฺฉุฑููุง ',\n",
       " ' ุญฺฉู ',\n",
       " ' ฺฉุดูุฑ ',\n",
       " ' ูุฑุฌ ุฏุงูุง ',\n",
       " ' ูุงุฏุฑุงู ฺุดู ุงูุชุธุงุฑ ',\n",
       " ' ุดุทุงู ',\n",
       " ' ุงูฺฉุฑุงู: ',\n",
       " ' IranTalksVienna ',\n",
       " ' ุญููู ูุคูู ',\n",
       " ' ุฎุจุฑุฎูุจุ ',\n",
       " ' ุณุชุงุฏ ูููุฏุณ ุชุธุงูุฑุงุช ',\n",
       " ' ูุทุนูุงูู ',\n",
       " ' ุขู ุฎูููู ',\n",
       " ' ุงุตูู ฺฏุฑุง ',\n",
       " ' ุญููู ',\n",
       " ' ุญุตุฑ ',\n",
       " ' ูุฑูุด ููุฑูุงูุงูู ',\n",
       " ' ุงู ุนู ฺฉ ุงุดฺฉุงู ูุณุช ',\n",
       " ' ุฒูุฑุง ุดุฌุงุน ',\n",
       " ' ูุทุนูุงูู ูุง ูุชุนุฏุฏ ',\n",
       " ' ูุณููุงูุงู ูุธููู ููุฏ ',\n",
       " ' ูุญุฏุช ',\n",
       " ' ููุงุถุน ุฎุทุฑูุงฺฉ ',\n",
       " ' ูููุฏูุงู ',\n",
       " ' ุงูููุงุจ ุฑูฺฏ ',\n",
       " ' ฺูพ ฺฏุฑุง ุงูุชุตุงุฏ ',\n",
       " ' ุงุฑุจุนู ',\n",
       " ' ุงุณุชุงุฏ ูพูุงูุงู ',\n",
       " ' ฺฏูุชฺฏู ',\n",
       " ' ุญุงูุธ ',\n",
       " ' ุงุตููฺฏุฑุง ',\n",
       " ' ุงุฑุชุด ',\n",
       " ' ุฏูุงุฑ ุฌูุงูฺฏุฑ ',\n",
       " ' ุฅูุฑุงู ',\n",
       " ' ุณููู ุงููุ ',\n",
       " ' ุชูุฑู  ',\n",
       " ' ุนูุงุจุณุชุงู: ',\n",
       " ' ุชูุฏุฏ ',\n",
       " ' ุงูุจุฑูุชูููู ุงูุฅุถุงูู ',\n",
       " ' ฑณ ุขุจุงู ',\n",
       " ' ูุธุฑุณูุฌ ',\n",
       " ' FreeIranianSoldiers ',\n",
       " ' ุญุงฺฉูุช ูุงูููุ ',\n",
       " ' ุฑูุฑุงูุฏู ',\n",
       " ' ูุฒุงุฑุช ุตูุช ',\n",
       " ' election ',\n",
       " ' ูุชูู ',\n",
       " ' IranTalks ',\n",
       " ' ุงููุตุฑ ',\n",
       " ' ุงุฑุงู ุฌุฏุฏ ',\n",
       " ' ุฎุงูู ุฏุงุฑ ุดุบู ุงุณุช ',\n",
       " ' ุฑูุญุงูุ ',\n",
       " ' ุชูุธู ุจุงุฒุงุฑ ',\n",
       " ' ุจูุฏุฌู ',\n",
       " ' ูุฌูุณ ุงุฒุฏูู ',\n",
       " ' ฺฉุงูุฏุฏุงูุง ',\n",
       " ' ูุฑููฺฏ ',\n",
       " ' ุงุนูุงู ',\n",
       " ' ุฏููู ููุงุดฺฏุงู ูุฌุงุฒ ฺฉุชุงุจ ',\n",
       " ' ุชุงุฌฺฉุณุชุงู ',\n",
       " ' ุชุงูุด ',\n",
       " ' ุฎุฑูุดูุฑ ',\n",
       " ' ุฑุณูู ',\n",
       " ' ูุณููุงู ุธุงูู ',\n",
       " ' ุฑูุงุช ุชุญุฑู ',\n",
       " ' PS752 ',\n",
       " ' ููุช ',\n",
       " ' ุฑูุณู ',\n",
       " ' ฺฉุงุฑ ',\n",
       " ' ุฎุงูุช ',\n",
       " ' ุฒูุฏุงู ',\n",
       " ' ูุฐุงูุฑู ',\n",
       " ' ุฒูุฑ ูุง ู ุฑุณุฏ ',\n",
       " ' ุณุฑูุฑุจ ุชูุงูููุฏ ',\n",
       " ' ุชุฑฺฉ ุชุงุจุนุช ',\n",
       " ' ุดุฑฺฉ ',\n",
       " ' ุฎุงููู ุง ',\n",
       " ' ูุฑูุฒฺฏุงู ',\n",
       " ' ุงูุนุฑุงููุฉ ',\n",
       " ' ุชููุช ',\n",
       " ' ูู ',\n",
       " ' ุฌูููุฑ ',\n",
       " ' ฑณุขุจุงู ',\n",
       " ' IranTeamโs ',\n",
       " ' ุงูุงู ููุณ ุตุฏุฑ ',\n",
       " ' ุญุฐู ุบุฑูุงููู ',\n",
       " ' ูุจุฑุงูุ ',\n",
       " ' ุงูุฅุชูุงู ุงููููู. ',\n",
       " ' ูุณุช ุฌูููุฑ ',\n",
       " ' ฺฉูุงูููุฏ ',\n",
       " ' ูุญุงฺฉูู ',\n",
       " ' ุจุช ุงููุงู ',\n",
       " ' ุงูุฅูุงุฑุงุช ',\n",
       " ' ุงูุฑฺ ',\n",
       " ' ุงูุชุตุงุฏ ',\n",
       " ' ูุบู ',\n",
       " ' ุฒุฑ ูุฒ ุงุฑุงู ูุฑุงุณ ',\n",
       " ' ุจุดุงุฑ ุงุณุฏ ',\n",
       " ' ุญุฌ ',\n",
       " ' ุงูุชุฏุงุฏ ุฑูุญุงู ',\n",
       " ' ฺฉุงุฑูุงูู ุฑูุญุงู ุฏุฑ ุญูุฒู ุฒูุงู ',\n",
       " ' ุฌูุฑู ',\n",
       " ' ุฑุง ุงุดุชุจุงู ',\n",
       " ' ูู ุชูุงู ',\n",
       " ' ุญุณุงุจูุง ุณูพุฑุฏู ููู ูุถุงู ',\n",
       " ' ูุณูุทุ ',\n",
       " ' ูุทุจูุนุงุช ',\n",
       " ' ุงูุชุตุงุฏ ุชุญุฑู ',\n",
       " ' ูุญุณู ุฑุถุง ',\n",
       " ' ููุงูพูุง ุงูฺฉุฑุงู ',\n",
       " ' ุตุฏุง ุจ ูุฏุฑุชุงู ',\n",
       " ' ูพุฏุงููุฏ ุบุฑุนุงูู ',\n",
       " ' ุญุฐู ุฏูุงุฑ ',\n",
       " ' ููุณุฏุงู ุงูุชุตุงุฏ ',\n",
       " ' ูุง ููุช ุงูุงู ุญุณูู ',\n",
       " ' ูุงฺฉุณู ุขููุฏู ุขูุฑฺฉุง ',\n",
       " ' ุฑุงุฆ ุดูุงุฑ ',\n",
       " ' ุฎุดููุช ุนูู ุฒูุงู ',\n",
       " ' ุขุชุด ุจู ุงุฎุชุงุฑ ',\n",
       " ' ุณู ุณูพุงู ',\n",
       " ' ฺุงูพู ',\n",
       " ' ููุฑุฒูุงู ',\n",
       " ' ุณูุตูู ู ุงููุฏุณ ',\n",
       " ' ุฎุงููู ุง: ',\n",
       " ' ูุฌุงุฒุงุช ',\n",
       " ' ูุฏุฑุชููุฏ ',\n",
       " ' TokyoParalympics2020 ',\n",
       " ' ุฏููุช ูุธุงูุงูุ ',\n",
       " ' ูุจูุงู ',\n",
       " ' ุทุฑุญ ูุงู ',\n",
       " ' ูุณุงุฏ ุฎูุฏ ',\n",
       " ' ฺฉู ฺฉุด! ',\n",
       " ' ุงูุฑุงู ุจุฏู ',\n",
       " ' ุฌุฑุงุญ ุฏุฑูู ูุธุงู ',\n",
       " ' ุขููุงู ',\n",
       " ' ุชูฺฏู ูุฑูุฒ ',\n",
       " ' ุงุณุชูุฑุงุถ ุบุฑูุณุชูู ',\n",
       " ' ุฑุงุณุช ุฌูููุฑุ ',\n",
       " ' ุงูููุงุจ ุงูุชุตุงุฏ ฑดฐฐ ',\n",
       " ' ุงุดุฏ ูุฌุงุฒุงุช ',\n",
       " ' ููุฑูุงู ',\n",
       " ' ุณุณุชุงู ูุจููฺุณุชุงู ',\n",
       " ' ุฒูุฏุงูุงู ',\n",
       " ' ุณูุฑ ',\n",
       " ' ุจ ุงุซุฑุณุงุฒ ุชุญุฑู ูุง ',\n",
       " ' ุฏูุดุบูู( ',\n",
       " ' ูุงูุงุช ',\n",
       " ' ฺฉูุงุจ ูุงูุณ ',\n",
       " ' ุดูุงู ุณุงุฒ ',\n",
       " ' ุขูุง ุฎูุดุด ุจุงุฏุ ',\n",
       " ' ุฑุงุจุทู ',\n",
       " ' ุชุญุฑู ุฏุงุฎู ',\n",
       " ' ูู ุชูุงูุฏ ',\n",
       " ' ุนุฒุช ',\n",
       " ' ูุงูู ุฑุฆุณ ููู ูุถุงุฆู ',\n",
       " ' ุชุญุฑูู ',\n",
       " ' ุณุนุฏ: ',\n",
       " ' ฺฉุงูููุฏุงุฑุงู ',\n",
       " ' ุถุฏ ุชุจุนุถ ',\n",
       " ' ูุฑุง ุฏุฑุงุจ ',\n",
       " ' ุชุฑุงูุจ! ',\n",
       " ' ูุณุงุฏ ',\n",
       " ' ูุฒุฑ ุงุทูุงุนุงุช ',\n",
       " ' ุญุงูุฏ ุงูุฑ ',\n",
       " ' ุขุฐุฑุจุงุฌุงู ุบุฑุจ ',\n",
       " ' ุญูุฏ ููุดูฺฏ ',\n",
       " ' ุฏููฺฉุฑุงุณู ',\n",
       " ' ุขุฑุงูุด ',\n",
       " ' ุณูุงู ุนุฏุงูุช. ',\n",
       " ' HERO ',\n",
       " ' ุฑุงูุช ู ุงูุญุตุงุฑ ',\n",
       " ' ูุงูุฒุฏูุง ',\n",
       " ' ุงูุณุชุงฺฏุฑุงู ',\n",
       " ' ุงูุฒุงูุงุช ุฌูููุฑ ',\n",
       " ' ุฑูุน ุชุญุฑู ',\n",
       " ' ุฑฺู ุตูููุณุช ',\n",
       " ' ุงุตูุงุญุงุช ุณุงุฎุชุงุฑ ',\n",
       " ' ููุณุทู ุญู= ',\n",
       " ' ุดุฑู ',\n",
       " ' ุจููุชูู ',\n",
       " ' ุญุงุดู ุณุงุฒ ',\n",
       " ' ุฌูฺฏ ูุฑู ',\n",
       " ' ุนุจุงุณ ุนุจุฏ ',\n",
       " ' ุฑูุง ุจุฎุด ',\n",
       " ' ุฑุง ูุฏูู ',\n",
       " ' ุฑูุญุงูุช ',\n",
       " ' ุณุณุชุงู ู ุจููฺุณุชุงู ',\n",
       " ' ุดูุฑุง ุณูุฌุด ',\n",
       " ' ฺฉุงุฑ ุขูุฑู ',\n",
       " ' ููู ูุถุงุฆู ',\n",
       " ' ูู ุจู ูพูุงุณุชฺฉ ',\n",
       " ' ุงุฑุจุนู ูุง ',\n",
       " ' ุชุตููุงุช ูุญูู ',\n",
       " ' ูุฌุฏู ุชุฑ ',\n",
       " ' ุฑูุน ุชุญุฑูู ',\n",
       " ' ุงุณุชุงู ฺฏูุงู ',\n",
       " ' ุงุญูุฏ ูฺุงุฏุ ',\n",
       " ' ุดุฎุตุช ุณุงุณ ูุฐูุจ ',\n",
       " ' ุฑุฆุณ ุฌูููุฑ ',\n",
       " ' ูพุฑูฺู ูููุฐ ',\n",
       " ...]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert multihahstags to words\n",
    "hashtag = []\n",
    "for ht_list in data['hashtags']:\n",
    "    for ht in ht_list:\n",
    "        hashtag.append(ht)\n",
    "hash_set = set(hashtag)\n",
    "ht_list = []\n",
    "for h in hash_set:\n",
    "    ht_list.append(h)\n",
    "hash_list = []\n",
    "for h in hash_set:\n",
    "    hash_list.append(' '+ ' '.join(h[1:].split('_')) + ' ')\n",
    "hash_list2 = []\n",
    "for h in hash_list:\n",
    "    hash_list2.append(' '.join(h.split('\\u200c')))\n",
    "hash_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d2a985a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if data['tweet_num_hashtags'].iloc[i] > 0:\n",
    "        for h in data['hashtags'].iloc[i]:\n",
    "            ind = ht_list.index(h)\n",
    "            data['full_text'].iloc[i] = data['full_text'].iloc[i].replace(h, hash_list2[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c6140a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    stop_words = list()\n",
    "    with open(r'C:/PRIVATE/Metodata/Metodata-Files/stopwords_new.txt',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stop_words.append(line.strip())\n",
    "    exclusions = [\"ir\",\"#ff\", \"ff\", \"rt\",\"RT\", \"FF\",\"\\u200c\",\"\\n\",\"'s\",\"n't\",\"'re\",\"'m\",'#','@','&','?','.','+','-','*','/','โ','...','โฆ','โ','โ','โ','โ','ุ','ุ','.','\"',';','!',':','%','.',',']\n",
    "    stop_words.extend(exclusions)\n",
    "\n",
    "    junk_chars_regex = r'[^a-zA-Z\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF \\u200c]'\n",
    "    url_regex = r\"\"\"((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b\"\"\"\n",
    "    RFC_5322_COMPLIANT_EMAIL_REGEX = r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\"\n",
    "    compile_patterns = lambda patterns: [(re.compile(pattern), repl) for pattern, repl in patterns]\n",
    "    \n",
    "    remove_url = (url_regex, ' ')\n",
    "    remove_email = (RFC_5322_COMPLIANT_EMAIL_REGEX, ' ')\n",
    "    remove_junk_characters = (junk_chars_regex, ' ')\n",
    "    compiled_patterns_before = compile_patterns([remove_url, remove_email])\n",
    "    compiled_patterns_after = compile_patterns([remove_junk_characters])\n",
    "    \n",
    "    text = text.lower()\n",
    "    for pattern, repl in compiled_patterns_before:\n",
    "        text = pattern.sub(repl, text)\n",
    "    text = re.sub(r'[\\u200c\\s]*\\s[\\s\\u200c]*', ' ', text)\n",
    "    text = re.sub(r'[\\u200c]+', '\\u200c', text)\n",
    "\n",
    "    for pattern, repl in compiled_patterns_after:\n",
    "        text = pattern.sub(repl, text)\n",
    "\n",
    "    tokenized_words = tokenizer.tokenize(text)\n",
    "    tokenized_words = [word for word in tokenized_words if word not in stop_words]\n",
    "\n",
    "    return tokenized_words\n",
    "\n",
    "def remove_emoji(text):\n",
    "    junk_chars_regex = r'[^a-zA-Z\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF \\u200c]'\n",
    "    compile_patterns = lambda patterns: [(re.compile(pattern), repl) for pattern, repl in patterns]\n",
    "    remove_junk_characters = (junk_chars_regex, ' ')\n",
    "    compiled_patterns_after = compile_patterns([remove_junk_characters])\n",
    "    for pattern, repl in compiled_patterns_after:\n",
    "        text = pattern.sub(repl, text)\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def make_farsi_text(x):\n",
    "    reshaped_text = arabic_reshaper.reshape(x)\n",
    "    farsi_text = get_display(reshaped_text)\n",
    "    return farsi_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "65516bd5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 50)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_5080/2797418508.py:6: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_5080/2797418508.py:7: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "data['full_text']=data['full_text'].fillna('')\n",
    "print(data.shape)\n",
    "data['full_text'] = [str(i).lower() for i in data['full_text']]\n",
    "print('1')\n",
    "data['full_text']=data['full_text'].str.replace('\\d+', '')\n",
    "data['full_text'] = data['full_text'].str.replace('@[\\w\\-]+','')\n",
    "print('2')\n",
    "data['full_text']=data['full_text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "print('3')\n",
    "data['full_text']=data['full_text'].apply(lambda x:remove_emoji(x) )\n",
    "print('4')\n",
    "data['full_text']=data['full_text'].apply(lambda x:preprocess(x))\n",
    "print('5')\n",
    "data['full_text']=data['full_text'].apply(lambda x:' '.join([word for word in x]))\n",
    "print('6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce9516",
   "metadata": {},
   "source": [
    "# Negativity/Character Attack/Political Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bd5192ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Character Attack and Political Attack run following comment\n",
    "\n",
    "data = data[data['neg'] == 1]\n",
    "\n",
    "#for Character Attack change data['neg'] to data['char'] in code\n",
    "#for Political Attack change data['neg'] to data['pol'] in code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1197da",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e55a2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, data['neg'], test_size=0.2, random_state=0, stratify=data['neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2a8ce",
   "metadata": {},
   "source": [
    "## Undersmapling - Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "4d626faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 50)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_data = X_train[X_train['neg'] == 1]\n",
    "positive_data = X_train[X_train['neg'] == 0]\n",
    "'''\n",
    "#Undersampling\n",
    "cutting_point = min(len(negative_data), len(positive_data))\n",
    "\n",
    "if cutting_point <= len(negative_data):\n",
    "    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "if cutting_point <= len(positive_data):\n",
    "    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)'''\n",
    "\n",
    "#Oversampling\n",
    "'''if len(positive_data) > len(negative_data):\n",
    "    negative_data = negative_data.sample(n=len(positive_data), replace=True).reset_index(drop=True)\n",
    "\n",
    "if len(positive_data) < len(negative_data):\n",
    "    positive_data = positive_data.sample(n=len(negative_data), replace=True).reset_index(drop=True)'''\n",
    "\n",
    "X_train = pd.concat([negative_data, positive_data])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccf2fa",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e6212911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 300)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_n_train = []\n",
    "for w in X_train['full_text']:\n",
    "    vecs_n_train.append(ft.get_word_vector(w))\n",
    "vecs_n_train = np.array(vecs_n_train)\n",
    "vecs_n_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "320a28af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 300)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_n_test = []\n",
    "for w in X_test['full_text'].to_list():\n",
    "    vecs_n_test.append(ft.get_word_vector(w))\n",
    "vecs_n_test = np.array(vecs_n_test)\n",
    "vecs_n_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "4fa0d51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831,)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n_train = X_train['neg'].to_list()\n",
    "y_n_train = np.array(y_n_train)\n",
    "y_n_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d547a40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208,)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n_test = y_test\n",
    "y_n_test = np.array(y_n_test)\n",
    "y_n_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "088b4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf0af4",
   "metadata": {},
   "source": [
    "## Textual Features for Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "7cd4c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pd.DataFrame({'full_text':X_train['full_text'], 'neg':X_train['neg']})\n",
    "df_s1 = S[S['neg'] == 1]\n",
    "df_s0 = S[S['neg'] == 0]\n",
    "def Merge(D1,D2):\n",
    "    py = D1 | D2\n",
    "    return py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "accfab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sort_Dic = {}\n",
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l1 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l2 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l3 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "616ba2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 850)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_Top = [x[0] for x in l1[:400]] + [x[0] for x in l2[:100]] + [x[0] for x in l3[:50]]\n",
    "L = X_train['full_text'].to_list()\n",
    "top_dic = {}\n",
    "for i in range(len(dic_Top)):\n",
    "    k = []\n",
    "    for j in range(len(L)):\n",
    "        if(dic_Top[i] in L[j]):\n",
    "            k.append(1)\n",
    "        else:\n",
    "            k.append(0)\n",
    "    top_dic[dic_Top[i]] = k\n",
    "df_top_s = pd.DataFrame(top_dic)\n",
    "vecs_n_train = pd.DataFrame(vecs_n_train)\n",
    "df_top_s = df_top_s.reset_index()\n",
    "vecs_n_train = vecs_n_train.reset_index()\n",
    "dic_all_train = pd.concat([vecs_n_train, df_top_s], axis=1)\n",
    "dic_all_train = dic_all_train.drop({'index'}, axis=1)\n",
    "dic_all_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "634faa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_1 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_2 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_3 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_1 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_2 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_3 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "sort_dic0_n = Merge(sort_dic0_1, sort_dic0_2)\n",
    "sort_dic0 = Merge(sort_dic0_n, sort_dic0_3)\n",
    "\n",
    "sort_dic1_n = Merge(sort_dic1_1, sort_dic1_2)\n",
    "sort_dic1 = Merge(sort_dic1_n, sort_dic1_3)\n",
    "\n",
    "sort_dic0 = dict(sorted(sort_dic0.items(), key=lambda item: item[1] , reverse=True))\n",
    "sort_dic1 = dict(sorted(sort_dic1.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "Top0 = [x for x in list(sort_dic0_1.keys())][:500] + [x for x in list(sort_dic0_2.keys())][:100] + [x for x in list(sort_dic0_3.keys())][:50]\n",
    "Top1 = [x for x in list(sort_dic1_1.keys())][:500] + [x for x in list(sort_dic1_2.keys())][:100] + [x for x in list(sort_dic1_3.keys())][:50]\n",
    "\n",
    "Top01 = [x for x in list(sort_dic0_1.keys()) if x not in sort_dic1_1.keys()][:500] + [x for x in list(sort_dic0_2.keys()) if x not in sort_dic1_2.keys()][:100] + [x for x in list(sort_dic0_3.keys()) if x not in sort_dic1_3.keys()][:100]\n",
    "Top10 = [x for x in list(sort_dic1_1.keys()) if x not in sort_dic0_1.keys()][:500] + [x for x in list(sort_dic1_2.keys()) if x not in sort_dic0_2.keys()][:100] + [x for x in list(sort_dic1_3.keys()) if x not in sort_dic0_3.keys()][:100]\n",
    "\n",
    "Top0_w = [x for x in list(sort_dic0_1.keys())][:500] + [x for x in list(sort_dic0_2.keys())][:100] + [x for x in list(sort_dic0_3.keys())][:50]\n",
    "Top1_w = [x for x in list(sort_dic1_1.keys())][:500] + [x for x in list(sort_dic1_2.keys())][:100] + [x for x in list(sort_dic1_3.keys())][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "98636e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 831, 831, 831, 831, 831)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_Top0 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0):\n",
    "                count += 1\n",
    "        feature_Top0.append(count)\n",
    "    else:\n",
    "        feature_Top0.append(0)\n",
    "        \n",
    "feature_Top1 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1):\n",
    "                count += 1\n",
    "        feature_Top1.append(count)\n",
    "    else:\n",
    "        feature_Top1.append(0)\n",
    "        \n",
    "feature_Top01 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top01):\n",
    "                count += 1\n",
    "        feature_Top01.append(count)\n",
    "    else:\n",
    "        feature_Top01.append(0)\n",
    "        \n",
    "feature_Top10 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top10):\n",
    "                count += 1\n",
    "        feature_Top10.append(count)\n",
    "    else:\n",
    "        feature_Top10.append(0)\n",
    "        \n",
    "feature_weight1 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1_w):\n",
    "                count += sort_dic1[tweet[i]]\n",
    "        feature_weight1.append(count)\n",
    "    else:\n",
    "        feature_weight1.append(0)\n",
    "\n",
    "feature_weight0 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0_w):\n",
    "                count += sort_dic0[tweet[i]]\n",
    "        feature_weight0.append(count)\n",
    "    else:\n",
    "        feature_weight0.append(0)\n",
    "\n",
    "len(feature_Top0), len(feature_Top1), len(feature_Top01), len(feature_Top10), len(feature_weight1), len(feature_weight0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f17353ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((831, 868), 831)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train = pd.DataFrame(\n",
    "    {'50Top_0': feature_Top0,\n",
    "     '50Top_1': feature_Top1,\n",
    "     'Top_0_1': feature_Top01,\n",
    "     'Top_1_0': feature_Top10,\n",
    "     'Weight_0': feature_weight0,\n",
    "     'Weight_1': feature_weight1})\n",
    "features_train['tweet_like'] = X_train['tweet_like']\n",
    "features_train['tweet_retweet_count'] = X_train['tweet_retweet_count']\n",
    "features_train['tweet_length_word'] = X_train['tweet_length_word']\n",
    "features_train['tweet_length_characters'] = X_train['tweet_length_characters']\n",
    "features_train['tweet_num_hashtags'] = X_train['tweet_num_hashtags']\n",
    "features_train['tweet_num_mention'] = X_train['tweet_num_mention']\n",
    "features_train['tweet_num_urls'] = X_train['tweet_num_urls']\n",
    "features_train['tweet_num_emoji'] = X_train['tweet_num_emoji']\n",
    "features_train['tweet_num_punctuation'] = X_train['tweet_num_punctuation']\n",
    "features_train['person_names'] = X_train['person_names']\n",
    "features_train['organize_names'] = X_train['organize_names']\n",
    "features_train['swear_words'] = X_train['swear_words']\n",
    "\n",
    "X_train_new = pd.concat([dic_all_train , features_train], axis=1)\n",
    "X_train_new = X_train_new.fillna(0)\n",
    "y_train_new = X_train['neg'].to_list()\n",
    "scaler_train = Normalizer()\n",
    "scaler_train.fit(X_train_new)\n",
    "X_train_new = scaler_train.transform(X_train_new)\n",
    "X_train_new.shape, len(y_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91db43",
   "metadata": {},
   "source": [
    "## Textual Features for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "980a5857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 850)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_Top = [x[0] for x in l1[:400]] + [x[0] for x in l2[:100]] + [x[0] for x in l3[:50]]\n",
    "L = X_test['full_text'].to_list()\n",
    "top_dic = {}\n",
    "for i in range(len(dic_Top)):\n",
    "    k = []\n",
    "    for j in range(len(L)):\n",
    "        if(dic_Top[i] in L[j]):\n",
    "            k.append(1)\n",
    "        else:\n",
    "            k.append(0)\n",
    "    top_dic[dic_Top[i]] = k\n",
    "df_top_s = pd.DataFrame(top_dic)\n",
    "vecs_n_test = pd.DataFrame(vecs_n_test)\n",
    "df_top_s = df_top_s.reset_index()\n",
    "vecs_n_test = vecs_n_test.reset_index()\n",
    "dic_all_test = pd.concat([vecs_n_test, df_top_s], axis=1)\n",
    "dic_all_test = dic_all_test.drop({'index'}, axis=1)\n",
    "dic_all_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9b90281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(208, 208, 208, 208, 208, 208)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_Top0_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0):\n",
    "                count += 1\n",
    "        feature_Top0_t.append(count)\n",
    "    else:\n",
    "        feature_Top0_t.append(0)\n",
    "\n",
    "feature_Top1_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1):\n",
    "                count += 1\n",
    "        feature_Top1_t.append(count)\n",
    "    else:\n",
    "        feature_Top1_t.append(0)\n",
    "\n",
    "feature_Top01_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top01):\n",
    "                count += 1\n",
    "        feature_Top01_t.append(count)\n",
    "    else:\n",
    "        feature_Top01_t.append(0)\n",
    "        \n",
    "feature_Top10_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top10):\n",
    "                count += 1\n",
    "        feature_Top10_t.append(count)\n",
    "    else:\n",
    "        feature_Top10_t.append(0)\n",
    "        \n",
    "feature_weight1_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1_w):\n",
    "                count += sort_dic1[tweet[i]]\n",
    "        feature_weight1_t.append(count)\n",
    "    else:\n",
    "        feature_weight1_t.append(0)\n",
    "        \n",
    "feature_weight0_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0_w):\n",
    "                count += sort_dic0[tweet[i]]\n",
    "        feature_weight0_t.append(count)\n",
    "    else:\n",
    "        feature_weight0_t.append(0)\n",
    "        \n",
    "len(feature_Top0_t), len(feature_Top1_t), len(feature_Top01_t), len(feature_Top10_t), len(feature_weight1_t), len(feature_weight0_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "54d08b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(208, 868)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test = pd.DataFrame(\n",
    "    {'50Top_0': feature_Top0_t,\n",
    "     '50Top_1': feature_Top1_t,\n",
    "     'Top_0_1': feature_Top01_t,\n",
    "     'Top_1_0': feature_Top10_t,\n",
    "     'Weight_0': feature_weight0_t,\n",
    "     'Weight_1': feature_weight1_t\n",
    "      })\n",
    "features_test['tweet_like'] = X_test['tweet_like']\n",
    "features_test['tweet_retweet_count'] = X_test['tweet_retweet_count']\n",
    "features_test['tweet_length_word'] = X_test['tweet_length_word']\n",
    "features_test['tweet_length_characters'] = X_test['tweet_length_characters']\n",
    "features_test['tweet_num_hashtags'] = X_test['tweet_num_hashtags']\n",
    "features_test['tweet_num_mention'] = X_test['tweet_num_mention']\n",
    "features_test['tweet_num_urls'] = X_test['tweet_num_urls']\n",
    "features_test['tweet_num_emoji'] = X_test['tweet_num_emoji']\n",
    "features_test['tweet_num_punctuation'] = X_test['tweet_num_punctuation']\n",
    "features_test['person_names'] = X_test['person_names']\n",
    "features_test['organize_names'] = X_test['organize_names']\n",
    "features_test['swear_words'] = X_test['swear_words']\n",
    "\n",
    "X_test_new = pd.concat([dic_all_test , features_test], axis=1)\n",
    "X_test_new = X_test_new.fillna(0)\n",
    "scaler_test = Normalizer()\n",
    "scaler_test.fit(X_test_new)\n",
    "X_test_new = scaler_test.transform(X_test_new)\n",
    "X_test_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45f8bf",
   "metadata": {},
   "source": [
    "## Classical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ad067",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLP\n",
    "'''parameter_space = {\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive']}'''\n",
    "\n",
    "clf1 = MLPClassifier(random_state=0, max_iter=400)\n",
    "#clf1 = GridSearchCV(mlp, parameter_space, cv=5, n_jobs=1)\n",
    "clf1.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('****MLP classificaion report for test set****')\n",
    "predictions1 = clf1.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions1))\n",
    "print(metrics.roc_auc_score(y_test,predictions1))\n",
    "\n",
    "print('\\n****MLP classificaion report for train set****')\n",
    "predictions1_1 = clf1.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_1))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_1))\n",
    "\n",
    "#XGB\n",
    "'''parameter_space={'max_depth': range(3, 18),\n",
    "                    'gamma': range(1,9)}'''\n",
    "\n",
    "eval_set = [(X_test_new, y_test)]\n",
    "clf2 = XGBClassifier(random_state=0, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)\n",
    "#clf2 = GridSearchCV(xgb, parameter_space, cv=3)\n",
    "clf2.fit(X_train_new, y_train_new, verbose=True)\n",
    "\n",
    "print('\\n\\n\\n\\n****XGB classificaion report for test set****')\n",
    "predictions2 = clf2.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions2))\n",
    "print(metrics.roc_auc_score(y_test,predictions2))\n",
    "\n",
    "print('\\n****XGB classificaion report for train set****')\n",
    "predictions1_2 = clf2.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_2))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_2))\n",
    "\n",
    "#RF\n",
    "'''parameter_space={'max_depth' : list(range(1, 50))}'''\n",
    "\n",
    "clf3 = RandomForestClassifier(random_state=0)\n",
    "#clf3 = GridSearchCV(rf, parameter_space, cv=3, scoring=p_score)\n",
    "clf3.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****RF classificaion report for test set****')\n",
    "predictions3 = clf3.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions3))\n",
    "print(metrics.roc_auc_score(y_test,predictions3))\n",
    "\n",
    "print('\\n****RF classificaion report for train set****')\n",
    "predictions1_3 = clf3.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_3))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_3))\n",
    "\n",
    "#LR\n",
    "'''parameter_space={'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'penalty' : ['l2'],\n",
    "                'C' : [100, 10, 1.0, 0.1, 0.01]}'''\n",
    "\n",
    "clf4 = LogisticRegression(random_state=0)\n",
    "#clf4 = GridSearchCV(lr, parameter_space, cv=3, scoring=p_score)\n",
    "clf4.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****LR classificaion report for test set****')\n",
    "predictions4 = clf4.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions4))\n",
    "print(metrics.roc_auc_score(y_test,predictions4))\n",
    "\n",
    "print('\\n****LR classificaion report for train set****')\n",
    "predictions1_4 = clf4.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_4))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_4))\n",
    "\n",
    "#NB\n",
    "clf5 = GaussianNB()\n",
    "clf5.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****NB classificaion report for test set****')\n",
    "predictions5 = clf5.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions5))\n",
    "print(metrics.roc_auc_score(y_test,predictions5))\n",
    "\n",
    "print('\\n****NB classificaion report for train set****')\n",
    "predictions1_5 = clf5.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_5))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_5))\n",
    "\n",
    "#SVM\n",
    "clf6 = svm.SVC(random_state=0)\n",
    "clf6.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****SVM classificaion report for test set****')\n",
    "predictions6 = clf6.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions6))\n",
    "print(metrics.roc_auc_score(y_test,predictions6))\n",
    "\n",
    "print('\\n****SVM classificaion report for train set****')\n",
    "predictions1_6 = clf6.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_6))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_6))\n",
    "\n",
    "#SGD\n",
    "clf7 = SGDClassifier(max_iter=2000, tol=1e-3, random_state=0)\n",
    "clf7.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****SGD classificaion report for test set****')\n",
    "predictions7 = clf7.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions7))\n",
    "print(metrics.roc_auc_score(y_test,predictions7))\n",
    "\n",
    "print('\\n****SGD classificaion report for train set****')\n",
    "predictions1_7 = clf7.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_7))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_7))\n",
    "\n",
    "#KNN\n",
    "'''parameter_space = {'n_neighbors' : list(range(1,300))}'''\n",
    "clf8 = KNeighborsClassifier()\n",
    "#clf8 = GridSearchCV(kn, parameter_space, n_jobs=-1, cv=5, scoring='f1')\n",
    "clf8.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****KNN classificaion report for test set****')\n",
    "predictions8 = clf8.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions8))\n",
    "print(metrics.roc_auc_score(y_test,predictions8))\n",
    "\n",
    "print('\\n****KNN classificaion report for train set****')\n",
    "predictions1_8 = clf8.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_8))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1b5b8",
   "metadata": {},
   "source": [
    "## Feature Importance Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0a4d0b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#feature importances for MLP\n",
    "l1 = []\n",
    "l2 = clf1.coefs_[0]\n",
    "for i in l2:\n",
    "    l1.append(sum(i))\n",
    "importance_1 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_1.append(i)\n",
    "importance_1 = [(float(i)-min(importance_1))/(max(importance_1)-min(importance_1)) for i in importance_1]\n",
    "\n",
    "#feature importances for XGB\n",
    "l1 = []\n",
    "l2 = clf2.feature_importances_\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_2 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_2.append(i)\n",
    "importance_2\n",
    "importance_2 = [(float(i)-min(importance_2))/(max(importance_2)-min(importance_2)) for i in importance_2]\n",
    "\n",
    "#feature importances for RF\n",
    "l1 = []\n",
    "l2 = clf3.feature_importances_\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_3 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_3.append(i)\n",
    "importance_3 = [(float(i)-min(importance_3))/(max(importance_3)-min(importance_3)) for i in importance_3]\n",
    "\n",
    "#feature importances for LR\n",
    "l1 = []\n",
    "l2 = clf4.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_4 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_4.append(i)\n",
    "importance_4 = [(float(i)-min(importance_4))/(max(importance_4)-min(importance_4)) for i in importance_4]\n",
    "\n",
    "#feature importances for NB\n",
    "l1 = []\n",
    "l2 = permutation_importance(clf5, X_test_new, y_test)\n",
    "l2 = l2.importances_mean\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_5 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_5.append(i)\n",
    "importance_5 = [(float(i)-min(importance_5))/(max(importance_5)-min(importance_5)) for i in importance_5]\n",
    "\n",
    "#feature importances for SVM\n",
    "l1 = []\n",
    "clf6 = svm.SVC(random_state=0, kernel='linear')\n",
    "clf6.fit(X_train_new, y_train_new)\n",
    "l2 = clf6.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_6 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_6.append(i)\n",
    "importance_6 = [(float(i)-min(importance_6))/(max(importance_6)-min(importance_6)) for i in importance_6]\n",
    "\n",
    "#feature importances for SGD\n",
    "l1 = []\n",
    "l2 = clf7.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_7 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_7.append(i)\n",
    "importance_7 = [(float(i)-min(importance_7))/(max(importance_7)-min(importance_7)) for i in importance_7]\n",
    "\n",
    "#feature importances for KNN\n",
    "l1 = []\n",
    "l2 = permutation_importance(clf, X_test_new, y_test, scoring='neg_mean_squared_error')\n",
    "l2 = l2.importances_mean\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_8 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_8.append(i)\n",
    "importance_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "886d8674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "col_name = ['embedding','unigram','bigram','trigram','Top_0','Top_1','Top_0_1','Top_1_0','Weight_0',\n",
    "            'Weight_1','like','retweet','len_word','len_chars','hashtags','mention','urls','emoji',\n",
    "            'punctuation','person','organize','swear']\n",
    "N = 12\n",
    "ind = np.arange(N)\n",
    "width = 0.1\n",
    "\n",
    "xvals = importance_1[10:]\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'r')\n",
    "  \n",
    "yvals = importance_2[10:]\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='g')\n",
    "\n",
    "zvals = importance_3[10:]\n",
    "bar3 = plt.bar(ind+width*2, zvals, width, color = 'b')\n",
    "\n",
    "z1vals = importance_4[10:]\n",
    "bar4 = plt.bar(ind+width*3, z1vals, width, color = 'c')\n",
    "\n",
    "z2vals = importance_5[10:]\n",
    "bar5 = plt.bar(ind+width*4, z2vals, width, color = 'm')\n",
    "\n",
    "z3vals = importance_6[10:]\n",
    "bar6 = plt.bar(ind+width*5, z3vals, width, color = 'y')\n",
    "\n",
    "z4vals = importance_7[10:]\n",
    "bar7 = plt.bar(ind+width*6, z4vals, width, color = 'lime')\n",
    "\n",
    "z5vals = importance_8[10:]\n",
    "bar8 = plt.bar(ind+width*7, z5vals, width, color = 'violet')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(23, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=15)\n",
    "plt.title(\"Textual Feature Importance for Negativity Detection Algorithms(for 2382 tweets with underampling)\",fontsize=15)\n",
    "plt.xticks(ind+width,col_name[10:],fontsize=15)\n",
    "plt.legend((bar1, bar2, bar3 , bar4, bar5, bar6, bar7, bar8), ('MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_2.jpg' ,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "N = 10\n",
    "ind = np.arange(N)\n",
    "width = 0.1\n",
    "\n",
    "xvals = importance_1[:10]\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'r')\n",
    "  \n",
    "yvals = importance_2[:10]\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='g')\n",
    "\n",
    "zvals = importance_3[:10]\n",
    "bar3 = plt.bar(ind+width*2, zvals, width, color = 'b')\n",
    "\n",
    "z1vals = importance_4[:10]\n",
    "bar4 = plt.bar(ind+width*3, z1vals, width, color = 'c')\n",
    "\n",
    "z2vals = importance_5[:10]\n",
    "bar5 = plt.bar(ind+width*4, z2vals, width, color = 'm')\n",
    "\n",
    "z3vals = importance_6[:10]\n",
    "bar6 = plt.bar(ind+width*5, z3vals, width, color = 'y')\n",
    "\n",
    "z4vals = importance_7[:10]\n",
    "bar7 = plt.bar(ind+width*6, z4vals, width, color = 'lime')\n",
    "\n",
    "z5vals = importance_8[:10]\n",
    "bar8 = plt.bar(ind+width*7, z5vals, width, color = 'violet')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(23, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=15)\n",
    "plt.title(\"Textual Feature Importance for Negativity Detection Algorithms(for 2382 tweets with underampling)\",fontsize=15)\n",
    "plt.xticks(ind+width,col_name[:10],fontsize=15)\n",
    "plt.legend((bar1, bar2, bar3 , bar4, bar5, bar6, bar7, bar8), ('MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_2.jpg' ,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7e3a5",
   "metadata": {},
   "source": [
    "## Overfitting Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3eed2cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_f1_test_0 = [0.69, 0.67, 0.77, 0.63, 0.56, 0.65, 0.58, 0.70, 0.76]\n",
    "list_f1_train_0 = [0.90, 1, 1, 0.64, 0.87, 0.65, 0.61, 0.77, 0.89]\n",
    "list_f1_test_1 = [0.53, 0.49, 0.52, 0.51, 0.50, 0.52 ,0.51, 0.50, 0.63]\n",
    "list_f1_train_1 = [0.91, 1, 1, 0.74, 0.89, 0.74, 0.74, 0.79, 0.89]\n",
    "N = 9\n",
    "ind = np.arange(N)\n",
    "width = 0.3\n",
    "\n",
    "xvals = list_f1_test_0\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'lightcoral')\n",
    "  \n",
    "yvals = list_f1_train_0\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='brown')\n",
    "\n",
    "fig.set_size_inches(20, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=12)\n",
    "plt.title(\"F1-Score for Class 0 in Negativity Detection(for 2382 tweets)\",fontsize=12)\n",
    "plt.xticks(ind+width,['MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN' , 'P-BERT'], fontsize=10)\n",
    "plt.legend((bar1, bar2), ('test' , 'train'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_overfit.jpg' ,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55944d4",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e593c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_excel(r'../Files/5100_new.xlsx')\n",
    "data['full_text'] = data1['full_text']\n",
    "data['rate'] = data1['neg']\n",
    "\n",
    "data['rate'] = data['rate'].apply(lambda r: r if r < 2 else None)\n",
    "\n",
    "data = data.dropna(subset=['rate'])\n",
    "data = data.dropna(subset=['full_text'])\n",
    "data = data.drop_duplicates(subset=['full_text'], keep='first')\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print('data information')\n",
    "print(data.info(), '\\n')\n",
    "\n",
    "# print missing values information\n",
    "print('missing values stats')\n",
    "print(data.isnull().sum(), '\\n')\n",
    "\n",
    "# print some missing values\n",
    "print('some missing values')\n",
    "print(data[data['rate'].isnull()].iloc[:5], '\\n')\n",
    "\n",
    "data['comment_len_by_words'] = data['full_text'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "\n",
    "min_max_len = data[\"comment_len_by_words\"].min(), data[\"comment_len_by_words\"].max()\n",
    "print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')\n",
    "\n",
    "minlim, maxlim = 0, 47\n",
    "\n",
    "data['comment_len_by_words'] = data['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\n",
    "data = data.dropna(subset=['comment_len_by_words'])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "unique_rates = list(sorted(data['rate'].unique()))\n",
    "\n",
    "def rate_to_label(rate, threshold=0.0):\n",
    "    if rate <= threshold:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "data['label'] = data['rate'].apply(lambda t: rate_to_label(t, 0.0))\n",
    "labels = list(sorted(data['label'].unique()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea330db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "  \n",
    "    text = text.strip()\n",
    "    \n",
    "    # regular cleaning\n",
    "    text = clean(text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lower=True,\n",
    "        no_line_breaks=True,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "    )\n",
    "\n",
    "    # cleaning htmls\n",
    "    text = cleanhtml(text)\n",
    "    \n",
    "    # normalizing\n",
    "    normalizer = hazm.Normalizer()\n",
    "    text = normalizer.normalize(text)\n",
    "    \n",
    "    # removing wierd patterns\n",
    "    wierd_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u2069\"\n",
    "        u\"\\u2066\"\n",
    "        # u\"\\u200c\"\n",
    "        u\"\\u2068\"\n",
    "        u\"\\u2067\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = wierd_pattern.sub(r'', text)\n",
    "    \n",
    "    # removing extra spaces, hashtags\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning comments\n",
    "data['cleaned_comment'] = data['full_text'].apply(cleaning)\n",
    "\n",
    "#calculate the length of comments based on their words\n",
    "data['cleaned_comment_len_by_words'] = data['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "\n",
    "#remove comments with the length of fewer than three words\n",
    "data['cleaned_comment_len_by_words'] = data['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\n",
    "data = data.dropna(subset=['cleaned_comment_len_by_words'])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "data = data[['full_text', 'label']]\n",
    "data.columns = ['comment', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c60652",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_id'] = data['label'].apply(lambda t: 1-labels.index(t))\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=0, stratify=data['label'])\n",
    "train, valid = train_test_split(train, test_size=0.2, random_state=0, stratify=train['label'])\n",
    "negative_data = train[train['label'] == 'negative']\n",
    "positive_data = train[train['label'] == 'positive']\n",
    "\n",
    "cutting_point = min(len(negative_data), len(positive_data))\n",
    "\n",
    "if cutting_point <= len(negative_data):\n",
    "    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "if cutting_point <= len(positive_data):\n",
    "    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "if len(positive_data) > len(negative_data):\n",
    "    negative_data = negative_data.sample(n=len(positive_data), replace=True).reset_index(drop=True)\n",
    "\n",
    "if len(positive_data) < len(negative_data):\n",
    "    positive_data = positive_data.sample(n=len(negative_data), replace=True).reset_index(drop=True)'''\n",
    "train = pd.concat([negative_data, positive_data])\n",
    "#new_data = new_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "x_train, y_train = train['comment'].values.tolist(), train['label_id'].values.tolist()\n",
    "x_valid, y_valid = valid['comment'].values.tolist(), valid['label_id'].values.tolist()\n",
    "x_test, y_test = test['comment'].values.tolist(), test['label_id'].values.tolist()\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import TFBertModel, TFBertForSequenceClassification\n",
    "from transformers import glue_convert_examples_to_features\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general config\n",
    "MAX_LEN = 79\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "\n",
    "EPOCHS = 4\n",
    "EEVERY_EPOCH = 500\n",
    "LEARNING_RATE = 2e-5\n",
    "CLIP = 0.0\n",
    "dropout_rate = 0.4\n",
    "\n",
    "MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'\n",
    "#MODEL_NAME_OR_PATH = 'distilbert-base-uncased'\n",
    "#MODEL_NAME_OR_PATH = 'bert-base-multilingual-cased'\n",
    "#MODEL_NAME_OR_PATH = 'distilbert-base-multilingual-cased'\n",
    "OUTPUT_PATH = r'./pytorch_model.bin'\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ccd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: 1-i for i, label in enumerate(labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "config = BertConfig.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH, **{\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label,\n",
    "    })\n",
    "config.hidden_dropout_prob = 0.5\n",
    "config.attention_probs_dropout_prob = 0.5\n",
    "print(config.to_json_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27cd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\" A single example for simple sequence classification. \"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\" Constructs a InputExample. \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def make_examples(tokenizer, x, y=None, maxlen=128, output_mode=\"classification\", is_tf_dataset=True):\n",
    "    examples = []\n",
    "    y = y if isinstance(y, list) or isinstance(y, np.ndarray) else [None] * len(x)\n",
    "\n",
    "    for i, (_x, _y) in tqdm(enumerate(zip(x, y)), position=0, total=len(x)):\n",
    "        guid = \"%s\" % i\n",
    "        label = int(_y)\n",
    "        \n",
    "        if isinstance(_x, str):\n",
    "            text_a = _x\n",
    "            text_b = None\n",
    "        else:\n",
    "            assert len(_x) == 2\n",
    "            text_a = _x[0]\n",
    "            text_b = _x[1]\n",
    "        \n",
    "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    \n",
    "    features = glue_convert_examples_to_features(\n",
    "        examples, \n",
    "        tokenizer, \n",
    "        maxlen, \n",
    "        output_mode=output_mode, \n",
    "        label_list=list(np.unique(y)))\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_token_type_ids = []\n",
    "    all_labels = []\n",
    "\n",
    "    for f in tqdm(features, position=0, total=len(examples)):\n",
    "        if is_tf_dataset:\n",
    "            all_input_ids.append(tf.constant(f.input_ids))\n",
    "            all_attention_masks.append(tf.constant(f.attention_mask))\n",
    "            all_token_type_ids.append(tf.constant(f.token_type_ids))\n",
    "            all_labels.append(tf.constant(f.label))\n",
    "        else:\n",
    "            all_input_ids.append(f.input_ids)\n",
    "            all_attention_masks.append(f.attention_mask)\n",
    "            all_token_type_ids.append(f.token_type_ids)\n",
    "            all_labels.append(f.label)\n",
    "\n",
    "    if is_tf_dataset:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "            'input_ids': all_input_ids,\n",
    "            'attention_mask': all_attention_masks,\n",
    "            'token_type_ids': all_token_type_ids\n",
    "        }, all_labels))\n",
    "\n",
    "        return dataset, features\n",
    "    \n",
    "    xdata = [np.array(all_input_ids), np.array(all_attention_masks), np.array(all_token_type_ids)]\n",
    "    ydata = all_labels\n",
    "\n",
    "    return [xdata, ydata], features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_base, train_examples = make_examples(tokenizer, x_train, y_train, maxlen=256)\n",
    "valid_dataset_base, valid_examples = make_examples(tokenizer, x_valid, y_valid, maxlen=256)\n",
    "\n",
    "test_dataset_base, test_examples = make_examples(tokenizer, x_test, y_test, maxlen=256)\n",
    "[xtest, ytest], test_examples = make_examples(tokenizer, x_test, y_test, maxlen=256, is_tf_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset(dataset, batch_size):\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(dataset, batch_size):\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_training_dataset(train_dataset_base, TRAIN_BATCH_SIZE)\n",
    "valid_dataset = get_training_dataset(valid_dataset_base, VALID_BATCH_SIZE)\n",
    "\n",
    "train_steps = len(train_examples) // TRAIN_BATCH_SIZE\n",
    "valid_steps = len(valid_examples) // VALID_BATCH_SIZE\n",
    "\n",
    "train_steps, valid_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "def build_model(model_name, config, learning_rate=3e-5):\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(MODEL_NAME_OR_PATH, config, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1)\n",
    "\n",
    "final_accuracy = r.history['val_accuracy']\n",
    "print('FINAL ACCURACY MEAN: ', np.mean(final_accuracy))\n",
    "\n",
    "model.save_pretrained(os.path.dirname(OUTPUT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d326397",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = model.evaluate(test_dataset_base.batch(TEST_BATCH_SIZE))\n",
    "print()\n",
    "print(f'Evaluation: {ev}')\n",
    "print()\n",
    "\n",
    "predictions = model.predict(xtest)\n",
    "ypred = predictions[0].argmax(axis=-1).tolist()\n",
    "\n",
    "print()\n",
    "print(classification_report(ytest, ypred, target_names=labels))\n",
    "print()\n",
    "\n",
    "print(f'F1: {f1_score(ytest, ypred, average=\"weighted\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
